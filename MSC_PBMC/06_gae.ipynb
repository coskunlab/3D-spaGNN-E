{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn.models import autoencoder\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "  \n",
    "  def __init__(self, in_channels, hidden_size, out_channels, dropout):\n",
    "    super(GCNEncoder, self).__init__()\n",
    "    self.conv1 = GCNConv(in_channels, hidden_size)\n",
    "    self.conv2 = GCNConv(hidden_size, out_channels)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    x = self.conv1(x, edge_index).relu()\n",
    "    x = self.dropout(x)\n",
    "    return self.conv2(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae_train(train_data, gae_model, optimizer):\n",
    "    gae_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = gae_model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = gae_model.recon_loss(z, train_data.pos_edge_label_index.to(device))\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def gae_test(test_data, gae_model):\n",
    "    gae_model.eval()\n",
    "    z = gae_model.encode(test_data.x, test_data.edge_index)\n",
    "    return gae_model.test(z, test_data.pos_edge_label_index, test_data.neg_edge_label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxi_df = pd.read_csv(r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\proxi_dfs\\cd4\\set1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_pickle(r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\connected_patches\\cd4\\set1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_centers = pd.read_csv(r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\patch_centers\\cd4\\set1_centers.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:00<00:00, 4623.75it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 229180.53it/s]\n",
      "61120it [00:46, 1328.61it/s]\n"
     ]
    }
   ],
   "source": [
    "edge_temp = []\n",
    "for item in tqdm(edges):\n",
    "    edge_temp = edge_temp + item\n",
    "\n",
    "total_connections = 0\n",
    "for item in tqdm(edges):\n",
    "    total_connections = total_connections + len(item)\n",
    "\n",
    "edges = np.zeros((2,total_connections))\n",
    "for i,item in tqdm(enumerate(edge_temp)):\n",
    "    idx1 = proxi_df.index.tolist().index(item[0])\n",
    "    idx2 = proxi_df.index.tolist().index(item[1])\n",
    "    edges[0,i] = idx1\n",
    "    edges[1,i] = idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = proxi_df.values\n",
    "X_tensor = torch.tensor(proxi_df.values, dtype=torch.float)\n",
    "edge_tensor = torch.tensor(edges, dtype=torch.long)\n",
    "data = Data(x=X_tensor,edge_index=edge_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation\n",
    "t = T.Compose([T.ToUndirected(),T.RandomLinkSplit(is_undirected=True,split_labels=True)])\n",
    "train_set,val_set,test_set = t(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[12224, 15], edge_index=[2, 66832], pos_edge_label=[8354], pos_edge_label_index=[2, 8354], neg_edge_label=[8354], neg_edge_label_index=[2, 8354])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.to(device)\n",
    "val_set.to(device)\n",
    "test_set.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = X.shape[1]\n",
    "HIDDEN_SIZE = 15\n",
    "OUT_CHANNELS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_model = GAE(GCNEncoder(NUM_FEATURES, HIDDEN_SIZE, OUT_CHANNELS, 0.5))\n",
    "gae_model = gae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, test AUC: 0.6231, test AP: 0.6408, train AUC: 0.6721, train AP: 0.6914, loss:2.3292\n",
      "Epoch: 002, test AUC: 0.6254, test AP: 0.6423, train AUC: 0.6744, train AP: 0.6929, loss:2.2526\n",
      "Epoch: 003, test AUC: 0.6276, test AP: 0.6438, train AUC: 0.6766, train AP: 0.6945, loss:2.1923\n",
      "Epoch: 004, test AUC: 0.6298, test AP: 0.6453, train AUC: 0.6789, train AP: 0.6961, loss:2.1510\n",
      "Epoch: 005, test AUC: 0.6321, test AP: 0.6470, train AUC: 0.6811, train AP: 0.6976, loss:2.1259\n",
      "Epoch: 006, test AUC: 0.6343, test AP: 0.6485, train AUC: 0.6834, train AP: 0.6992, loss:2.0838\n",
      "Epoch: 007, test AUC: 0.6365, test AP: 0.6499, train AUC: 0.6856, train AP: 0.7007, loss:2.0197\n",
      "Epoch: 008, test AUC: 0.6387, test AP: 0.6515, train AUC: 0.6878, train AP: 0.7022, loss:1.9774\n",
      "Epoch: 009, test AUC: 0.6409, test AP: 0.6530, train AUC: 0.6900, train AP: 0.7037, loss:1.9026\n",
      "Epoch: 010, test AUC: 0.6431, test AP: 0.6545, train AUC: 0.6922, train AP: 0.7052, loss:1.8939\n",
      "Epoch: 011, test AUC: 0.6452, test AP: 0.6560, train AUC: 0.6944, train AP: 0.7068, loss:1.8796\n",
      "Epoch: 012, test AUC: 0.6474, test AP: 0.6575, train AUC: 0.6966, train AP: 0.7083, loss:1.8269\n",
      "Epoch: 013, test AUC: 0.6495, test AP: 0.6590, train AUC: 0.6988, train AP: 0.7098, loss:1.7758\n",
      "Epoch: 014, test AUC: 0.6517, test AP: 0.6605, train AUC: 0.7009, train AP: 0.7113, loss:1.7467\n",
      "Epoch: 015, test AUC: 0.6538, test AP: 0.6620, train AUC: 0.7031, train AP: 0.7128, loss:1.7574\n",
      "Epoch: 016, test AUC: 0.6559, test AP: 0.6635, train AUC: 0.7052, train AP: 0.7143, loss:1.7222\n",
      "Epoch: 017, test AUC: 0.6580, test AP: 0.6650, train AUC: 0.7073, train AP: 0.7158, loss:1.6879\n",
      "Epoch: 018, test AUC: 0.6601, test AP: 0.6665, train AUC: 0.7094, train AP: 0.7173, loss:1.6327\n",
      "Epoch: 019, test AUC: 0.6622, test AP: 0.6679, train AUC: 0.7116, train AP: 0.7188, loss:1.6317\n",
      "Epoch: 020, test AUC: 0.6642, test AP: 0.6694, train AUC: 0.7137, train AP: 0.7202, loss:1.6263\n",
      "Epoch: 021, test AUC: 0.6663, test AP: 0.6709, train AUC: 0.7158, train AP: 0.7217, loss:1.5978\n",
      "Epoch: 022, test AUC: 0.6684, test AP: 0.6724, train AUC: 0.7179, train AP: 0.7232, loss:1.5940\n",
      "Epoch: 023, test AUC: 0.6704, test AP: 0.6739, train AUC: 0.7199, train AP: 0.7247, loss:1.5703\n",
      "Epoch: 024, test AUC: 0.6724, test AP: 0.6754, train AUC: 0.7220, train AP: 0.7262, loss:1.5304\n",
      "Epoch: 025, test AUC: 0.6744, test AP: 0.6768, train AUC: 0.7241, train AP: 0.7278, loss:1.5239\n",
      "Epoch: 026, test AUC: 0.6764, test AP: 0.6783, train AUC: 0.7262, train AP: 0.7293, loss:1.5161\n",
      "Epoch: 027, test AUC: 0.6784, test AP: 0.6797, train AUC: 0.7282, train AP: 0.7308, loss:1.4813\n",
      "Epoch: 028, test AUC: 0.6804, test AP: 0.6812, train AUC: 0.7303, train AP: 0.7322, loss:1.4759\n",
      "Epoch: 029, test AUC: 0.6823, test AP: 0.6827, train AUC: 0.7323, train AP: 0.7338, loss:1.4715\n",
      "Epoch: 030, test AUC: 0.6843, test AP: 0.6841, train AUC: 0.7344, train AP: 0.7353, loss:1.4422\n",
      "Epoch: 031, test AUC: 0.6862, test AP: 0.6856, train AUC: 0.7364, train AP: 0.7368, loss:1.4390\n",
      "Epoch: 032, test AUC: 0.6882, test AP: 0.6871, train AUC: 0.7385, train AP: 0.7383, loss:1.4285\n",
      "Epoch: 033, test AUC: 0.6901, test AP: 0.6885, train AUC: 0.7405, train AP: 0.7398, loss:1.4072\n",
      "Epoch: 034, test AUC: 0.6920, test AP: 0.6900, train AUC: 0.7426, train AP: 0.7414, loss:1.4039\n",
      "Epoch: 035, test AUC: 0.6939, test AP: 0.6914, train AUC: 0.7446, train AP: 0.7429, loss:1.4014\n",
      "Epoch: 036, test AUC: 0.6958, test AP: 0.6929, train AUC: 0.7466, train AP: 0.7444, loss:1.3820\n",
      "Epoch: 037, test AUC: 0.6976, test AP: 0.6943, train AUC: 0.7487, train AP: 0.7460, loss:1.3799\n",
      "Epoch: 038, test AUC: 0.6995, test AP: 0.6958, train AUC: 0.7507, train AP: 0.7475, loss:1.3784\n",
      "Epoch: 039, test AUC: 0.7014, test AP: 0.6973, train AUC: 0.7527, train AP: 0.7491, loss:1.3725\n",
      "Epoch: 040, test AUC: 0.7032, test AP: 0.6988, train AUC: 0.7547, train AP: 0.7507, loss:1.3599\n",
      "Epoch: 041, test AUC: 0.7051, test AP: 0.7003, train AUC: 0.7568, train AP: 0.7523, loss:1.3548\n",
      "Epoch: 042, test AUC: 0.7069, test AP: 0.7018, train AUC: 0.7588, train AP: 0.7539, loss:1.3389\n",
      "Epoch: 043, test AUC: 0.7088, test AP: 0.7033, train AUC: 0.7608, train AP: 0.7554, loss:1.3373\n",
      "Epoch: 044, test AUC: 0.7106, test AP: 0.7049, train AUC: 0.7628, train AP: 0.7570, loss:1.3267\n",
      "Epoch: 045, test AUC: 0.7125, test AP: 0.7064, train AUC: 0.7648, train AP: 0.7587, loss:1.3158\n",
      "Epoch: 046, test AUC: 0.7143, test AP: 0.7080, train AUC: 0.7668, train AP: 0.7603, loss:1.3229\n",
      "Epoch: 047, test AUC: 0.7162, test AP: 0.7095, train AUC: 0.7688, train AP: 0.7619, loss:1.3079\n",
      "Epoch: 048, test AUC: 0.7180, test AP: 0.7111, train AUC: 0.7707, train AP: 0.7635, loss:1.2957\n",
      "Epoch: 049, test AUC: 0.7198, test AP: 0.7126, train AUC: 0.7727, train AP: 0.7651, loss:1.3020\n",
      "Epoch: 050, test AUC: 0.7216, test AP: 0.7141, train AUC: 0.7747, train AP: 0.7667, loss:1.2878\n",
      "Epoch: 051, test AUC: 0.7234, test AP: 0.7156, train AUC: 0.7766, train AP: 0.7683, loss:1.2889\n",
      "Epoch: 052, test AUC: 0.7252, test AP: 0.7171, train AUC: 0.7785, train AP: 0.7699, loss:1.2922\n",
      "Epoch: 053, test AUC: 0.7269, test AP: 0.7187, train AUC: 0.7804, train AP: 0.7715, loss:1.2846\n",
      "Epoch: 054, test AUC: 0.7287, test AP: 0.7202, train AUC: 0.7824, train AP: 0.7731, loss:1.2838\n",
      "Epoch: 055, test AUC: 0.7304, test AP: 0.7217, train AUC: 0.7843, train AP: 0.7747, loss:1.2832\n",
      "Epoch: 056, test AUC: 0.7321, test AP: 0.7232, train AUC: 0.7862, train AP: 0.7763, loss:1.2726\n",
      "Epoch: 057, test AUC: 0.7338, test AP: 0.7247, train AUC: 0.7880, train AP: 0.7779, loss:1.2651\n",
      "Epoch: 058, test AUC: 0.7355, test AP: 0.7262, train AUC: 0.7899, train AP: 0.7795, loss:1.2677\n",
      "Epoch: 059, test AUC: 0.7372, test AP: 0.7278, train AUC: 0.7918, train AP: 0.7811, loss:1.2569\n",
      "Epoch: 060, test AUC: 0.7388, test AP: 0.7293, train AUC: 0.7936, train AP: 0.7826, loss:1.2635\n",
      "Epoch: 061, test AUC: 0.7405, test AP: 0.7307, train AUC: 0.7954, train AP: 0.7842, loss:1.2523\n",
      "Epoch: 062, test AUC: 0.7421, test AP: 0.7322, train AUC: 0.7972, train AP: 0.7857, loss:1.2598\n",
      "Epoch: 063, test AUC: 0.7437, test AP: 0.7337, train AUC: 0.7990, train AP: 0.7873, loss:1.2510\n",
      "Epoch: 064, test AUC: 0.7453, test AP: 0.7351, train AUC: 0.8008, train AP: 0.7888, loss:1.2440\n",
      "Epoch: 065, test AUC: 0.7469, test AP: 0.7366, train AUC: 0.8025, train AP: 0.7903, loss:1.2376\n",
      "Epoch: 066, test AUC: 0.7485, test AP: 0.7380, train AUC: 0.8043, train AP: 0.7919, loss:1.2438\n",
      "Epoch: 067, test AUC: 0.7501, test AP: 0.7395, train AUC: 0.8060, train AP: 0.7934, loss:1.2335\n",
      "Epoch: 068, test AUC: 0.7516, test AP: 0.7409, train AUC: 0.8077, train AP: 0.7948, loss:1.2369\n",
      "Epoch: 069, test AUC: 0.7532, test AP: 0.7423, train AUC: 0.8094, train AP: 0.7963, loss:1.2320\n",
      "Epoch: 070, test AUC: 0.7547, test AP: 0.7437, train AUC: 0.8110, train AP: 0.7977, loss:1.2241\n",
      "Epoch: 071, test AUC: 0.7562, test AP: 0.7450, train AUC: 0.8126, train AP: 0.7991, loss:1.2232\n",
      "Epoch: 072, test AUC: 0.7576, test AP: 0.7463, train AUC: 0.8142, train AP: 0.8005, loss:1.2253\n",
      "Epoch: 073, test AUC: 0.7591, test AP: 0.7476, train AUC: 0.8158, train AP: 0.8019, loss:1.2184\n",
      "Epoch: 074, test AUC: 0.7605, test AP: 0.7489, train AUC: 0.8173, train AP: 0.8033, loss:1.2153\n",
      "Epoch: 075, test AUC: 0.7619, test AP: 0.7502, train AUC: 0.8189, train AP: 0.8046, loss:1.2186\n",
      "Epoch: 076, test AUC: 0.7633, test AP: 0.7515, train AUC: 0.8204, train AP: 0.8059, loss:1.2154\n",
      "Epoch: 077, test AUC: 0.7647, test AP: 0.7527, train AUC: 0.8219, train AP: 0.8073, loss:1.2179\n",
      "Epoch: 078, test AUC: 0.7661, test AP: 0.7540, train AUC: 0.8233, train AP: 0.8086, loss:1.2069\n",
      "Epoch: 079, test AUC: 0.7675, test AP: 0.7552, train AUC: 0.8248, train AP: 0.8098, loss:1.2006\n",
      "Epoch: 080, test AUC: 0.7688, test AP: 0.7564, train AUC: 0.8262, train AP: 0.8111, loss:1.2113\n",
      "Epoch: 081, test AUC: 0.7701, test AP: 0.7576, train AUC: 0.8276, train AP: 0.8124, loss:1.2036\n",
      "Epoch: 082, test AUC: 0.7714, test AP: 0.7588, train AUC: 0.8290, train AP: 0.8136, loss:1.1974\n",
      "Epoch: 083, test AUC: 0.7727, test AP: 0.7599, train AUC: 0.8304, train AP: 0.8148, loss:1.1975\n",
      "Epoch: 084, test AUC: 0.7740, test AP: 0.7611, train AUC: 0.8317, train AP: 0.8160, loss:1.1932\n",
      "Epoch: 085, test AUC: 0.7752, test AP: 0.7622, train AUC: 0.8331, train AP: 0.8171, loss:1.1950\n",
      "Epoch: 086, test AUC: 0.7764, test AP: 0.7633, train AUC: 0.8344, train AP: 0.8183, loss:1.2048\n",
      "Epoch: 087, test AUC: 0.7776, test AP: 0.7644, train AUC: 0.8356, train AP: 0.8194, loss:1.1915\n",
      "Epoch: 088, test AUC: 0.7788, test AP: 0.7655, train AUC: 0.8369, train AP: 0.8206, loss:1.1905\n",
      "Epoch: 089, test AUC: 0.7800, test AP: 0.7665, train AUC: 0.8381, train AP: 0.8217, loss:1.1860\n",
      "Epoch: 090, test AUC: 0.7812, test AP: 0.7676, train AUC: 0.8394, train AP: 0.8228, loss:1.1884\n",
      "Epoch: 091, test AUC: 0.7823, test AP: 0.7686, train AUC: 0.8406, train AP: 0.8239, loss:1.1837\n",
      "Epoch: 092, test AUC: 0.7834, test AP: 0.7697, train AUC: 0.8418, train AP: 0.8250, loss:1.1859\n",
      "Epoch: 093, test AUC: 0.7845, test AP: 0.7707, train AUC: 0.8429, train AP: 0.8260, loss:1.1750\n",
      "Epoch: 094, test AUC: 0.7856, test AP: 0.7717, train AUC: 0.8441, train AP: 0.8270, loss:1.1781\n",
      "Epoch: 095, test AUC: 0.7866, test AP: 0.7727, train AUC: 0.8452, train AP: 0.8281, loss:1.1798\n",
      "Epoch: 096, test AUC: 0.7877, test AP: 0.7737, train AUC: 0.8463, train AP: 0.8291, loss:1.1772\n",
      "Epoch: 097, test AUC: 0.7887, test AP: 0.7746, train AUC: 0.8474, train AP: 0.8301, loss:1.1732\n",
      "Epoch: 098, test AUC: 0.7897, test AP: 0.7756, train AUC: 0.8485, train AP: 0.8311, loss:1.1713\n",
      "Epoch: 099, test AUC: 0.7907, test AP: 0.7765, train AUC: 0.8495, train AP: 0.8320, loss:1.1788\n",
      "Epoch: 100, test AUC: 0.7917, test AP: 0.7774, train AUC: 0.8506, train AP: 0.8329, loss:1.1696\n",
      "Epoch: 101, test AUC: 0.7926, test AP: 0.7783, train AUC: 0.8516, train AP: 0.8339, loss:1.1684\n",
      "Epoch: 102, test AUC: 0.7936, test AP: 0.7792, train AUC: 0.8525, train AP: 0.8348, loss:1.1641\n",
      "Epoch: 103, test AUC: 0.7945, test AP: 0.7801, train AUC: 0.8535, train AP: 0.8357, loss:1.1656\n",
      "Epoch: 104, test AUC: 0.7954, test AP: 0.7809, train AUC: 0.8544, train AP: 0.8365, loss:1.1601\n",
      "Epoch: 105, test AUC: 0.7963, test AP: 0.7818, train AUC: 0.8554, train AP: 0.8374, loss:1.1580\n",
      "Epoch: 106, test AUC: 0.7971, test AP: 0.7826, train AUC: 0.8563, train AP: 0.8382, loss:1.1585\n",
      "Epoch: 107, test AUC: 0.7980, test AP: 0.7834, train AUC: 0.8572, train AP: 0.8390, loss:1.1595\n",
      "Epoch: 108, test AUC: 0.7988, test AP: 0.7841, train AUC: 0.8580, train AP: 0.8398, loss:1.1523\n",
      "Epoch: 109, test AUC: 0.7997, test AP: 0.7849, train AUC: 0.8589, train AP: 0.8406, loss:1.1555\n",
      "Epoch: 110, test AUC: 0.8005, test AP: 0.7856, train AUC: 0.8597, train AP: 0.8413, loss:1.1552\n",
      "Epoch: 111, test AUC: 0.8012, test AP: 0.7863, train AUC: 0.8605, train AP: 0.8421, loss:1.1460\n",
      "Epoch: 112, test AUC: 0.8020, test AP: 0.7871, train AUC: 0.8613, train AP: 0.8428, loss:1.1592\n",
      "Epoch: 113, test AUC: 0.8028, test AP: 0.7877, train AUC: 0.8621, train AP: 0.8435, loss:1.1499\n",
      "Epoch: 114, test AUC: 0.8035, test AP: 0.7884, train AUC: 0.8628, train AP: 0.8442, loss:1.1461\n",
      "Epoch: 115, test AUC: 0.8042, test AP: 0.7891, train AUC: 0.8635, train AP: 0.8448, loss:1.1481\n",
      "Epoch: 116, test AUC: 0.8049, test AP: 0.7897, train AUC: 0.8642, train AP: 0.8455, loss:1.1426\n",
      "Epoch: 117, test AUC: 0.8056, test AP: 0.7904, train AUC: 0.8649, train AP: 0.8461, loss:1.1462\n",
      "Epoch: 118, test AUC: 0.8062, test AP: 0.7910, train AUC: 0.8656, train AP: 0.8467, loss:1.1419\n",
      "Epoch: 119, test AUC: 0.8069, test AP: 0.7916, train AUC: 0.8662, train AP: 0.8473, loss:1.1430\n",
      "Epoch: 120, test AUC: 0.8075, test AP: 0.7922, train AUC: 0.8669, train AP: 0.8479, loss:1.1368\n",
      "Epoch: 121, test AUC: 0.8081, test AP: 0.7928, train AUC: 0.8675, train AP: 0.8485, loss:1.1343\n",
      "Epoch: 122, test AUC: 0.8087, test AP: 0.7933, train AUC: 0.8681, train AP: 0.8490, loss:1.1346\n",
      "Epoch: 123, test AUC: 0.8093, test AP: 0.7939, train AUC: 0.8687, train AP: 0.8496, loss:1.1382\n",
      "Epoch: 124, test AUC: 0.8098, test AP: 0.7944, train AUC: 0.8692, train AP: 0.8501, loss:1.1351\n",
      "Epoch: 125, test AUC: 0.8104, test AP: 0.7950, train AUC: 0.8698, train AP: 0.8506, loss:1.1336\n",
      "Epoch: 126, test AUC: 0.8109, test AP: 0.7955, train AUC: 0.8703, train AP: 0.8511, loss:1.1373\n",
      "Epoch: 127, test AUC: 0.8114, test AP: 0.7960, train AUC: 0.8708, train AP: 0.8516, loss:1.1246\n",
      "Epoch: 128, test AUC: 0.8119, test AP: 0.7964, train AUC: 0.8713, train AP: 0.8521, loss:1.1317\n",
      "Epoch: 129, test AUC: 0.8124, test AP: 0.7969, train AUC: 0.8718, train AP: 0.8526, loss:1.1347\n",
      "Epoch: 130, test AUC: 0.8129, test AP: 0.7974, train AUC: 0.8723, train AP: 0.8530, loss:1.1294\n",
      "Epoch: 131, test AUC: 0.8134, test AP: 0.7978, train AUC: 0.8728, train AP: 0.8535, loss:1.1239\n",
      "Epoch: 132, test AUC: 0.8138, test AP: 0.7983, train AUC: 0.8732, train AP: 0.8539, loss:1.1228\n",
      "Epoch: 133, test AUC: 0.8142, test AP: 0.7987, train AUC: 0.8737, train AP: 0.8543, loss:1.1253\n",
      "Epoch: 134, test AUC: 0.8147, test AP: 0.7991, train AUC: 0.8741, train AP: 0.8547, loss:1.1206\n",
      "Epoch: 135, test AUC: 0.8151, test AP: 0.7995, train AUC: 0.8745, train AP: 0.8551, loss:1.1235\n",
      "Epoch: 136, test AUC: 0.8155, test AP: 0.7999, train AUC: 0.8749, train AP: 0.8555, loss:1.1198\n",
      "Epoch: 137, test AUC: 0.8158, test AP: 0.8002, train AUC: 0.8753, train AP: 0.8559, loss:1.1238\n",
      "Epoch: 138, test AUC: 0.8162, test AP: 0.8006, train AUC: 0.8757, train AP: 0.8563, loss:1.1209\n",
      "Epoch: 139, test AUC: 0.8166, test AP: 0.8010, train AUC: 0.8761, train AP: 0.8567, loss:1.1152\n",
      "Epoch: 140, test AUC: 0.8169, test AP: 0.8013, train AUC: 0.8765, train AP: 0.8570, loss:1.1165\n",
      "Epoch: 141, test AUC: 0.8172, test AP: 0.8016, train AUC: 0.8768, train AP: 0.8574, loss:1.1139\n",
      "Epoch: 142, test AUC: 0.8176, test AP: 0.8019, train AUC: 0.8772, train AP: 0.8577, loss:1.1167\n",
      "Epoch: 143, test AUC: 0.8179, test AP: 0.8022, train AUC: 0.8775, train AP: 0.8580, loss:1.1129\n",
      "Epoch: 144, test AUC: 0.8182, test AP: 0.8026, train AUC: 0.8779, train AP: 0.8584, loss:1.1100\n",
      "Epoch: 145, test AUC: 0.8185, test AP: 0.8029, train AUC: 0.8782, train AP: 0.8587, loss:1.1123\n",
      "Epoch: 146, test AUC: 0.8188, test AP: 0.8032, train AUC: 0.8785, train AP: 0.8590, loss:1.1098\n",
      "Epoch: 147, test AUC: 0.8191, test AP: 0.8035, train AUC: 0.8788, train AP: 0.8593, loss:1.1131\n",
      "Epoch: 148, test AUC: 0.8194, test AP: 0.8038, train AUC: 0.8791, train AP: 0.8596, loss:1.1144\n",
      "Epoch: 149, test AUC: 0.8197, test AP: 0.8041, train AUC: 0.8794, train AP: 0.8599, loss:1.1091\n",
      "Epoch: 150, test AUC: 0.8200, test AP: 0.8044, train AUC: 0.8797, train AP: 0.8602, loss:1.1163\n",
      "Epoch: 151, test AUC: 0.8202, test AP: 0.8047, train AUC: 0.8800, train AP: 0.8605, loss:1.1135\n",
      "Epoch: 152, test AUC: 0.8205, test AP: 0.8049, train AUC: 0.8803, train AP: 0.8608, loss:1.1115\n",
      "Epoch: 153, test AUC: 0.8207, test AP: 0.8052, train AUC: 0.8806, train AP: 0.8611, loss:1.1038\n",
      "Epoch: 154, test AUC: 0.8210, test AP: 0.8055, train AUC: 0.8808, train AP: 0.8614, loss:1.0980\n",
      "Epoch: 155, test AUC: 0.8212, test AP: 0.8057, train AUC: 0.8811, train AP: 0.8616, loss:1.1110\n",
      "Epoch: 156, test AUC: 0.8214, test AP: 0.8060, train AUC: 0.8814, train AP: 0.8619, loss:1.1063\n",
      "Epoch: 157, test AUC: 0.8217, test AP: 0.8063, train AUC: 0.8816, train AP: 0.8622, loss:1.1139\n",
      "Epoch: 158, test AUC: 0.8219, test AP: 0.8065, train AUC: 0.8819, train AP: 0.8625, loss:1.1118\n",
      "Epoch: 159, test AUC: 0.8221, test AP: 0.8068, train AUC: 0.8821, train AP: 0.8627, loss:1.1105\n",
      "Epoch: 160, test AUC: 0.8223, test AP: 0.8071, train AUC: 0.8824, train AP: 0.8630, loss:1.0963\n",
      "Epoch: 161, test AUC: 0.8226, test AP: 0.8073, train AUC: 0.8826, train AP: 0.8632, loss:1.1034\n",
      "Epoch: 162, test AUC: 0.8228, test AP: 0.8076, train AUC: 0.8828, train AP: 0.8635, loss:1.1026\n",
      "Epoch: 163, test AUC: 0.8230, test AP: 0.8078, train AUC: 0.8830, train AP: 0.8637, loss:1.1071\n",
      "Epoch: 164, test AUC: 0.8232, test AP: 0.8081, train AUC: 0.8833, train AP: 0.8640, loss:1.1012\n",
      "Epoch: 165, test AUC: 0.8234, test AP: 0.8083, train AUC: 0.8835, train AP: 0.8642, loss:1.1015\n",
      "Epoch: 166, test AUC: 0.8236, test AP: 0.8086, train AUC: 0.8837, train AP: 0.8645, loss:1.0974\n",
      "Epoch: 167, test AUC: 0.8238, test AP: 0.8088, train AUC: 0.8839, train AP: 0.8647, loss:1.1053\n",
      "Epoch: 168, test AUC: 0.8240, test AP: 0.8090, train AUC: 0.8841, train AP: 0.8650, loss:1.0995\n",
      "Epoch: 169, test AUC: 0.8242, test AP: 0.8093, train AUC: 0.8844, train AP: 0.8652, loss:1.1019\n",
      "Epoch: 170, test AUC: 0.8243, test AP: 0.8095, train AUC: 0.8846, train AP: 0.8654, loss:1.0992\n",
      "Epoch: 171, test AUC: 0.8245, test AP: 0.8098, train AUC: 0.8848, train AP: 0.8657, loss:1.0995\n",
      "Epoch: 172, test AUC: 0.8247, test AP: 0.8100, train AUC: 0.8850, train AP: 0.8659, loss:1.1036\n",
      "Epoch: 173, test AUC: 0.8249, test AP: 0.8103, train AUC: 0.8852, train AP: 0.8662, loss:1.0994\n",
      "Epoch: 174, test AUC: 0.8251, test AP: 0.8105, train AUC: 0.8854, train AP: 0.8664, loss:1.0998\n",
      "Epoch: 175, test AUC: 0.8252, test AP: 0.8107, train AUC: 0.8856, train AP: 0.8667, loss:1.1017\n",
      "Epoch: 176, test AUC: 0.8254, test AP: 0.8110, train AUC: 0.8858, train AP: 0.8669, loss:1.0968\n",
      "Epoch: 177, test AUC: 0.8256, test AP: 0.8112, train AUC: 0.8860, train AP: 0.8672, loss:1.0969\n",
      "Epoch: 178, test AUC: 0.8258, test AP: 0.8115, train AUC: 0.8862, train AP: 0.8674, loss:1.0956\n",
      "Epoch: 179, test AUC: 0.8259, test AP: 0.8117, train AUC: 0.8864, train AP: 0.8677, loss:1.0951\n",
      "Epoch: 180, test AUC: 0.8261, test AP: 0.8119, train AUC: 0.8866, train AP: 0.8679, loss:1.0942\n",
      "Epoch: 181, test AUC: 0.8262, test AP: 0.8121, train AUC: 0.8868, train AP: 0.8681, loss:1.0849\n",
      "Epoch: 182, test AUC: 0.8264, test AP: 0.8124, train AUC: 0.8870, train AP: 0.8684, loss:1.0926\n",
      "Epoch: 183, test AUC: 0.8265, test AP: 0.8126, train AUC: 0.8872, train AP: 0.8686, loss:1.0905\n",
      "Epoch: 184, test AUC: 0.8267, test AP: 0.8128, train AUC: 0.8874, train AP: 0.8688, loss:1.0921\n",
      "Epoch: 185, test AUC: 0.8268, test AP: 0.8130, train AUC: 0.8875, train AP: 0.8690, loss:1.0882\n",
      "Epoch: 186, test AUC: 0.8270, test AP: 0.8131, train AUC: 0.8877, train AP: 0.8692, loss:1.0880\n",
      "Epoch: 187, test AUC: 0.8271, test AP: 0.8133, train AUC: 0.8879, train AP: 0.8694, loss:1.0953\n",
      "Epoch: 188, test AUC: 0.8273, test AP: 0.8135, train AUC: 0.8881, train AP: 0.8697, loss:1.0811\n",
      "Epoch: 189, test AUC: 0.8274, test AP: 0.8137, train AUC: 0.8883, train AP: 0.8699, loss:1.0876\n",
      "Epoch: 190, test AUC: 0.8276, test AP: 0.8139, train AUC: 0.8885, train AP: 0.8701, loss:1.0877\n",
      "Epoch: 191, test AUC: 0.8277, test AP: 0.8141, train AUC: 0.8886, train AP: 0.8703, loss:1.0870\n",
      "Epoch: 192, test AUC: 0.8278, test AP: 0.8143, train AUC: 0.8888, train AP: 0.8705, loss:1.0887\n",
      "Epoch: 193, test AUC: 0.8280, test AP: 0.8145, train AUC: 0.8890, train AP: 0.8707, loss:1.0827\n",
      "Epoch: 194, test AUC: 0.8281, test AP: 0.8147, train AUC: 0.8891, train AP: 0.8709, loss:1.0884\n",
      "Epoch: 195, test AUC: 0.8283, test AP: 0.8149, train AUC: 0.8893, train AP: 0.8711, loss:1.0843\n",
      "Epoch: 196, test AUC: 0.8284, test AP: 0.8151, train AUC: 0.8894, train AP: 0.8713, loss:1.0909\n",
      "Epoch: 197, test AUC: 0.8285, test AP: 0.8153, train AUC: 0.8896, train AP: 0.8715, loss:1.0846\n",
      "Epoch: 198, test AUC: 0.8287, test AP: 0.8155, train AUC: 0.8898, train AP: 0.8717, loss:1.0808\n",
      "Epoch: 199, test AUC: 0.8288, test AP: 0.8157, train AUC: 0.8899, train AP: 0.8719, loss:1.0856\n",
      "Epoch: 200, test AUC: 0.8289, test AP: 0.8158, train AUC: 0.8901, train AP: 0.8721, loss:1.0885\n",
      "Epoch: 201, test AUC: 0.8291, test AP: 0.8160, train AUC: 0.8902, train AP: 0.8723, loss:1.0876\n",
      "Epoch: 202, test AUC: 0.8292, test AP: 0.8162, train AUC: 0.8904, train AP: 0.8725, loss:1.0857\n",
      "Epoch: 203, test AUC: 0.8293, test AP: 0.8164, train AUC: 0.8906, train AP: 0.8727, loss:1.0883\n",
      "Epoch: 204, test AUC: 0.8295, test AP: 0.8166, train AUC: 0.8908, train AP: 0.8729, loss:1.0871\n",
      "Epoch: 205, test AUC: 0.8296, test AP: 0.8168, train AUC: 0.8909, train AP: 0.8731, loss:1.0811\n",
      "Epoch: 206, test AUC: 0.8298, test AP: 0.8170, train AUC: 0.8911, train AP: 0.8734, loss:1.0880\n",
      "Epoch: 207, test AUC: 0.8299, test AP: 0.8172, train AUC: 0.8913, train AP: 0.8736, loss:1.0812\n",
      "Epoch: 208, test AUC: 0.8300, test AP: 0.8174, train AUC: 0.8915, train AP: 0.8738, loss:1.0877\n",
      "Epoch: 209, test AUC: 0.8302, test AP: 0.8176, train AUC: 0.8916, train AP: 0.8741, loss:1.0805\n",
      "Epoch: 210, test AUC: 0.8303, test AP: 0.8178, train AUC: 0.8918, train AP: 0.8743, loss:1.0865\n",
      "Epoch: 211, test AUC: 0.8304, test AP: 0.8181, train AUC: 0.8920, train AP: 0.8745, loss:1.0843\n",
      "Epoch: 212, test AUC: 0.8306, test AP: 0.8183, train AUC: 0.8921, train AP: 0.8747, loss:1.0887\n",
      "Epoch: 213, test AUC: 0.8307, test AP: 0.8184, train AUC: 0.8923, train AP: 0.8749, loss:1.0842\n",
      "Epoch: 214, test AUC: 0.8308, test AP: 0.8186, train AUC: 0.8925, train AP: 0.8751, loss:1.0866\n",
      "Epoch: 215, test AUC: 0.8309, test AP: 0.8188, train AUC: 0.8926, train AP: 0.8753, loss:1.0806\n",
      "Epoch: 216, test AUC: 0.8311, test AP: 0.8190, train AUC: 0.8928, train AP: 0.8755, loss:1.0819\n",
      "Epoch: 217, test AUC: 0.8312, test AP: 0.8192, train AUC: 0.8930, train AP: 0.8757, loss:1.0830\n",
      "Epoch: 218, test AUC: 0.8313, test AP: 0.8193, train AUC: 0.8931, train AP: 0.8759, loss:1.0852\n",
      "Epoch: 219, test AUC: 0.8314, test AP: 0.8195, train AUC: 0.8933, train AP: 0.8762, loss:1.0811\n",
      "Epoch: 220, test AUC: 0.8315, test AP: 0.8197, train AUC: 0.8935, train AP: 0.8764, loss:1.0815\n",
      "Epoch: 221, test AUC: 0.8317, test AP: 0.8199, train AUC: 0.8936, train AP: 0.8766, loss:1.0812\n",
      "Epoch: 222, test AUC: 0.8318, test AP: 0.8200, train AUC: 0.8938, train AP: 0.8767, loss:1.0765\n",
      "Epoch: 223, test AUC: 0.8319, test AP: 0.8202, train AUC: 0.8939, train AP: 0.8769, loss:1.0749\n",
      "Epoch: 224, test AUC: 0.8320, test AP: 0.8203, train AUC: 0.8941, train AP: 0.8771, loss:1.0822\n",
      "Epoch: 225, test AUC: 0.8321, test AP: 0.8205, train AUC: 0.8942, train AP: 0.8773, loss:1.0735\n",
      "Epoch: 226, test AUC: 0.8322, test AP: 0.8206, train AUC: 0.8944, train AP: 0.8775, loss:1.0760\n",
      "Epoch: 227, test AUC: 0.8324, test AP: 0.8208, train AUC: 0.8945, train AP: 0.8776, loss:1.0723\n",
      "Epoch: 228, test AUC: 0.8325, test AP: 0.8209, train AUC: 0.8947, train AP: 0.8778, loss:1.0784\n",
      "Epoch: 229, test AUC: 0.8326, test AP: 0.8211, train AUC: 0.8948, train AP: 0.8780, loss:1.0801\n",
      "Epoch: 230, test AUC: 0.8327, test AP: 0.8212, train AUC: 0.8950, train AP: 0.8782, loss:1.0731\n",
      "Epoch: 231, test AUC: 0.8329, test AP: 0.8214, train AUC: 0.8951, train AP: 0.8784, loss:1.0749\n",
      "Epoch: 232, test AUC: 0.8330, test AP: 0.8216, train AUC: 0.8953, train AP: 0.8786, loss:1.0802\n",
      "Epoch: 233, test AUC: 0.8331, test AP: 0.8217, train AUC: 0.8954, train AP: 0.8788, loss:1.0728\n",
      "Epoch: 234, test AUC: 0.8333, test AP: 0.8219, train AUC: 0.8956, train AP: 0.8790, loss:1.0800\n",
      "Epoch: 235, test AUC: 0.8334, test AP: 0.8221, train AUC: 0.8958, train AP: 0.8792, loss:1.0707\n",
      "Epoch: 236, test AUC: 0.8335, test AP: 0.8223, train AUC: 0.8959, train AP: 0.8794, loss:1.0845\n",
      "Epoch: 237, test AUC: 0.8337, test AP: 0.8226, train AUC: 0.8961, train AP: 0.8796, loss:1.0733\n",
      "Epoch: 238, test AUC: 0.8338, test AP: 0.8228, train AUC: 0.8962, train AP: 0.8799, loss:1.0770\n",
      "Epoch: 239, test AUC: 0.8339, test AP: 0.8230, train AUC: 0.8964, train AP: 0.8801, loss:1.0781\n",
      "Epoch: 240, test AUC: 0.8341, test AP: 0.8232, train AUC: 0.8966, train AP: 0.8803, loss:1.0730\n",
      "Epoch: 241, test AUC: 0.8342, test AP: 0.8234, train AUC: 0.8967, train AP: 0.8805, loss:1.0735\n",
      "Epoch: 242, test AUC: 0.8344, test AP: 0.8235, train AUC: 0.8969, train AP: 0.8807, loss:1.0648\n",
      "Epoch: 243, test AUC: 0.8345, test AP: 0.8237, train AUC: 0.8971, train AP: 0.8809, loss:1.0746\n",
      "Epoch: 244, test AUC: 0.8346, test AP: 0.8239, train AUC: 0.8972, train AP: 0.8811, loss:1.0609\n",
      "Epoch: 245, test AUC: 0.8347, test AP: 0.8240, train AUC: 0.8974, train AP: 0.8813, loss:1.0757\n",
      "Epoch: 246, test AUC: 0.8349, test AP: 0.8242, train AUC: 0.8975, train AP: 0.8815, loss:1.0727\n",
      "Epoch: 247, test AUC: 0.8350, test AP: 0.8243, train AUC: 0.8977, train AP: 0.8816, loss:1.0650\n",
      "Epoch: 248, test AUC: 0.8351, test AP: 0.8245, train AUC: 0.8978, train AP: 0.8818, loss:1.0767\n",
      "Epoch: 249, test AUC: 0.8352, test AP: 0.8247, train AUC: 0.8979, train AP: 0.8820, loss:1.0688\n",
      "Epoch: 250, test AUC: 0.8354, test AP: 0.8248, train AUC: 0.8981, train AP: 0.8822, loss:1.0667\n",
      "Epoch: 251, test AUC: 0.8355, test AP: 0.8249, train AUC: 0.8982, train AP: 0.8823, loss:1.0688\n",
      "Epoch: 252, test AUC: 0.8356, test AP: 0.8251, train AUC: 0.8984, train AP: 0.8825, loss:1.0756\n",
      "Epoch: 253, test AUC: 0.8357, test AP: 0.8252, train AUC: 0.8985, train AP: 0.8827, loss:1.0687\n",
      "Epoch: 254, test AUC: 0.8358, test AP: 0.8254, train AUC: 0.8987, train AP: 0.8829, loss:1.0730\n",
      "Epoch: 255, test AUC: 0.8359, test AP: 0.8255, train AUC: 0.8988, train AP: 0.8831, loss:1.0668\n",
      "Epoch: 256, test AUC: 0.8361, test AP: 0.8257, train AUC: 0.8990, train AP: 0.8832, loss:1.0730\n",
      "Epoch: 257, test AUC: 0.8362, test AP: 0.8258, train AUC: 0.8991, train AP: 0.8834, loss:1.0627\n",
      "Epoch: 258, test AUC: 0.8363, test AP: 0.8260, train AUC: 0.8993, train AP: 0.8836, loss:1.0642\n",
      "Epoch: 259, test AUC: 0.8364, test AP: 0.8262, train AUC: 0.8994, train AP: 0.8838, loss:1.0631\n",
      "Epoch: 260, test AUC: 0.8365, test AP: 0.8263, train AUC: 0.8995, train AP: 0.8839, loss:1.0680\n",
      "Epoch: 261, test AUC: 0.8367, test AP: 0.8265, train AUC: 0.8997, train AP: 0.8841, loss:1.0594\n",
      "Epoch: 262, test AUC: 0.8368, test AP: 0.8267, train AUC: 0.8998, train AP: 0.8842, loss:1.0586\n",
      "Epoch: 263, test AUC: 0.8369, test AP: 0.8269, train AUC: 0.8999, train AP: 0.8844, loss:1.0625\n",
      "Epoch: 264, test AUC: 0.8371, test AP: 0.8270, train AUC: 0.9000, train AP: 0.8846, loss:1.0641\n",
      "Epoch: 265, test AUC: 0.8372, test AP: 0.8272, train AUC: 0.9002, train AP: 0.8847, loss:1.0710\n",
      "Epoch: 266, test AUC: 0.8373, test AP: 0.8274, train AUC: 0.9003, train AP: 0.8849, loss:1.0624\n",
      "Epoch: 267, test AUC: 0.8374, test AP: 0.8275, train AUC: 0.9004, train AP: 0.8851, loss:1.0739\n",
      "Epoch: 268, test AUC: 0.8376, test AP: 0.8277, train AUC: 0.9006, train AP: 0.8852, loss:1.0644\n",
      "Epoch: 269, test AUC: 0.8377, test AP: 0.8278, train AUC: 0.9007, train AP: 0.8854, loss:1.0581\n",
      "Epoch: 270, test AUC: 0.8378, test AP: 0.8280, train AUC: 0.9008, train AP: 0.8855, loss:1.0648\n",
      "Epoch: 271, test AUC: 0.8379, test AP: 0.8282, train AUC: 0.9010, train AP: 0.8857, loss:1.0651\n",
      "Epoch: 272, test AUC: 0.8380, test AP: 0.8283, train AUC: 0.9011, train AP: 0.8859, loss:1.0678\n",
      "Epoch: 273, test AUC: 0.8382, test AP: 0.8285, train AUC: 0.9012, train AP: 0.8860, loss:1.0659\n",
      "Epoch: 274, test AUC: 0.8383, test AP: 0.8287, train AUC: 0.9014, train AP: 0.8862, loss:1.0683\n",
      "Epoch: 275, test AUC: 0.8384, test AP: 0.8288, train AUC: 0.9015, train AP: 0.8864, loss:1.0594\n",
      "Epoch: 276, test AUC: 0.8386, test AP: 0.8290, train AUC: 0.9017, train AP: 0.8866, loss:1.0661\n",
      "Epoch: 277, test AUC: 0.8387, test AP: 0.8292, train AUC: 0.9018, train AP: 0.8868, loss:1.0729\n",
      "Epoch: 278, test AUC: 0.8389, test AP: 0.8294, train AUC: 0.9020, train AP: 0.8869, loss:1.0612\n",
      "Epoch: 279, test AUC: 0.8390, test AP: 0.8296, train AUC: 0.9021, train AP: 0.8871, loss:1.0589\n",
      "Epoch: 280, test AUC: 0.8391, test AP: 0.8297, train AUC: 0.9023, train AP: 0.8873, loss:1.0693\n",
      "Epoch: 281, test AUC: 0.8393, test AP: 0.8299, train AUC: 0.9024, train AP: 0.8875, loss:1.0638\n",
      "Epoch: 282, test AUC: 0.8394, test AP: 0.8301, train AUC: 0.9026, train AP: 0.8877, loss:1.0721\n",
      "Epoch: 283, test AUC: 0.8396, test AP: 0.8303, train AUC: 0.9027, train AP: 0.8879, loss:1.0631\n",
      "Epoch: 284, test AUC: 0.8397, test AP: 0.8305, train AUC: 0.9029, train AP: 0.8881, loss:1.0562\n",
      "Epoch: 285, test AUC: 0.8399, test AP: 0.8307, train AUC: 0.9031, train AP: 0.8883, loss:1.0653\n",
      "Epoch: 286, test AUC: 0.8401, test AP: 0.8309, train AUC: 0.9032, train AP: 0.8885, loss:1.0588\n",
      "Epoch: 287, test AUC: 0.8402, test AP: 0.8311, train AUC: 0.9034, train AP: 0.8887, loss:1.0625\n",
      "Epoch: 288, test AUC: 0.8404, test AP: 0.8313, train AUC: 0.9036, train AP: 0.8889, loss:1.0585\n",
      "Epoch: 289, test AUC: 0.8405, test AP: 0.8315, train AUC: 0.9037, train AP: 0.8891, loss:1.0640\n",
      "Epoch: 290, test AUC: 0.8407, test AP: 0.8317, train AUC: 0.9039, train AP: 0.8893, loss:1.0629\n",
      "Epoch: 291, test AUC: 0.8408, test AP: 0.8319, train AUC: 0.9041, train AP: 0.8895, loss:1.0560\n",
      "Epoch: 292, test AUC: 0.8410, test AP: 0.8321, train AUC: 0.9042, train AP: 0.8897, loss:1.0595\n",
      "Epoch: 293, test AUC: 0.8411, test AP: 0.8322, train AUC: 0.9043, train AP: 0.8899, loss:1.0604\n",
      "Epoch: 294, test AUC: 0.8412, test AP: 0.8324, train AUC: 0.9045, train AP: 0.8901, loss:1.0642\n",
      "Epoch: 295, test AUC: 0.8413, test AP: 0.8326, train AUC: 0.9046, train AP: 0.8902, loss:1.0564\n",
      "Epoch: 296, test AUC: 0.8415, test AP: 0.8327, train AUC: 0.9048, train AP: 0.8904, loss:1.0625\n",
      "Epoch: 297, test AUC: 0.8416, test AP: 0.8329, train AUC: 0.9049, train AP: 0.8906, loss:1.0580\n",
      "Epoch: 298, test AUC: 0.8417, test AP: 0.8330, train AUC: 0.9051, train AP: 0.8908, loss:1.0611\n",
      "Epoch: 299, test AUC: 0.8418, test AP: 0.8332, train AUC: 0.9052, train AP: 0.8909, loss:1.0527\n",
      "Epoch: 300, test AUC: 0.8419, test AP: 0.8333, train AUC: 0.9053, train AP: 0.8911, loss:1.0514\n",
      "Epoch: 301, test AUC: 0.8420, test AP: 0.8334, train AUC: 0.9055, train AP: 0.8912, loss:1.0560\n",
      "Epoch: 302, test AUC: 0.8421, test AP: 0.8335, train AUC: 0.9056, train AP: 0.8913, loss:1.0496\n",
      "Epoch: 303, test AUC: 0.8422, test AP: 0.8336, train AUC: 0.9057, train AP: 0.8915, loss:1.0496\n",
      "Epoch: 304, test AUC: 0.8423, test AP: 0.8337, train AUC: 0.9058, train AP: 0.8916, loss:1.0583\n",
      "Epoch: 305, test AUC: 0.8424, test AP: 0.8339, train AUC: 0.9059, train AP: 0.8917, loss:1.0482\n",
      "Epoch: 306, test AUC: 0.8426, test AP: 0.8340, train AUC: 0.9061, train AP: 0.8919, loss:1.0556\n",
      "Epoch: 307, test AUC: 0.8427, test AP: 0.8342, train AUC: 0.9062, train AP: 0.8921, loss:1.0555\n",
      "Epoch: 308, test AUC: 0.8428, test AP: 0.8344, train AUC: 0.9064, train AP: 0.8923, loss:1.0487\n",
      "Epoch: 309, test AUC: 0.8430, test AP: 0.8345, train AUC: 0.9065, train AP: 0.8924, loss:1.0527\n",
      "Epoch: 310, test AUC: 0.8431, test AP: 0.8347, train AUC: 0.9067, train AP: 0.8926, loss:1.0468\n",
      "Epoch: 311, test AUC: 0.8433, test AP: 0.8349, train AUC: 0.9068, train AP: 0.8928, loss:1.0588\n",
      "Epoch: 312, test AUC: 0.8434, test AP: 0.8351, train AUC: 0.9069, train AP: 0.8930, loss:1.0497\n",
      "Epoch: 313, test AUC: 0.8435, test AP: 0.8352, train AUC: 0.9071, train AP: 0.8931, loss:1.0560\n",
      "Epoch: 314, test AUC: 0.8437, test AP: 0.8354, train AUC: 0.9072, train AP: 0.8933, loss:1.0569\n",
      "Epoch: 315, test AUC: 0.8438, test AP: 0.8355, train AUC: 0.9073, train AP: 0.8934, loss:1.0530\n",
      "Epoch: 316, test AUC: 0.8439, test AP: 0.8357, train AUC: 0.9075, train AP: 0.8936, loss:1.0501\n",
      "Epoch: 317, test AUC: 0.8441, test AP: 0.8358, train AUC: 0.9076, train AP: 0.8938, loss:1.0504\n",
      "Epoch: 318, test AUC: 0.8442, test AP: 0.8360, train AUC: 0.9077, train AP: 0.8939, loss:1.0511\n",
      "Epoch: 319, test AUC: 0.8443, test AP: 0.8361, train AUC: 0.9079, train AP: 0.8940, loss:1.0521\n",
      "Epoch: 320, test AUC: 0.8444, test AP: 0.8363, train AUC: 0.9080, train AP: 0.8942, loss:1.0562\n",
      "Epoch: 321, test AUC: 0.8446, test AP: 0.8365, train AUC: 0.9081, train AP: 0.8943, loss:1.0531\n",
      "Epoch: 322, test AUC: 0.8447, test AP: 0.8366, train AUC: 0.9082, train AP: 0.8945, loss:1.0531\n",
      "Epoch: 323, test AUC: 0.8448, test AP: 0.8367, train AUC: 0.9083, train AP: 0.8946, loss:1.0523\n",
      "Epoch: 324, test AUC: 0.8450, test AP: 0.8369, train AUC: 0.9085, train AP: 0.8948, loss:1.0532\n",
      "Epoch: 325, test AUC: 0.8451, test AP: 0.8370, train AUC: 0.9086, train AP: 0.8949, loss:1.0522\n",
      "Epoch: 326, test AUC: 0.8452, test AP: 0.8372, train AUC: 0.9087, train AP: 0.8951, loss:1.0537\n",
      "Epoch: 327, test AUC: 0.8454, test AP: 0.8374, train AUC: 0.9089, train AP: 0.8952, loss:1.0478\n",
      "Epoch: 328, test AUC: 0.8455, test AP: 0.8375, train AUC: 0.9090, train AP: 0.8954, loss:1.0506\n",
      "Epoch: 329, test AUC: 0.8456, test AP: 0.8377, train AUC: 0.9091, train AP: 0.8955, loss:1.0519\n",
      "Epoch: 330, test AUC: 0.8458, test AP: 0.8378, train AUC: 0.9092, train AP: 0.8957, loss:1.0452\n",
      "Epoch: 331, test AUC: 0.8459, test AP: 0.8380, train AUC: 0.9094, train AP: 0.8959, loss:1.0449\n",
      "Epoch: 332, test AUC: 0.8461, test AP: 0.8382, train AUC: 0.9095, train AP: 0.8960, loss:1.0534\n",
      "Epoch: 333, test AUC: 0.8462, test AP: 0.8383, train AUC: 0.9097, train AP: 0.8962, loss:1.0569\n",
      "Epoch: 334, test AUC: 0.8463, test AP: 0.8385, train AUC: 0.9098, train AP: 0.8964, loss:1.0511\n",
      "Epoch: 335, test AUC: 0.8464, test AP: 0.8386, train AUC: 0.9099, train AP: 0.8966, loss:1.0464\n",
      "Epoch: 336, test AUC: 0.8466, test AP: 0.8388, train AUC: 0.9101, train AP: 0.8967, loss:1.0547\n",
      "Epoch: 337, test AUC: 0.8467, test AP: 0.8389, train AUC: 0.9102, train AP: 0.8969, loss:1.0496\n",
      "Epoch: 338, test AUC: 0.8468, test AP: 0.8391, train AUC: 0.9104, train AP: 0.8970, loss:1.0561\n",
      "Epoch: 339, test AUC: 0.8469, test AP: 0.8392, train AUC: 0.9105, train AP: 0.8972, loss:1.0474\n",
      "Epoch: 340, test AUC: 0.8470, test AP: 0.8393, train AUC: 0.9106, train AP: 0.8974, loss:1.0484\n",
      "Epoch: 341, test AUC: 0.8471, test AP: 0.8395, train AUC: 0.9108, train AP: 0.8975, loss:1.0491\n",
      "Epoch: 342, test AUC: 0.8473, test AP: 0.8396, train AUC: 0.9109, train AP: 0.8977, loss:1.0538\n",
      "Epoch: 343, test AUC: 0.8474, test AP: 0.8398, train AUC: 0.9111, train AP: 0.8979, loss:1.0488\n",
      "Epoch: 344, test AUC: 0.8475, test AP: 0.8400, train AUC: 0.9112, train AP: 0.8980, loss:1.0471\n",
      "Epoch: 345, test AUC: 0.8477, test AP: 0.8402, train AUC: 0.9113, train AP: 0.8982, loss:1.0407\n",
      "Epoch: 346, test AUC: 0.8478, test AP: 0.8404, train AUC: 0.9115, train AP: 0.8984, loss:1.0425\n",
      "Epoch: 347, test AUC: 0.8479, test AP: 0.8405, train AUC: 0.9116, train AP: 0.8985, loss:1.0387\n",
      "Epoch: 348, test AUC: 0.8481, test AP: 0.8407, train AUC: 0.9117, train AP: 0.8987, loss:1.0471\n",
      "Epoch: 349, test AUC: 0.8482, test AP: 0.8409, train AUC: 0.9119, train AP: 0.8989, loss:1.0454\n",
      "Epoch: 350, test AUC: 0.8483, test AP: 0.8410, train AUC: 0.9120, train AP: 0.8990, loss:1.0472\n",
      "Epoch: 351, test AUC: 0.8485, test AP: 0.8412, train AUC: 0.9122, train AP: 0.8992, loss:1.0480\n",
      "Epoch: 352, test AUC: 0.8486, test AP: 0.8414, train AUC: 0.9123, train AP: 0.8994, loss:1.0418\n",
      "Epoch: 353, test AUC: 0.8488, test AP: 0.8415, train AUC: 0.9124, train AP: 0.8995, loss:1.0397\n",
      "Epoch: 354, test AUC: 0.8489, test AP: 0.8416, train AUC: 0.9126, train AP: 0.8996, loss:1.0457\n",
      "Epoch: 355, test AUC: 0.8490, test AP: 0.8417, train AUC: 0.9127, train AP: 0.8998, loss:1.0407\n",
      "Epoch: 356, test AUC: 0.8491, test AP: 0.8418, train AUC: 0.9128, train AP: 0.8999, loss:1.0439\n",
      "Epoch: 357, test AUC: 0.8492, test AP: 0.8419, train AUC: 0.9129, train AP: 0.9000, loss:1.0453\n",
      "Epoch: 358, test AUC: 0.8493, test AP: 0.8420, train AUC: 0.9130, train AP: 0.9001, loss:1.0402\n",
      "Epoch: 359, test AUC: 0.8494, test AP: 0.8422, train AUC: 0.9131, train AP: 0.9003, loss:1.0409\n",
      "Epoch: 360, test AUC: 0.8496, test AP: 0.8423, train AUC: 0.9132, train AP: 0.9004, loss:1.0433\n",
      "Epoch: 361, test AUC: 0.8497, test AP: 0.8424, train AUC: 0.9133, train AP: 0.9005, loss:1.0383\n",
      "Epoch: 362, test AUC: 0.8498, test AP: 0.8425, train AUC: 0.9134, train AP: 0.9006, loss:1.0424\n",
      "Epoch: 363, test AUC: 0.8499, test AP: 0.8426, train AUC: 0.9135, train AP: 0.9007, loss:1.0371\n",
      "Epoch: 364, test AUC: 0.8500, test AP: 0.8428, train AUC: 0.9136, train AP: 0.9009, loss:1.0406\n",
      "Epoch: 365, test AUC: 0.8501, test AP: 0.8429, train AUC: 0.9138, train AP: 0.9010, loss:1.0420\n",
      "Epoch: 366, test AUC: 0.8502, test AP: 0.8430, train AUC: 0.9139, train AP: 0.9011, loss:1.0354\n",
      "Epoch: 367, test AUC: 0.8503, test AP: 0.8432, train AUC: 0.9140, train AP: 0.9013, loss:1.0397\n",
      "Epoch: 368, test AUC: 0.8505, test AP: 0.8433, train AUC: 0.9141, train AP: 0.9014, loss:1.0427\n",
      "Epoch: 369, test AUC: 0.8506, test AP: 0.8435, train AUC: 0.9143, train AP: 0.9016, loss:1.0409\n",
      "Epoch: 370, test AUC: 0.8507, test AP: 0.8436, train AUC: 0.9144, train AP: 0.9017, loss:1.0388\n",
      "Epoch: 371, test AUC: 0.8507, test AP: 0.8437, train AUC: 0.9145, train AP: 0.9018, loss:1.0383\n",
      "Epoch: 372, test AUC: 0.8508, test AP: 0.8438, train AUC: 0.9146, train AP: 0.9019, loss:1.0427\n",
      "Epoch: 373, test AUC: 0.8509, test AP: 0.8439, train AUC: 0.9147, train AP: 0.9020, loss:1.0353\n",
      "Epoch: 374, test AUC: 0.8510, test AP: 0.8440, train AUC: 0.9148, train AP: 0.9022, loss:1.0358\n",
      "Epoch: 375, test AUC: 0.8511, test AP: 0.8442, train AUC: 0.9149, train AP: 0.9023, loss:1.0339\n",
      "Epoch: 376, test AUC: 0.8512, test AP: 0.8443, train AUC: 0.9150, train AP: 0.9024, loss:1.0363\n",
      "Epoch: 377, test AUC: 0.8513, test AP: 0.8444, train AUC: 0.9151, train AP: 0.9025, loss:1.0348\n",
      "Epoch: 378, test AUC: 0.8514, test AP: 0.8445, train AUC: 0.9152, train AP: 0.9026, loss:1.0327\n",
      "Epoch: 379, test AUC: 0.8515, test AP: 0.8446, train AUC: 0.9153, train AP: 0.9028, loss:1.0336\n",
      "Epoch: 380, test AUC: 0.8516, test AP: 0.8448, train AUC: 0.9154, train AP: 0.9029, loss:1.0395\n",
      "Epoch: 381, test AUC: 0.8517, test AP: 0.8449, train AUC: 0.9155, train AP: 0.9030, loss:1.0334\n",
      "Epoch: 382, test AUC: 0.8518, test AP: 0.8450, train AUC: 0.9156, train AP: 0.9031, loss:1.0287\n",
      "Epoch: 383, test AUC: 0.8519, test AP: 0.8451, train AUC: 0.9157, train AP: 0.9033, loss:1.0405\n",
      "Epoch: 384, test AUC: 0.8520, test AP: 0.8452, train AUC: 0.9158, train AP: 0.9034, loss:1.0360\n",
      "Epoch: 385, test AUC: 0.8521, test AP: 0.8453, train AUC: 0.9159, train AP: 0.9035, loss:1.0392\n",
      "Epoch: 386, test AUC: 0.8522, test AP: 0.8455, train AUC: 0.9160, train AP: 0.9036, loss:1.0391\n",
      "Epoch: 387, test AUC: 0.8523, test AP: 0.8456, train AUC: 0.9161, train AP: 0.9037, loss:1.0354\n",
      "Epoch: 388, test AUC: 0.8524, test AP: 0.8457, train AUC: 0.9162, train AP: 0.9038, loss:1.0321\n",
      "Epoch: 389, test AUC: 0.8525, test AP: 0.8457, train AUC: 0.9163, train AP: 0.9038, loss:1.0363\n",
      "Epoch: 390, test AUC: 0.8525, test AP: 0.8458, train AUC: 0.9163, train AP: 0.9039, loss:1.0361\n",
      "Epoch: 391, test AUC: 0.8526, test AP: 0.8459, train AUC: 0.9164, train AP: 0.9040, loss:1.0354\n",
      "Epoch: 392, test AUC: 0.8527, test AP: 0.8460, train AUC: 0.9165, train AP: 0.9041, loss:1.0324\n",
      "Epoch: 393, test AUC: 0.8528, test AP: 0.8460, train AUC: 0.9166, train AP: 0.9042, loss:1.0332\n",
      "Epoch: 394, test AUC: 0.8529, test AP: 0.8461, train AUC: 0.9167, train AP: 0.9043, loss:1.0319\n",
      "Epoch: 395, test AUC: 0.8529, test AP: 0.8462, train AUC: 0.9168, train AP: 0.9044, loss:1.0351\n",
      "Epoch: 396, test AUC: 0.8530, test AP: 0.8463, train AUC: 0.9169, train AP: 0.9046, loss:1.0328\n",
      "Epoch: 397, test AUC: 0.8531, test AP: 0.8464, train AUC: 0.9170, train AP: 0.9047, loss:1.0363\n",
      "Epoch: 398, test AUC: 0.8532, test AP: 0.8466, train AUC: 0.9172, train AP: 0.9048, loss:1.0336\n",
      "Epoch: 399, test AUC: 0.8533, test AP: 0.8467, train AUC: 0.9173, train AP: 0.9049, loss:1.0366\n",
      "Epoch: 400, test AUC: 0.8534, test AP: 0.8468, train AUC: 0.9174, train AP: 0.9051, loss:1.0364\n",
      "Epoch: 401, test AUC: 0.8536, test AP: 0.8469, train AUC: 0.9175, train AP: 0.9052, loss:1.0355\n",
      "Epoch: 402, test AUC: 0.8537, test AP: 0.8471, train AUC: 0.9176, train AP: 0.9053, loss:1.0319\n",
      "Epoch: 403, test AUC: 0.8537, test AP: 0.8472, train AUC: 0.9177, train AP: 0.9055, loss:1.0282\n",
      "Epoch: 404, test AUC: 0.8538, test AP: 0.8473, train AUC: 0.9178, train AP: 0.9056, loss:1.0343\n",
      "Epoch: 405, test AUC: 0.8539, test AP: 0.8474, train AUC: 0.9179, train AP: 0.9057, loss:1.0342\n",
      "Epoch: 406, test AUC: 0.8540, test AP: 0.8475, train AUC: 0.9179, train AP: 0.9058, loss:1.0236\n",
      "Epoch: 407, test AUC: 0.8541, test AP: 0.8476, train AUC: 0.9180, train AP: 0.9058, loss:1.0297\n",
      "Epoch: 408, test AUC: 0.8542, test AP: 0.8477, train AUC: 0.9181, train AP: 0.9059, loss:1.0364\n",
      "Epoch: 409, test AUC: 0.8542, test AP: 0.8478, train AUC: 0.9182, train AP: 0.9060, loss:1.0244\n",
      "Epoch: 410, test AUC: 0.8543, test AP: 0.8479, train AUC: 0.9183, train AP: 0.9062, loss:1.0327\n",
      "Epoch: 411, test AUC: 0.8544, test AP: 0.8480, train AUC: 0.9184, train AP: 0.9063, loss:1.0315\n",
      "Epoch: 412, test AUC: 0.8545, test AP: 0.8481, train AUC: 0.9185, train AP: 0.9064, loss:1.0350\n",
      "Epoch: 413, test AUC: 0.8546, test AP: 0.8482, train AUC: 0.9186, train AP: 0.9065, loss:1.0362\n",
      "Epoch: 414, test AUC: 0.8547, test AP: 0.8483, train AUC: 0.9187, train AP: 0.9066, loss:1.0294\n",
      "Epoch: 415, test AUC: 0.8548, test AP: 0.8484, train AUC: 0.9188, train AP: 0.9067, loss:1.0310\n",
      "Epoch: 416, test AUC: 0.8549, test AP: 0.8485, train AUC: 0.9189, train AP: 0.9068, loss:1.0304\n",
      "Epoch: 417, test AUC: 0.8550, test AP: 0.8486, train AUC: 0.9190, train AP: 0.9070, loss:1.0288\n",
      "Epoch: 418, test AUC: 0.8551, test AP: 0.8487, train AUC: 0.9191, train AP: 0.9071, loss:1.0287\n",
      "Epoch: 419, test AUC: 0.8552, test AP: 0.8489, train AUC: 0.9192, train AP: 0.9072, loss:1.0327\n",
      "Epoch: 420, test AUC: 0.8553, test AP: 0.8490, train AUC: 0.9194, train AP: 0.9073, loss:1.0220\n",
      "Epoch: 421, test AUC: 0.8555, test AP: 0.8492, train AUC: 0.9195, train AP: 0.9074, loss:1.0309\n",
      "Epoch: 422, test AUC: 0.8556, test AP: 0.8493, train AUC: 0.9196, train AP: 0.9076, loss:1.0227\n",
      "Epoch: 423, test AUC: 0.8557, test AP: 0.8494, train AUC: 0.9197, train AP: 0.9077, loss:1.0233\n",
      "Epoch: 424, test AUC: 0.8558, test AP: 0.8496, train AUC: 0.9197, train AP: 0.9078, loss:1.0366\n",
      "Epoch: 425, test AUC: 0.8559, test AP: 0.8497, train AUC: 0.9198, train AP: 0.9079, loss:1.0262\n",
      "Epoch: 426, test AUC: 0.8560, test AP: 0.8498, train AUC: 0.9199, train AP: 0.9080, loss:1.0261\n",
      "Epoch: 427, test AUC: 0.8560, test AP: 0.8498, train AUC: 0.9200, train AP: 0.9081, loss:1.0249\n",
      "Epoch: 428, test AUC: 0.8561, test AP: 0.8499, train AUC: 0.9201, train AP: 0.9082, loss:1.0237\n",
      "Epoch: 429, test AUC: 0.8562, test AP: 0.8500, train AUC: 0.9202, train AP: 0.9083, loss:1.0348\n",
      "Epoch: 430, test AUC: 0.8562, test AP: 0.8501, train AUC: 0.9203, train AP: 0.9084, loss:1.0280\n",
      "Epoch: 431, test AUC: 0.8563, test AP: 0.8502, train AUC: 0.9204, train AP: 0.9085, loss:1.0310\n",
      "Epoch: 432, test AUC: 0.8564, test AP: 0.8503, train AUC: 0.9205, train AP: 0.9086, loss:1.0346\n",
      "Epoch: 433, test AUC: 0.8565, test AP: 0.8504, train AUC: 0.9206, train AP: 0.9088, loss:1.0325\n",
      "Epoch: 434, test AUC: 0.8566, test AP: 0.8506, train AUC: 0.9207, train AP: 0.9089, loss:1.0219\n",
      "Epoch: 435, test AUC: 0.8568, test AP: 0.8507, train AUC: 0.9209, train AP: 0.9090, loss:1.0285\n",
      "Epoch: 436, test AUC: 0.8569, test AP: 0.8509, train AUC: 0.9210, train AP: 0.9092, loss:1.0150\n",
      "Epoch: 437, test AUC: 0.8570, test AP: 0.8510, train AUC: 0.9211, train AP: 0.9093, loss:1.0270\n",
      "Epoch: 438, test AUC: 0.8572, test AP: 0.8512, train AUC: 0.9212, train AP: 0.9094, loss:1.0272\n",
      "Epoch: 439, test AUC: 0.8573, test AP: 0.8514, train AUC: 0.9213, train AP: 0.9096, loss:1.0246\n",
      "Epoch: 440, test AUC: 0.8575, test AP: 0.8515, train AUC: 0.9214, train AP: 0.9097, loss:1.0280\n",
      "Epoch: 441, test AUC: 0.8576, test AP: 0.8516, train AUC: 0.9215, train AP: 0.9098, loss:1.0280\n",
      "Epoch: 442, test AUC: 0.8577, test AP: 0.8517, train AUC: 0.9216, train AP: 0.9099, loss:1.0291\n",
      "Epoch: 443, test AUC: 0.8577, test AP: 0.8518, train AUC: 0.9217, train AP: 0.9099, loss:1.0257\n",
      "Epoch: 444, test AUC: 0.8578, test AP: 0.8519, train AUC: 0.9217, train AP: 0.9100, loss:1.0270\n",
      "Epoch: 445, test AUC: 0.8579, test AP: 0.8519, train AUC: 0.9218, train AP: 0.9101, loss:1.0330\n",
      "Epoch: 446, test AUC: 0.8579, test AP: 0.8520, train AUC: 0.9219, train AP: 0.9102, loss:1.0262\n",
      "Epoch: 447, test AUC: 0.8580, test AP: 0.8520, train AUC: 0.9219, train AP: 0.9102, loss:1.0251\n",
      "Epoch: 448, test AUC: 0.8580, test AP: 0.8520, train AUC: 0.9220, train AP: 0.9103, loss:1.0231\n",
      "Epoch: 449, test AUC: 0.8580, test AP: 0.8521, train AUC: 0.9220, train AP: 0.9103, loss:1.0252\n",
      "Epoch: 450, test AUC: 0.8581, test AP: 0.8521, train AUC: 0.9221, train AP: 0.9104, loss:1.0282\n",
      "Epoch: 451, test AUC: 0.8581, test AP: 0.8522, train AUC: 0.9222, train AP: 0.9104, loss:1.0232\n",
      "Epoch: 452, test AUC: 0.8582, test AP: 0.8522, train AUC: 0.9222, train AP: 0.9105, loss:1.0276\n",
      "Epoch: 453, test AUC: 0.8583, test AP: 0.8523, train AUC: 0.9223, train AP: 0.9106, loss:1.0224\n",
      "Epoch: 454, test AUC: 0.8584, test AP: 0.8524, train AUC: 0.9224, train AP: 0.9107, loss:1.0264\n",
      "Epoch: 455, test AUC: 0.8585, test AP: 0.8526, train AUC: 0.9225, train AP: 0.9108, loss:1.0212\n",
      "Epoch: 456, test AUC: 0.8586, test AP: 0.8527, train AUC: 0.9226, train AP: 0.9110, loss:1.0275\n",
      "Epoch: 457, test AUC: 0.8588, test AP: 0.8529, train AUC: 0.9227, train AP: 0.9111, loss:1.0173\n",
      "Epoch: 458, test AUC: 0.8589, test AP: 0.8530, train AUC: 0.9228, train AP: 0.9112, loss:1.0149\n",
      "Epoch: 459, test AUC: 0.8591, test AP: 0.8532, train AUC: 0.9229, train AP: 0.9113, loss:1.0212\n",
      "Epoch: 460, test AUC: 0.8592, test AP: 0.8534, train AUC: 0.9230, train AP: 0.9115, loss:1.0156\n",
      "Epoch: 461, test AUC: 0.8594, test AP: 0.8535, train AUC: 0.9231, train AP: 0.9116, loss:1.0135\n",
      "Epoch: 462, test AUC: 0.8595, test AP: 0.8536, train AUC: 0.9232, train AP: 0.9117, loss:1.0237\n",
      "Epoch: 463, test AUC: 0.8596, test AP: 0.8538, train AUC: 0.9233, train AP: 0.9118, loss:1.0240\n",
      "Epoch: 464, test AUC: 0.8597, test AP: 0.8539, train AUC: 0.9234, train AP: 0.9119, loss:1.0252\n",
      "Epoch: 465, test AUC: 0.8598, test AP: 0.8540, train AUC: 0.9235, train AP: 0.9120, loss:1.0257\n",
      "Epoch: 466, test AUC: 0.8599, test AP: 0.8541, train AUC: 0.9236, train AP: 0.9121, loss:1.0229\n",
      "Epoch: 467, test AUC: 0.8601, test AP: 0.8542, train AUC: 0.9237, train AP: 0.9122, loss:1.0237\n",
      "Epoch: 468, test AUC: 0.8602, test AP: 0.8543, train AUC: 0.9237, train AP: 0.9123, loss:1.0173\n",
      "Epoch: 469, test AUC: 0.8603, test AP: 0.8545, train AUC: 0.9238, train AP: 0.9124, loss:1.0278\n",
      "Epoch: 470, test AUC: 0.8604, test AP: 0.8546, train AUC: 0.9239, train AP: 0.9125, loss:1.0167\n",
      "Epoch: 471, test AUC: 0.8605, test AP: 0.8547, train AUC: 0.9240, train AP: 0.9126, loss:1.0151\n",
      "Epoch: 472, test AUC: 0.8606, test AP: 0.8548, train AUC: 0.9241, train AP: 0.9127, loss:1.0255\n",
      "Epoch: 473, test AUC: 0.8607, test AP: 0.8549, train AUC: 0.9241, train AP: 0.9128, loss:1.0216\n",
      "Epoch: 474, test AUC: 0.8609, test AP: 0.8550, train AUC: 0.9242, train AP: 0.9128, loss:1.0175\n",
      "Epoch: 475, test AUC: 0.8610, test AP: 0.8552, train AUC: 0.9243, train AP: 0.9129, loss:1.0216\n",
      "Epoch: 476, test AUC: 0.8612, test AP: 0.8553, train AUC: 0.9244, train AP: 0.9130, loss:1.0238\n",
      "Epoch: 477, test AUC: 0.8613, test AP: 0.8554, train AUC: 0.9245, train AP: 0.9131, loss:1.0229\n",
      "Epoch: 478, test AUC: 0.8614, test AP: 0.8556, train AUC: 0.9245, train AP: 0.9132, loss:1.0255\n",
      "Epoch: 479, test AUC: 0.8615, test AP: 0.8557, train AUC: 0.9246, train AP: 0.9133, loss:1.0268\n",
      "Epoch: 480, test AUC: 0.8617, test AP: 0.8558, train AUC: 0.9247, train AP: 0.9134, loss:1.0157\n",
      "Epoch: 481, test AUC: 0.8618, test AP: 0.8560, train AUC: 0.9248, train AP: 0.9135, loss:1.0198\n",
      "Epoch: 482, test AUC: 0.8619, test AP: 0.8561, train AUC: 0.9248, train AP: 0.9136, loss:1.0195\n",
      "Epoch: 483, test AUC: 0.8620, test AP: 0.8562, train AUC: 0.9249, train AP: 0.9136, loss:1.0150\n",
      "Epoch: 484, test AUC: 0.8621, test AP: 0.8563, train AUC: 0.9250, train AP: 0.9137, loss:1.0223\n",
      "Epoch: 485, test AUC: 0.8622, test AP: 0.8563, train AUC: 0.9250, train AP: 0.9138, loss:1.0187\n",
      "Epoch: 486, test AUC: 0.8622, test AP: 0.8564, train AUC: 0.9251, train AP: 0.9138, loss:1.0144\n",
      "Epoch: 487, test AUC: 0.8623, test AP: 0.8565, train AUC: 0.9251, train AP: 0.9139, loss:1.0153\n",
      "Epoch: 488, test AUC: 0.8624, test AP: 0.8566, train AUC: 0.9252, train AP: 0.9140, loss:1.0174\n",
      "Epoch: 489, test AUC: 0.8625, test AP: 0.8567, train AUC: 0.9252, train AP: 0.9140, loss:1.0155\n",
      "Epoch: 490, test AUC: 0.8626, test AP: 0.8568, train AUC: 0.9253, train AP: 0.9141, loss:1.0178\n",
      "Epoch: 491, test AUC: 0.8626, test AP: 0.8568, train AUC: 0.9254, train AP: 0.9142, loss:1.0169\n",
      "Epoch: 492, test AUC: 0.8627, test AP: 0.8569, train AUC: 0.9254, train AP: 0.9142, loss:1.0131\n",
      "Epoch: 493, test AUC: 0.8628, test AP: 0.8570, train AUC: 0.9255, train AP: 0.9143, loss:1.0089\n",
      "Epoch: 494, test AUC: 0.8629, test AP: 0.8571, train AUC: 0.9256, train AP: 0.9144, loss:1.0171\n",
      "Epoch: 495, test AUC: 0.8631, test AP: 0.8573, train AUC: 0.9257, train AP: 0.9145, loss:1.0116\n",
      "Epoch: 496, test AUC: 0.8633, test AP: 0.8575, train AUC: 0.9258, train AP: 0.9147, loss:1.0202\n",
      "Epoch: 497, test AUC: 0.8634, test AP: 0.8576, train AUC: 0.9259, train AP: 0.9148, loss:1.0183\n",
      "Epoch: 498, test AUC: 0.8635, test AP: 0.8578, train AUC: 0.9261, train AP: 0.9150, loss:1.0136\n",
      "Epoch: 499, test AUC: 0.8637, test AP: 0.8580, train AUC: 0.9262, train AP: 0.9151, loss:1.0187\n",
      "Epoch: 500, test AUC: 0.8638, test AP: 0.8581, train AUC: 0.9263, train AP: 0.9153, loss:1.0129\n",
      "Epoch: 501, test AUC: 0.8640, test AP: 0.8583, train AUC: 0.9264, train AP: 0.9154, loss:1.0148\n",
      "Epoch: 502, test AUC: 0.8641, test AP: 0.8584, train AUC: 0.9265, train AP: 0.9155, loss:1.0106\n",
      "Epoch: 503, test AUC: 0.8642, test AP: 0.8585, train AUC: 0.9266, train AP: 0.9156, loss:1.0110\n",
      "Epoch: 504, test AUC: 0.8643, test AP: 0.8586, train AUC: 0.9267, train AP: 0.9157, loss:1.0150\n",
      "Epoch: 505, test AUC: 0.8644, test AP: 0.8587, train AUC: 0.9267, train AP: 0.9158, loss:1.0089\n",
      "Epoch: 506, test AUC: 0.8645, test AP: 0.8588, train AUC: 0.9268, train AP: 0.9159, loss:1.0137\n",
      "Epoch: 507, test AUC: 0.8645, test AP: 0.8589, train AUC: 0.9269, train AP: 0.9160, loss:1.0112\n",
      "Epoch: 508, test AUC: 0.8646, test AP: 0.8590, train AUC: 0.9269, train AP: 0.9160, loss:1.0079\n",
      "Epoch: 509, test AUC: 0.8647, test AP: 0.8591, train AUC: 0.9270, train AP: 0.9161, loss:1.0214\n",
      "Epoch: 510, test AUC: 0.8648, test AP: 0.8592, train AUC: 0.9270, train AP: 0.9161, loss:1.0067\n",
      "Epoch: 511, test AUC: 0.8649, test AP: 0.8593, train AUC: 0.9271, train AP: 0.9162, loss:1.0201\n",
      "Epoch: 512, test AUC: 0.8650, test AP: 0.8594, train AUC: 0.9271, train AP: 0.9163, loss:1.0116\n",
      "Epoch: 513, test AUC: 0.8651, test AP: 0.8595, train AUC: 0.9272, train AP: 0.9163, loss:1.0120\n",
      "Epoch: 514, test AUC: 0.8652, test AP: 0.8595, train AUC: 0.9272, train AP: 0.9164, loss:1.0130\n",
      "Epoch: 515, test AUC: 0.8652, test AP: 0.8596, train AUC: 0.9273, train AP: 0.9165, loss:1.0129\n",
      "Epoch: 516, test AUC: 0.8653, test AP: 0.8597, train AUC: 0.9274, train AP: 0.9165, loss:1.0095\n",
      "Epoch: 517, test AUC: 0.8654, test AP: 0.8598, train AUC: 0.9275, train AP: 0.9166, loss:1.0143\n",
      "Epoch: 518, test AUC: 0.8656, test AP: 0.8599, train AUC: 0.9276, train AP: 0.9167, loss:1.0143\n",
      "Epoch: 519, test AUC: 0.8657, test AP: 0.8601, train AUC: 0.9276, train AP: 0.9168, loss:1.0131\n",
      "Epoch: 520, test AUC: 0.8658, test AP: 0.8602, train AUC: 0.9277, train AP: 0.9169, loss:1.0114\n",
      "Epoch: 521, test AUC: 0.8659, test AP: 0.8603, train AUC: 0.9278, train AP: 0.9170, loss:1.0098\n",
      "Epoch: 522, test AUC: 0.8660, test AP: 0.8604, train AUC: 0.9279, train AP: 0.9170, loss:1.0095\n",
      "Epoch: 523, test AUC: 0.8661, test AP: 0.8605, train AUC: 0.9279, train AP: 0.9171, loss:1.0145\n",
      "Epoch: 524, test AUC: 0.8662, test AP: 0.8607, train AUC: 0.9280, train AP: 0.9172, loss:1.0072\n",
      "Epoch: 525, test AUC: 0.8664, test AP: 0.8608, train AUC: 0.9281, train AP: 0.9173, loss:1.0148\n",
      "Epoch: 526, test AUC: 0.8665, test AP: 0.8609, train AUC: 0.9282, train AP: 0.9174, loss:1.0091\n",
      "Epoch: 527, test AUC: 0.8666, test AP: 0.8610, train AUC: 0.9283, train AP: 0.9175, loss:1.0085\n",
      "Epoch: 528, test AUC: 0.8667, test AP: 0.8611, train AUC: 0.9283, train AP: 0.9176, loss:1.0125\n",
      "Epoch: 529, test AUC: 0.8668, test AP: 0.8612, train AUC: 0.9284, train AP: 0.9176, loss:1.0113\n",
      "Epoch: 530, test AUC: 0.8669, test AP: 0.8613, train AUC: 0.9285, train AP: 0.9177, loss:1.0116\n",
      "Epoch: 531, test AUC: 0.8669, test AP: 0.8614, train AUC: 0.9285, train AP: 0.9178, loss:1.0193\n",
      "Epoch: 532, test AUC: 0.8670, test AP: 0.8615, train AUC: 0.9286, train AP: 0.9178, loss:1.0069\n",
      "Epoch: 533, test AUC: 0.8670, test AP: 0.8615, train AUC: 0.9287, train AP: 0.9179, loss:1.0019\n",
      "Epoch: 534, test AUC: 0.8671, test AP: 0.8616, train AUC: 0.9287, train AP: 0.9180, loss:1.0091\n",
      "Epoch: 535, test AUC: 0.8671, test AP: 0.8616, train AUC: 0.9288, train AP: 0.9180, loss:1.0039\n",
      "Epoch: 536, test AUC: 0.8672, test AP: 0.8617, train AUC: 0.9289, train AP: 0.9181, loss:1.0218\n",
      "Epoch: 537, test AUC: 0.8672, test AP: 0.8617, train AUC: 0.9289, train AP: 0.9182, loss:1.0053\n",
      "Epoch: 538, test AUC: 0.8673, test AP: 0.8618, train AUC: 0.9290, train AP: 0.9183, loss:1.0022\n",
      "Epoch: 539, test AUC: 0.8675, test AP: 0.8619, train AUC: 0.9291, train AP: 0.9184, loss:1.0018\n",
      "Epoch: 540, test AUC: 0.8676, test AP: 0.8621, train AUC: 0.9292, train AP: 0.9185, loss:1.0123\n",
      "Epoch: 541, test AUC: 0.8677, test AP: 0.8623, train AUC: 0.9293, train AP: 0.9186, loss:1.0163\n",
      "Epoch: 542, test AUC: 0.8679, test AP: 0.8624, train AUC: 0.9294, train AP: 0.9187, loss:1.0085\n",
      "Epoch: 543, test AUC: 0.8680, test AP: 0.8625, train AUC: 0.9295, train AP: 0.9188, loss:1.0061\n",
      "Epoch: 544, test AUC: 0.8681, test AP: 0.8627, train AUC: 0.9296, train AP: 0.9189, loss:1.0131\n",
      "Epoch: 545, test AUC: 0.8682, test AP: 0.8628, train AUC: 0.9297, train AP: 0.9190, loss:1.0089\n",
      "Epoch: 546, test AUC: 0.8683, test AP: 0.8629, train AUC: 0.9298, train AP: 0.9191, loss:0.9991\n",
      "Epoch: 547, test AUC: 0.8685, test AP: 0.8631, train AUC: 0.9299, train AP: 0.9192, loss:1.0083\n",
      "Epoch: 548, test AUC: 0.8686, test AP: 0.8633, train AUC: 0.9300, train AP: 0.9194, loss:1.0099\n",
      "Epoch: 549, test AUC: 0.8687, test AP: 0.8634, train AUC: 0.9301, train AP: 0.9195, loss:1.0078\n",
      "Epoch: 550, test AUC: 0.8688, test AP: 0.8635, train AUC: 0.9301, train AP: 0.9195, loss:1.0082\n",
      "Epoch: 551, test AUC: 0.8690, test AP: 0.8637, train AUC: 0.9302, train AP: 0.9196, loss:1.0124\n",
      "Epoch: 552, test AUC: 0.8691, test AP: 0.8638, train AUC: 0.9303, train AP: 0.9197, loss:1.0135\n",
      "Epoch: 553, test AUC: 0.8692, test AP: 0.8639, train AUC: 0.9304, train AP: 0.9198, loss:1.0116\n",
      "Epoch: 554, test AUC: 0.8693, test AP: 0.8640, train AUC: 0.9305, train AP: 0.9199, loss:1.0096\n",
      "Epoch: 555, test AUC: 0.8694, test AP: 0.8641, train AUC: 0.9305, train AP: 0.9199, loss:1.0115\n",
      "Epoch: 556, test AUC: 0.8694, test AP: 0.8642, train AUC: 0.9306, train AP: 0.9200, loss:1.0070\n",
      "Epoch: 557, test AUC: 0.8695, test AP: 0.8643, train AUC: 0.9307, train AP: 0.9201, loss:0.9979\n",
      "Epoch: 558, test AUC: 0.8696, test AP: 0.8644, train AUC: 0.9307, train AP: 0.9202, loss:1.0034\n",
      "Epoch: 559, test AUC: 0.8697, test AP: 0.8645, train AUC: 0.9308, train AP: 0.9202, loss:1.0080\n",
      "Epoch: 560, test AUC: 0.8698, test AP: 0.8646, train AUC: 0.9309, train AP: 0.9203, loss:1.0030\n",
      "Epoch: 561, test AUC: 0.8700, test AP: 0.8648, train AUC: 0.9309, train AP: 0.9204, loss:1.0059\n",
      "Epoch: 562, test AUC: 0.8701, test AP: 0.8649, train AUC: 0.9310, train AP: 0.9205, loss:1.0018\n",
      "Epoch: 563, test AUC: 0.8702, test AP: 0.8650, train AUC: 0.9311, train AP: 0.9206, loss:1.0080\n",
      "Epoch: 564, test AUC: 0.8702, test AP: 0.8651, train AUC: 0.9312, train AP: 0.9207, loss:1.0063\n",
      "Epoch: 565, test AUC: 0.8703, test AP: 0.8652, train AUC: 0.9312, train AP: 0.9208, loss:1.0001\n",
      "Epoch: 566, test AUC: 0.8705, test AP: 0.8653, train AUC: 0.9313, train AP: 0.9208, loss:1.0068\n",
      "Epoch: 567, test AUC: 0.8705, test AP: 0.8655, train AUC: 0.9314, train AP: 0.9209, loss:1.0047\n",
      "Epoch: 568, test AUC: 0.8706, test AP: 0.8656, train AUC: 0.9315, train AP: 0.9210, loss:1.0106\n",
      "Epoch: 569, test AUC: 0.8707, test AP: 0.8657, train AUC: 0.9315, train AP: 0.9211, loss:1.0072\n",
      "Epoch: 570, test AUC: 0.8708, test AP: 0.8657, train AUC: 0.9316, train AP: 0.9212, loss:1.0026\n",
      "Epoch: 571, test AUC: 0.8708, test AP: 0.8658, train AUC: 0.9317, train AP: 0.9213, loss:0.9997\n",
      "Epoch: 572, test AUC: 0.8709, test AP: 0.8659, train AUC: 0.9318, train AP: 0.9214, loss:1.0084\n",
      "Epoch: 573, test AUC: 0.8710, test AP: 0.8660, train AUC: 0.9319, train AP: 0.9215, loss:1.0052\n",
      "Epoch: 574, test AUC: 0.8711, test AP: 0.8661, train AUC: 0.9319, train AP: 0.9215, loss:1.0083\n",
      "Epoch: 575, test AUC: 0.8711, test AP: 0.8661, train AUC: 0.9320, train AP: 0.9216, loss:0.9989\n",
      "Epoch: 576, test AUC: 0.8711, test AP: 0.8662, train AUC: 0.9321, train AP: 0.9217, loss:1.0019\n",
      "Epoch: 577, test AUC: 0.8712, test AP: 0.8663, train AUC: 0.9321, train AP: 0.9218, loss:1.0049\n",
      "Epoch: 578, test AUC: 0.8712, test AP: 0.8663, train AUC: 0.9322, train AP: 0.9218, loss:1.0078\n",
      "Epoch: 579, test AUC: 0.8713, test AP: 0.8664, train AUC: 0.9322, train AP: 0.9219, loss:1.0030\n",
      "Epoch: 580, test AUC: 0.8714, test AP: 0.8665, train AUC: 0.9323, train AP: 0.9219, loss:1.0018\n",
      "Epoch: 581, test AUC: 0.8714, test AP: 0.8666, train AUC: 0.9323, train AP: 0.9220, loss:0.9977\n",
      "Epoch: 582, test AUC: 0.8715, test AP: 0.8667, train AUC: 0.9324, train AP: 0.9221, loss:1.0005\n",
      "Epoch: 583, test AUC: 0.8716, test AP: 0.8668, train AUC: 0.9325, train AP: 0.9222, loss:1.0028\n",
      "Epoch: 584, test AUC: 0.8717, test AP: 0.8668, train AUC: 0.9325, train AP: 0.9223, loss:1.0081\n",
      "Epoch: 585, test AUC: 0.8718, test AP: 0.8670, train AUC: 0.9326, train AP: 0.9223, loss:1.0022\n",
      "Epoch: 586, test AUC: 0.8719, test AP: 0.8671, train AUC: 0.9327, train AP: 0.9224, loss:1.0000\n",
      "Epoch: 587, test AUC: 0.8720, test AP: 0.8672, train AUC: 0.9327, train AP: 0.9225, loss:1.0009\n",
      "Epoch: 588, test AUC: 0.8721, test AP: 0.8673, train AUC: 0.9328, train AP: 0.9226, loss:0.9985\n",
      "Epoch: 589, test AUC: 0.8722, test AP: 0.8675, train AUC: 0.9329, train AP: 0.9227, loss:1.0042\n",
      "Epoch: 590, test AUC: 0.8723, test AP: 0.8676, train AUC: 0.9330, train AP: 0.9227, loss:1.0047\n",
      "Epoch: 591, test AUC: 0.8724, test AP: 0.8677, train AUC: 0.9330, train AP: 0.9228, loss:1.0045\n",
      "Epoch: 592, test AUC: 0.8725, test AP: 0.8678, train AUC: 0.9331, train AP: 0.9229, loss:1.0052\n",
      "Epoch: 593, test AUC: 0.8727, test AP: 0.8679, train AUC: 0.9332, train AP: 0.9230, loss:1.0040\n",
      "Epoch: 594, test AUC: 0.8727, test AP: 0.8680, train AUC: 0.9333, train AP: 0.9231, loss:1.0002\n",
      "Epoch: 595, test AUC: 0.8728, test AP: 0.8681, train AUC: 0.9333, train AP: 0.9231, loss:0.9996\n",
      "Epoch: 596, test AUC: 0.8729, test AP: 0.8681, train AUC: 0.9334, train AP: 0.9232, loss:0.9987\n",
      "Epoch: 597, test AUC: 0.8729, test AP: 0.8682, train AUC: 0.9335, train AP: 0.9232, loss:0.9970\n",
      "Epoch: 598, test AUC: 0.8730, test AP: 0.8682, train AUC: 0.9335, train AP: 0.9233, loss:1.0102\n",
      "Epoch: 599, test AUC: 0.8730, test AP: 0.8683, train AUC: 0.9336, train AP: 0.9234, loss:1.0010\n",
      "Epoch: 600, test AUC: 0.8731, test AP: 0.8684, train AUC: 0.9336, train AP: 0.9234, loss:1.0030\n",
      "Epoch: 601, test AUC: 0.8732, test AP: 0.8685, train AUC: 0.9337, train AP: 0.9236, loss:0.9999\n",
      "Epoch: 602, test AUC: 0.8734, test AP: 0.8687, train AUC: 0.9338, train AP: 0.9237, loss:1.0014\n",
      "Epoch: 603, test AUC: 0.8735, test AP: 0.8688, train AUC: 0.9339, train AP: 0.9238, loss:1.0012\n",
      "Epoch: 604, test AUC: 0.8736, test AP: 0.8690, train AUC: 0.9340, train AP: 0.9239, loss:0.9990\n",
      "Epoch: 605, test AUC: 0.8738, test AP: 0.8691, train AUC: 0.9341, train AP: 0.9240, loss:1.0026\n",
      "Epoch: 606, test AUC: 0.8739, test AP: 0.8693, train AUC: 0.9342, train AP: 0.9241, loss:1.0005\n",
      "Epoch: 607, test AUC: 0.8740, test AP: 0.8694, train AUC: 0.9343, train AP: 0.9242, loss:0.9964\n",
      "Epoch: 608, test AUC: 0.8741, test AP: 0.8695, train AUC: 0.9344, train AP: 0.9242, loss:1.0015\n",
      "Epoch: 609, test AUC: 0.8742, test AP: 0.8696, train AUC: 0.9344, train AP: 0.9243, loss:1.0024\n",
      "Epoch: 610, test AUC: 0.8743, test AP: 0.8697, train AUC: 0.9345, train AP: 0.9243, loss:1.0005\n",
      "Epoch: 611, test AUC: 0.8744, test AP: 0.8698, train AUC: 0.9345, train AP: 0.9244, loss:0.9977\n",
      "Epoch: 612, test AUC: 0.8745, test AP: 0.8699, train AUC: 0.9346, train AP: 0.9244, loss:0.9914\n",
      "Epoch: 613, test AUC: 0.8746, test AP: 0.8699, train AUC: 0.9347, train AP: 0.9245, loss:1.0066\n",
      "Epoch: 614, test AUC: 0.8746, test AP: 0.8700, train AUC: 0.9347, train AP: 0.9245, loss:1.0038\n",
      "Epoch: 615, test AUC: 0.8747, test AP: 0.8701, train AUC: 0.9348, train AP: 0.9246, loss:0.9945\n",
      "Epoch: 616, test AUC: 0.8747, test AP: 0.8701, train AUC: 0.9348, train AP: 0.9247, loss:1.0063\n",
      "Epoch: 617, test AUC: 0.8747, test AP: 0.8702, train AUC: 0.9349, train AP: 0.9247, loss:1.0002\n",
      "Epoch: 618, test AUC: 0.8748, test AP: 0.8702, train AUC: 0.9349, train AP: 0.9248, loss:1.0025\n",
      "Epoch: 619, test AUC: 0.8748, test AP: 0.8703, train AUC: 0.9350, train AP: 0.9248, loss:0.9968\n",
      "Epoch: 620, test AUC: 0.8749, test AP: 0.8704, train AUC: 0.9351, train AP: 0.9249, loss:1.0031\n",
      "Epoch: 621, test AUC: 0.8750, test AP: 0.8705, train AUC: 0.9352, train AP: 0.9250, loss:1.0023\n",
      "Epoch: 622, test AUC: 0.8751, test AP: 0.8707, train AUC: 0.9353, train AP: 0.9252, loss:1.0010\n",
      "Epoch: 623, test AUC: 0.8753, test AP: 0.8708, train AUC: 0.9354, train AP: 0.9253, loss:0.9980\n",
      "Epoch: 624, test AUC: 0.8754, test AP: 0.8710, train AUC: 0.9355, train AP: 0.9254, loss:1.0041\n",
      "Epoch: 625, test AUC: 0.8756, test AP: 0.8711, train AUC: 0.9356, train AP: 0.9255, loss:1.0082\n",
      "Epoch: 626, test AUC: 0.8756, test AP: 0.8712, train AUC: 0.9357, train AP: 0.9256, loss:0.9926\n",
      "Epoch: 627, test AUC: 0.8757, test AP: 0.8713, train AUC: 0.9358, train AP: 0.9257, loss:0.9892\n",
      "Epoch: 628, test AUC: 0.8757, test AP: 0.8713, train AUC: 0.9358, train AP: 0.9257, loss:0.9882\n",
      "Epoch: 629, test AUC: 0.8758, test AP: 0.8714, train AUC: 0.9359, train AP: 0.9258, loss:1.0031\n",
      "Epoch: 630, test AUC: 0.8759, test AP: 0.8714, train AUC: 0.9360, train AP: 0.9258, loss:1.0021\n",
      "Epoch: 631, test AUC: 0.8759, test AP: 0.8715, train AUC: 0.9360, train AP: 0.9259, loss:0.9963\n",
      "Epoch: 632, test AUC: 0.8760, test AP: 0.8716, train AUC: 0.9361, train AP: 0.9260, loss:1.0039\n",
      "Epoch: 633, test AUC: 0.8761, test AP: 0.8717, train AUC: 0.9362, train AP: 0.9261, loss:0.9903\n",
      "Epoch: 634, test AUC: 0.8763, test AP: 0.8719, train AUC: 0.9363, train AP: 0.9262, loss:1.0019\n",
      "Epoch: 635, test AUC: 0.8765, test AP: 0.8722, train AUC: 0.9364, train AP: 0.9263, loss:0.9953\n",
      "Epoch: 636, test AUC: 0.8767, test AP: 0.8724, train AUC: 0.9365, train AP: 0.9264, loss:1.0000\n",
      "Epoch: 637, test AUC: 0.8768, test AP: 0.8726, train AUC: 0.9366, train AP: 0.9265, loss:0.9970\n",
      "Epoch: 638, test AUC: 0.8770, test AP: 0.8727, train AUC: 0.9366, train AP: 0.9266, loss:0.9937\n",
      "Epoch: 639, test AUC: 0.8771, test AP: 0.8728, train AUC: 0.9367, train AP: 0.9267, loss:0.9971\n",
      "Epoch: 640, test AUC: 0.8771, test AP: 0.8729, train AUC: 0.9367, train AP: 0.9267, loss:0.9988\n",
      "Epoch: 641, test AUC: 0.8771, test AP: 0.8729, train AUC: 0.9368, train AP: 0.9268, loss:1.0010\n",
      "Epoch: 642, test AUC: 0.8772, test AP: 0.8729, train AUC: 0.9368, train AP: 0.9268, loss:0.9989\n",
      "Epoch: 643, test AUC: 0.8772, test AP: 0.8729, train AUC: 0.9368, train AP: 0.9268, loss:0.9948\n",
      "Epoch: 644, test AUC: 0.8772, test AP: 0.8729, train AUC: 0.9368, train AP: 0.9268, loss:0.9931\n",
      "Epoch: 645, test AUC: 0.8771, test AP: 0.8729, train AUC: 0.9368, train AP: 0.9268, loss:0.9982\n",
      "Epoch: 646, test AUC: 0.8771, test AP: 0.8728, train AUC: 0.9368, train AP: 0.9267, loss:0.9990\n",
      "Epoch: 647, test AUC: 0.8771, test AP: 0.8728, train AUC: 0.9369, train AP: 0.9268, loss:0.9983\n",
      "Epoch: 648, test AUC: 0.8772, test AP: 0.8729, train AUC: 0.9369, train AP: 0.9268, loss:0.9988\n",
      "Epoch: 649, test AUC: 0.8773, test AP: 0.8730, train AUC: 0.9370, train AP: 0.9269, loss:0.9940\n",
      "Epoch: 650, test AUC: 0.8774, test AP: 0.8730, train AUC: 0.9371, train AP: 0.9269, loss:0.9919\n",
      "Epoch: 651, test AUC: 0.8775, test AP: 0.8732, train AUC: 0.9372, train AP: 0.9270, loss:0.9934\n",
      "Epoch: 652, test AUC: 0.8776, test AP: 0.8733, train AUC: 0.9373, train AP: 0.9271, loss:0.9876\n",
      "Epoch: 653, test AUC: 0.8778, test AP: 0.8735, train AUC: 0.9374, train AP: 0.9272, loss:0.9933\n",
      "Epoch: 654, test AUC: 0.8779, test AP: 0.8736, train AUC: 0.9375, train AP: 0.9274, loss:0.9964\n",
      "Epoch: 655, test AUC: 0.8780, test AP: 0.8738, train AUC: 0.9375, train AP: 0.9274, loss:0.9901\n",
      "Epoch: 656, test AUC: 0.8781, test AP: 0.8739, train AUC: 0.9376, train AP: 0.9275, loss:0.9973\n",
      "Epoch: 657, test AUC: 0.8782, test AP: 0.8740, train AUC: 0.9377, train AP: 0.9276, loss:0.9874\n",
      "Epoch: 658, test AUC: 0.8784, test AP: 0.8742, train AUC: 0.9378, train AP: 0.9278, loss:0.9956\n",
      "Epoch: 659, test AUC: 0.8785, test AP: 0.8744, train AUC: 0.9379, train AP: 0.9279, loss:1.0004\n",
      "Epoch: 660, test AUC: 0.8787, test AP: 0.8746, train AUC: 0.9380, train AP: 0.9280, loss:0.9967\n",
      "Epoch: 661, test AUC: 0.8788, test AP: 0.8747, train AUC: 0.9381, train AP: 0.9281, loss:0.9970\n",
      "Epoch: 662, test AUC: 0.8789, test AP: 0.8748, train AUC: 0.9382, train AP: 0.9282, loss:1.0017\n",
      "Epoch: 663, test AUC: 0.8790, test AP: 0.8749, train AUC: 0.9383, train AP: 0.9283, loss:0.9827\n",
      "Epoch: 664, test AUC: 0.8790, test AP: 0.8749, train AUC: 0.9384, train AP: 0.9284, loss:0.9989\n",
      "Epoch: 665, test AUC: 0.8791, test AP: 0.8749, train AUC: 0.9384, train AP: 0.9284, loss:0.9973\n",
      "Epoch: 666, test AUC: 0.8791, test AP: 0.8749, train AUC: 0.9384, train AP: 0.9284, loss:0.9972\n",
      "Epoch: 667, test AUC: 0.8791, test AP: 0.8750, train AUC: 0.9385, train AP: 0.9284, loss:0.9916\n",
      "Epoch: 668, test AUC: 0.8792, test AP: 0.8750, train AUC: 0.9385, train AP: 0.9285, loss:1.0008\n",
      "Epoch: 669, test AUC: 0.8793, test AP: 0.8751, train AUC: 0.9386, train AP: 0.9286, loss:0.9990\n",
      "Epoch: 670, test AUC: 0.8793, test AP: 0.8752, train AUC: 0.9387, train AP: 0.9286, loss:0.9907\n",
      "Epoch: 671, test AUC: 0.8795, test AP: 0.8754, train AUC: 0.9387, train AP: 0.9288, loss:0.9897\n",
      "Epoch: 672, test AUC: 0.8796, test AP: 0.8756, train AUC: 0.9389, train AP: 0.9289, loss:0.9898\n",
      "Epoch: 673, test AUC: 0.8798, test AP: 0.8757, train AUC: 0.9389, train AP: 0.9290, loss:0.9939\n",
      "Epoch: 674, test AUC: 0.8799, test AP: 0.8758, train AUC: 0.9390, train AP: 0.9290, loss:0.9916\n",
      "Epoch: 675, test AUC: 0.8800, test AP: 0.8760, train AUC: 0.9391, train AP: 0.9291, loss:0.9900\n",
      "Epoch: 676, test AUC: 0.8801, test AP: 0.8761, train AUC: 0.9391, train AP: 0.9292, loss:0.9985\n",
      "Epoch: 677, test AUC: 0.8802, test AP: 0.8761, train AUC: 0.9392, train AP: 0.9292, loss:0.9957\n",
      "Epoch: 678, test AUC: 0.8802, test AP: 0.8761, train AUC: 0.9392, train AP: 0.9292, loss:0.9966\n",
      "Epoch: 679, test AUC: 0.8802, test AP: 0.8761, train AUC: 0.9392, train AP: 0.9292, loss:0.9973\n",
      "Epoch: 680, test AUC: 0.8803, test AP: 0.8762, train AUC: 0.9393, train AP: 0.9293, loss:0.9957\n",
      "Epoch: 681, test AUC: 0.8803, test AP: 0.8762, train AUC: 0.9393, train AP: 0.9293, loss:0.9899\n",
      "Epoch: 682, test AUC: 0.8804, test AP: 0.8762, train AUC: 0.9393, train AP: 0.9293, loss:0.9923\n",
      "Epoch: 683, test AUC: 0.8804, test AP: 0.8762, train AUC: 0.9394, train AP: 0.9293, loss:0.9953\n",
      "Epoch: 684, test AUC: 0.8804, test AP: 0.8763, train AUC: 0.9394, train AP: 0.9293, loss:0.9971\n",
      "Epoch: 685, test AUC: 0.8805, test AP: 0.8762, train AUC: 0.9394, train AP: 0.9293, loss:0.9907\n",
      "Epoch: 686, test AUC: 0.8805, test AP: 0.8763, train AUC: 0.9395, train AP: 0.9294, loss:0.9881\n",
      "Epoch: 687, test AUC: 0.8805, test AP: 0.8763, train AUC: 0.9395, train AP: 0.9294, loss:0.9894\n",
      "Epoch: 688, test AUC: 0.8806, test AP: 0.8764, train AUC: 0.9396, train AP: 0.9295, loss:0.9920\n",
      "Epoch: 689, test AUC: 0.8808, test AP: 0.8766, train AUC: 0.9398, train AP: 0.9297, loss:0.9963\n",
      "Epoch: 690, test AUC: 0.8809, test AP: 0.8768, train AUC: 0.9399, train AP: 0.9299, loss:0.9878\n",
      "Epoch: 691, test AUC: 0.8811, test AP: 0.8771, train AUC: 0.9401, train AP: 0.9301, loss:0.9916\n",
      "Epoch: 692, test AUC: 0.8812, test AP: 0.8772, train AUC: 0.9402, train AP: 0.9302, loss:0.9844\n",
      "Epoch: 693, test AUC: 0.8814, test AP: 0.8774, train AUC: 0.9403, train AP: 0.9303, loss:0.9935\n",
      "Epoch: 694, test AUC: 0.8815, test AP: 0.8775, train AUC: 0.9404, train AP: 0.9304, loss:0.9947\n",
      "Epoch: 695, test AUC: 0.8816, test AP: 0.8776, train AUC: 0.9405, train AP: 0.9305, loss:0.9908\n",
      "Epoch: 696, test AUC: 0.8817, test AP: 0.8777, train AUC: 0.9405, train AP: 0.9306, loss:0.9873\n",
      "Epoch: 697, test AUC: 0.8818, test AP: 0.8778, train AUC: 0.9406, train AP: 0.9306, loss:0.9887\n",
      "Epoch: 698, test AUC: 0.8818, test AP: 0.8778, train AUC: 0.9406, train AP: 0.9306, loss:0.9921\n",
      "Epoch: 699, test AUC: 0.8818, test AP: 0.8778, train AUC: 0.9406, train AP: 0.9306, loss:0.9903\n",
      "Epoch: 700, test AUC: 0.8818, test AP: 0.8778, train AUC: 0.9406, train AP: 0.9306, loss:0.9929\n",
      "Epoch: 701, test AUC: 0.8819, test AP: 0.8779, train AUC: 0.9407, train AP: 0.9307, loss:0.9864\n",
      "Epoch: 702, test AUC: 0.8820, test AP: 0.8780, train AUC: 0.9408, train AP: 0.9308, loss:0.9966\n",
      "Epoch: 703, test AUC: 0.8822, test AP: 0.8782, train AUC: 0.9409, train AP: 0.9309, loss:0.9942\n",
      "Epoch: 704, test AUC: 0.8824, test AP: 0.8784, train AUC: 0.9410, train AP: 0.9310, loss:0.9884\n",
      "Epoch: 705, test AUC: 0.8825, test AP: 0.8786, train AUC: 0.9411, train AP: 0.9311, loss:0.9896\n",
      "Epoch: 706, test AUC: 0.8826, test AP: 0.8787, train AUC: 0.9412, train AP: 0.9312, loss:0.9964\n",
      "Epoch: 707, test AUC: 0.8827, test AP: 0.8788, train AUC: 0.9412, train AP: 0.9313, loss:0.9906\n",
      "Epoch: 708, test AUC: 0.8828, test AP: 0.8789, train AUC: 0.9413, train AP: 0.9313, loss:0.9941\n",
      "Epoch: 709, test AUC: 0.8828, test AP: 0.8789, train AUC: 0.9413, train AP: 0.9313, loss:0.9950\n",
      "Epoch: 710, test AUC: 0.8828, test AP: 0.8789, train AUC: 0.9413, train AP: 0.9313, loss:0.9975\n",
      "Epoch: 711, test AUC: 0.8828, test AP: 0.8788, train AUC: 0.9413, train AP: 0.9313, loss:0.9916\n",
      "Epoch: 712, test AUC: 0.8827, test AP: 0.8787, train AUC: 0.9413, train AP: 0.9312, loss:0.9945\n",
      "Epoch: 713, test AUC: 0.8827, test AP: 0.8787, train AUC: 0.9413, train AP: 0.9312, loss:0.9873\n",
      "Epoch: 714, test AUC: 0.8827, test AP: 0.8786, train AUC: 0.9413, train AP: 0.9312, loss:0.9959\n",
      "Epoch: 715, test AUC: 0.8827, test AP: 0.8786, train AUC: 0.9413, train AP: 0.9311, loss:0.9886\n",
      "Epoch: 716, test AUC: 0.8827, test AP: 0.8786, train AUC: 0.9413, train AP: 0.9312, loss:0.9898\n",
      "Epoch: 717, test AUC: 0.8828, test AP: 0.8787, train AUC: 0.9414, train AP: 0.9313, loss:0.9860\n",
      "Epoch: 718, test AUC: 0.8830, test AP: 0.8789, train AUC: 0.9415, train AP: 0.9314, loss:0.9884\n",
      "Epoch: 719, test AUC: 0.8832, test AP: 0.8792, train AUC: 0.9417, train AP: 0.9316, loss:0.9886\n",
      "Epoch: 720, test AUC: 0.8833, test AP: 0.8794, train AUC: 0.9418, train AP: 0.9318, loss:0.9895\n",
      "Epoch: 721, test AUC: 0.8835, test AP: 0.8796, train AUC: 0.9419, train AP: 0.9319, loss:0.9844\n",
      "Epoch: 722, test AUC: 0.8836, test AP: 0.8798, train AUC: 0.9420, train AP: 0.9321, loss:0.9922\n",
      "Epoch: 723, test AUC: 0.8838, test AP: 0.8800, train AUC: 0.9421, train AP: 0.9322, loss:0.9899\n",
      "Epoch: 724, test AUC: 0.8838, test AP: 0.8801, train AUC: 0.9422, train AP: 0.9322, loss:0.9815\n",
      "Epoch: 725, test AUC: 0.8839, test AP: 0.8801, train AUC: 0.9422, train AP: 0.9322, loss:0.9858\n",
      "Epoch: 726, test AUC: 0.8839, test AP: 0.8800, train AUC: 0.9422, train AP: 0.9322, loss:0.9905\n",
      "Epoch: 727, test AUC: 0.8838, test AP: 0.8799, train AUC: 0.9421, train AP: 0.9321, loss:0.9886\n",
      "Epoch: 728, test AUC: 0.8838, test AP: 0.8799, train AUC: 0.9421, train AP: 0.9321, loss:0.9840\n",
      "Epoch: 729, test AUC: 0.8839, test AP: 0.8799, train AUC: 0.9421, train AP: 0.9321, loss:0.9885\n",
      "Epoch: 730, test AUC: 0.8839, test AP: 0.8800, train AUC: 0.9422, train AP: 0.9321, loss:0.9941\n",
      "Epoch: 731, test AUC: 0.8840, test AP: 0.8800, train AUC: 0.9422, train AP: 0.9322, loss:0.9907\n",
      "Epoch: 732, test AUC: 0.8841, test AP: 0.8801, train AUC: 0.9423, train AP: 0.9322, loss:0.9906\n",
      "Epoch: 733, test AUC: 0.8842, test AP: 0.8803, train AUC: 0.9424, train AP: 0.9324, loss:0.9856\n",
      "Epoch: 734, test AUC: 0.8843, test AP: 0.8804, train AUC: 0.9425, train AP: 0.9325, loss:0.9921\n",
      "Epoch: 735, test AUC: 0.8844, test AP: 0.8806, train AUC: 0.9426, train AP: 0.9326, loss:0.9913\n",
      "Epoch: 736, test AUC: 0.8845, test AP: 0.8806, train AUC: 0.9427, train AP: 0.9327, loss:0.9888\n",
      "Epoch: 737, test AUC: 0.8845, test AP: 0.8807, train AUC: 0.9427, train AP: 0.9327, loss:0.9875\n",
      "Epoch: 738, test AUC: 0.8846, test AP: 0.8807, train AUC: 0.9428, train AP: 0.9328, loss:0.9928\n",
      "Epoch: 739, test AUC: 0.8846, test AP: 0.8807, train AUC: 0.9428, train AP: 0.9328, loss:0.9898\n",
      "Epoch: 740, test AUC: 0.8846, test AP: 0.8807, train AUC: 0.9429, train AP: 0.9329, loss:0.9890\n",
      "Epoch: 741, test AUC: 0.8847, test AP: 0.8807, train AUC: 0.9429, train AP: 0.9329, loss:0.9937\n",
      "Epoch: 742, test AUC: 0.8847, test AP: 0.8808, train AUC: 0.9430, train AP: 0.9330, loss:0.9890\n",
      "Epoch: 743, test AUC: 0.8848, test AP: 0.8810, train AUC: 0.9431, train AP: 0.9331, loss:0.9876\n",
      "Epoch: 744, test AUC: 0.8849, test AP: 0.8811, train AUC: 0.9432, train AP: 0.9332, loss:0.9913\n",
      "Epoch: 745, test AUC: 0.8850, test AP: 0.8811, train AUC: 0.9432, train AP: 0.9332, loss:0.9880\n",
      "Epoch: 746, test AUC: 0.8850, test AP: 0.8812, train AUC: 0.9433, train AP: 0.9333, loss:0.9847\n",
      "Epoch: 747, test AUC: 0.8851, test AP: 0.8814, train AUC: 0.9434, train AP: 0.9334, loss:0.9854\n",
      "Epoch: 748, test AUC: 0.8852, test AP: 0.8815, train AUC: 0.9435, train AP: 0.9335, loss:0.9880\n",
      "Epoch: 749, test AUC: 0.8853, test AP: 0.8816, train AUC: 0.9436, train AP: 0.9336, loss:0.9862\n",
      "Epoch: 750, test AUC: 0.8854, test AP: 0.8818, train AUC: 0.9437, train AP: 0.9338, loss:0.9789\n",
      "Epoch: 751, test AUC: 0.8856, test AP: 0.8820, train AUC: 0.9438, train AP: 0.9339, loss:0.9897\n",
      "Epoch: 752, test AUC: 0.8857, test AP: 0.8821, train AUC: 0.9439, train AP: 0.9340, loss:0.9883\n",
      "Epoch: 753, test AUC: 0.8858, test AP: 0.8822, train AUC: 0.9440, train AP: 0.9341, loss:0.9845\n",
      "Epoch: 754, test AUC: 0.8859, test AP: 0.8823, train AUC: 0.9440, train AP: 0.9342, loss:0.9824\n",
      "Epoch: 755, test AUC: 0.8860, test AP: 0.8824, train AUC: 0.9441, train AP: 0.9342, loss:0.9800\n",
      "Epoch: 756, test AUC: 0.8861, test AP: 0.8824, train AUC: 0.9441, train AP: 0.9342, loss:0.9861\n",
      "Epoch: 757, test AUC: 0.8860, test AP: 0.8824, train AUC: 0.9441, train AP: 0.9342, loss:0.9903\n",
      "Epoch: 758, test AUC: 0.8860, test AP: 0.8823, train AUC: 0.9440, train AP: 0.9341, loss:0.9867\n",
      "Epoch: 759, test AUC: 0.8860, test AP: 0.8822, train AUC: 0.9440, train AP: 0.9340, loss:0.9871\n",
      "Epoch: 760, test AUC: 0.8858, test AP: 0.8820, train AUC: 0.9439, train AP: 0.9339, loss:0.9886\n",
      "Epoch: 761, test AUC: 0.8858, test AP: 0.8820, train AUC: 0.9439, train AP: 0.9338, loss:0.9891\n",
      "Epoch: 762, test AUC: 0.8859, test AP: 0.8820, train AUC: 0.9439, train AP: 0.9338, loss:0.9852\n",
      "Epoch: 763, test AUC: 0.8859, test AP: 0.8820, train AUC: 0.9440, train AP: 0.9339, loss:0.9871\n",
      "Epoch: 764, test AUC: 0.8860, test AP: 0.8821, train AUC: 0.9440, train AP: 0.9340, loss:0.9858\n",
      "Epoch: 765, test AUC: 0.8861, test AP: 0.8823, train AUC: 0.9441, train AP: 0.9341, loss:0.9834\n",
      "Epoch: 766, test AUC: 0.8862, test AP: 0.8824, train AUC: 0.9443, train AP: 0.9343, loss:0.9902\n",
      "Epoch: 767, test AUC: 0.8863, test AP: 0.8826, train AUC: 0.9444, train AP: 0.9344, loss:0.9845\n",
      "Epoch: 768, test AUC: 0.8865, test AP: 0.8828, train AUC: 0.9445, train AP: 0.9346, loss:0.9872\n",
      "Epoch: 769, test AUC: 0.8866, test AP: 0.8829, train AUC: 0.9446, train AP: 0.9347, loss:0.9930\n",
      "Epoch: 770, test AUC: 0.8866, test AP: 0.8830, train AUC: 0.9447, train AP: 0.9348, loss:0.9836\n",
      "Epoch: 771, test AUC: 0.8866, test AP: 0.8830, train AUC: 0.9447, train AP: 0.9348, loss:0.9824\n",
      "Epoch: 772, test AUC: 0.8867, test AP: 0.8831, train AUC: 0.9448, train AP: 0.9349, loss:0.9899\n",
      "Epoch: 773, test AUC: 0.8867, test AP: 0.8831, train AUC: 0.9449, train AP: 0.9349, loss:0.9885\n",
      "Epoch: 774, test AUC: 0.8868, test AP: 0.8831, train AUC: 0.9449, train AP: 0.9350, loss:0.9875\n",
      "Epoch: 775, test AUC: 0.8868, test AP: 0.8832, train AUC: 0.9449, train AP: 0.9350, loss:0.9901\n",
      "Epoch: 776, test AUC: 0.8869, test AP: 0.8833, train AUC: 0.9450, train AP: 0.9351, loss:0.9808\n",
      "Epoch: 777, test AUC: 0.8870, test AP: 0.8834, train AUC: 0.9451, train AP: 0.9351, loss:0.9890\n",
      "Epoch: 778, test AUC: 0.8871, test AP: 0.8835, train AUC: 0.9451, train AP: 0.9352, loss:0.9877\n",
      "Epoch: 779, test AUC: 0.8872, test AP: 0.8836, train AUC: 0.9452, train AP: 0.9353, loss:0.9827\n",
      "Epoch: 780, test AUC: 0.8872, test AP: 0.8836, train AUC: 0.9452, train AP: 0.9353, loss:0.9902\n",
      "Epoch: 781, test AUC: 0.8872, test AP: 0.8835, train AUC: 0.9452, train AP: 0.9353, loss:0.9863\n",
      "Epoch: 782, test AUC: 0.8872, test AP: 0.8834, train AUC: 0.9452, train AP: 0.9352, loss:0.9857\n",
      "Epoch: 783, test AUC: 0.8871, test AP: 0.8833, train AUC: 0.9451, train AP: 0.9351, loss:0.9817\n",
      "Epoch: 784, test AUC: 0.8870, test AP: 0.8832, train AUC: 0.9450, train AP: 0.9350, loss:0.9838\n",
      "Epoch: 785, test AUC: 0.8869, test AP: 0.8831, train AUC: 0.9450, train AP: 0.9349, loss:0.9810\n",
      "Epoch: 786, test AUC: 0.8869, test AP: 0.8831, train AUC: 0.9450, train AP: 0.9349, loss:0.9896\n",
      "Epoch: 787, test AUC: 0.8870, test AP: 0.8831, train AUC: 0.9451, train AP: 0.9350, loss:0.9796\n",
      "Epoch: 788, test AUC: 0.8871, test AP: 0.8833, train AUC: 0.9452, train AP: 0.9352, loss:0.9779\n",
      "Epoch: 789, test AUC: 0.8872, test AP: 0.8835, train AUC: 0.9454, train AP: 0.9353, loss:0.9808\n",
      "Epoch: 790, test AUC: 0.8874, test AP: 0.8837, train AUC: 0.9455, train AP: 0.9355, loss:0.9823\n",
      "Epoch: 791, test AUC: 0.8875, test AP: 0.8839, train AUC: 0.9457, train AP: 0.9357, loss:0.9916\n",
      "Epoch: 792, test AUC: 0.8877, test AP: 0.8841, train AUC: 0.9458, train AP: 0.9359, loss:0.9851\n",
      "Epoch: 793, test AUC: 0.8877, test AP: 0.8842, train AUC: 0.9458, train AP: 0.9360, loss:0.9855\n",
      "Epoch: 794, test AUC: 0.8878, test AP: 0.8843, train AUC: 0.9459, train AP: 0.9361, loss:0.9872\n",
      "Epoch: 795, test AUC: 0.8878, test AP: 0.8843, train AUC: 0.9459, train AP: 0.9361, loss:0.9808\n",
      "Epoch: 796, test AUC: 0.8878, test AP: 0.8843, train AUC: 0.9460, train AP: 0.9361, loss:0.9857\n",
      "Epoch: 797, test AUC: 0.8878, test AP: 0.8843, train AUC: 0.9460, train AP: 0.9361, loss:0.9813\n",
      "Epoch: 798, test AUC: 0.8878, test AP: 0.8843, train AUC: 0.9460, train AP: 0.9361, loss:0.9841\n",
      "Epoch: 799, test AUC: 0.8879, test AP: 0.8843, train AUC: 0.9461, train AP: 0.9361, loss:0.9891\n",
      "Epoch: 800, test AUC: 0.8879, test AP: 0.8843, train AUC: 0.9461, train AP: 0.9361, loss:0.9761\n",
      "Epoch: 801, test AUC: 0.8879, test AP: 0.8843, train AUC: 0.9461, train AP: 0.9361, loss:0.9754\n",
      "Epoch: 802, test AUC: 0.8880, test AP: 0.8843, train AUC: 0.9462, train AP: 0.9362, loss:0.9869\n",
      "Epoch: 803, test AUC: 0.8880, test AP: 0.8844, train AUC: 0.9462, train AP: 0.9362, loss:0.9879\n",
      "Epoch: 804, test AUC: 0.8880, test AP: 0.8843, train AUC: 0.9462, train AP: 0.9362, loss:0.9798\n",
      "Epoch: 805, test AUC: 0.8880, test AP: 0.8842, train AUC: 0.9462, train AP: 0.9361, loss:0.9901\n",
      "Epoch: 806, test AUC: 0.8880, test AP: 0.8843, train AUC: 0.9462, train AP: 0.9362, loss:0.9878\n",
      "Epoch: 807, test AUC: 0.8881, test AP: 0.8843, train AUC: 0.9462, train AP: 0.9362, loss:0.9737\n",
      "Epoch: 808, test AUC: 0.8882, test AP: 0.8844, train AUC: 0.9463, train AP: 0.9363, loss:0.9825\n",
      "Epoch: 809, test AUC: 0.8882, test AP: 0.8845, train AUC: 0.9464, train AP: 0.9364, loss:0.9832\n",
      "Epoch: 810, test AUC: 0.8883, test AP: 0.8846, train AUC: 0.9465, train AP: 0.9365, loss:0.9799\n",
      "Epoch: 811, test AUC: 0.8884, test AP: 0.8847, train AUC: 0.9465, train AP: 0.9365, loss:0.9883\n",
      "Epoch: 812, test AUC: 0.8884, test AP: 0.8847, train AUC: 0.9466, train AP: 0.9366, loss:0.9799\n",
      "Epoch: 813, test AUC: 0.8885, test AP: 0.8848, train AUC: 0.9466, train AP: 0.9367, loss:0.9838\n",
      "Epoch: 814, test AUC: 0.8885, test AP: 0.8849, train AUC: 0.9467, train AP: 0.9368, loss:0.9911\n",
      "Epoch: 815, test AUC: 0.8886, test AP: 0.8850, train AUC: 0.9468, train AP: 0.9369, loss:0.9842\n",
      "Epoch: 816, test AUC: 0.8887, test AP: 0.8851, train AUC: 0.9469, train AP: 0.9370, loss:0.9731\n",
      "Epoch: 817, test AUC: 0.8888, test AP: 0.8852, train AUC: 0.9470, train AP: 0.9371, loss:0.9799\n",
      "Epoch: 818, test AUC: 0.8889, test AP: 0.8853, train AUC: 0.9471, train AP: 0.9371, loss:0.9848\n",
      "Epoch: 819, test AUC: 0.8889, test AP: 0.8853, train AUC: 0.9471, train AP: 0.9372, loss:0.9792\n",
      "Epoch: 820, test AUC: 0.8890, test AP: 0.8853, train AUC: 0.9471, train AP: 0.9372, loss:0.9825\n",
      "Epoch: 821, test AUC: 0.8890, test AP: 0.8853, train AUC: 0.9471, train AP: 0.9372, loss:0.9831\n",
      "Epoch: 822, test AUC: 0.8890, test AP: 0.8853, train AUC: 0.9471, train AP: 0.9372, loss:0.9787\n",
      "Epoch: 823, test AUC: 0.8890, test AP: 0.8853, train AUC: 0.9472, train AP: 0.9372, loss:0.9844\n",
      "Epoch: 824, test AUC: 0.8890, test AP: 0.8853, train AUC: 0.9472, train AP: 0.9372, loss:0.9831\n",
      "Epoch: 825, test AUC: 0.8890, test AP: 0.8853, train AUC: 0.9472, train AP: 0.9372, loss:0.9921\n",
      "Epoch: 826, test AUC: 0.8891, test AP: 0.8854, train AUC: 0.9472, train AP: 0.9372, loss:0.9821\n",
      "Epoch: 827, test AUC: 0.8891, test AP: 0.8855, train AUC: 0.9473, train AP: 0.9373, loss:0.9768\n",
      "Epoch: 828, test AUC: 0.8892, test AP: 0.8855, train AUC: 0.9474, train AP: 0.9374, loss:0.9875\n",
      "Epoch: 829, test AUC: 0.8893, test AP: 0.8856, train AUC: 0.9474, train AP: 0.9375, loss:0.9790\n",
      "Epoch: 830, test AUC: 0.8893, test AP: 0.8857, train AUC: 0.9474, train AP: 0.9375, loss:0.9874\n",
      "Epoch: 831, test AUC: 0.8894, test AP: 0.8858, train AUC: 0.9475, train AP: 0.9375, loss:0.9821\n",
      "Epoch: 832, test AUC: 0.8894, test AP: 0.8858, train AUC: 0.9475, train AP: 0.9376, loss:0.9843\n",
      "Epoch: 833, test AUC: 0.8895, test AP: 0.8859, train AUC: 0.9476, train AP: 0.9377, loss:0.9853\n",
      "Epoch: 834, test AUC: 0.8896, test AP: 0.8860, train AUC: 0.9476, train AP: 0.9377, loss:0.9870\n",
      "Epoch: 835, test AUC: 0.8897, test AP: 0.8861, train AUC: 0.9477, train AP: 0.9378, loss:0.9779\n",
      "Epoch: 836, test AUC: 0.8898, test AP: 0.8862, train AUC: 0.9477, train AP: 0.9379, loss:0.9803\n",
      "Epoch: 837, test AUC: 0.8898, test AP: 0.8862, train AUC: 0.9478, train AP: 0.9380, loss:0.9786\n",
      "Epoch: 838, test AUC: 0.8898, test AP: 0.8862, train AUC: 0.9478, train AP: 0.9380, loss:0.9810\n",
      "Epoch: 839, test AUC: 0.8899, test AP: 0.8863, train AUC: 0.9479, train AP: 0.9380, loss:0.9846\n",
      "Epoch: 840, test AUC: 0.8899, test AP: 0.8863, train AUC: 0.9479, train AP: 0.9381, loss:0.9831\n",
      "Epoch: 841, test AUC: 0.8900, test AP: 0.8864, train AUC: 0.9480, train AP: 0.9381, loss:0.9779\n",
      "Epoch: 842, test AUC: 0.8900, test AP: 0.8863, train AUC: 0.9480, train AP: 0.9381, loss:0.9747\n",
      "Epoch: 843, test AUC: 0.8899, test AP: 0.8863, train AUC: 0.9480, train AP: 0.9381, loss:0.9842\n",
      "Epoch: 844, test AUC: 0.8899, test AP: 0.8862, train AUC: 0.9479, train AP: 0.9380, loss:0.9847\n",
      "Epoch: 845, test AUC: 0.8900, test AP: 0.8863, train AUC: 0.9480, train AP: 0.9381, loss:0.9800\n",
      "Epoch: 846, test AUC: 0.8900, test AP: 0.8864, train AUC: 0.9480, train AP: 0.9381, loss:0.9832\n",
      "Epoch: 847, test AUC: 0.8901, test AP: 0.8864, train AUC: 0.9481, train AP: 0.9382, loss:0.9799\n",
      "Epoch: 848, test AUC: 0.8901, test AP: 0.8865, train AUC: 0.9481, train AP: 0.9382, loss:0.9836\n",
      "Epoch: 849, test AUC: 0.8902, test AP: 0.8865, train AUC: 0.9482, train AP: 0.9383, loss:0.9801\n",
      "Epoch: 850, test AUC: 0.8902, test AP: 0.8866, train AUC: 0.9482, train AP: 0.9384, loss:0.9788\n",
      "Epoch: 851, test AUC: 0.8902, test AP: 0.8866, train AUC: 0.9482, train AP: 0.9384, loss:0.9779\n",
      "Epoch: 852, test AUC: 0.8903, test AP: 0.8866, train AUC: 0.9483, train AP: 0.9384, loss:0.9788\n",
      "Epoch: 853, test AUC: 0.8903, test AP: 0.8866, train AUC: 0.9483, train AP: 0.9384, loss:0.9800\n",
      "Epoch: 854, test AUC: 0.8904, test AP: 0.8867, train AUC: 0.9484, train AP: 0.9385, loss:0.9793\n",
      "Epoch: 855, test AUC: 0.8905, test AP: 0.8868, train AUC: 0.9484, train AP: 0.9386, loss:0.9769\n",
      "Epoch: 856, test AUC: 0.8905, test AP: 0.8869, train AUC: 0.9485, train AP: 0.9387, loss:0.9754\n",
      "Epoch: 857, test AUC: 0.8906, test AP: 0.8870, train AUC: 0.9486, train AP: 0.9388, loss:0.9769\n",
      "Epoch: 858, test AUC: 0.8907, test AP: 0.8871, train AUC: 0.9487, train AP: 0.9389, loss:0.9719\n",
      "Epoch: 859, test AUC: 0.8908, test AP: 0.8872, train AUC: 0.9487, train AP: 0.9390, loss:0.9823\n",
      "Epoch: 860, test AUC: 0.8909, test AP: 0.8873, train AUC: 0.9488, train AP: 0.9391, loss:0.9831\n",
      "Epoch: 861, test AUC: 0.8910, test AP: 0.8874, train AUC: 0.9489, train AP: 0.9392, loss:0.9752\n",
      "Epoch: 862, test AUC: 0.8911, test AP: 0.8876, train AUC: 0.9491, train AP: 0.9394, loss:0.9827\n",
      "Epoch: 863, test AUC: 0.8912, test AP: 0.8877, train AUC: 0.9491, train AP: 0.9395, loss:0.9789\n",
      "Epoch: 864, test AUC: 0.8912, test AP: 0.8877, train AUC: 0.9492, train AP: 0.9395, loss:0.9805\n",
      "Epoch: 865, test AUC: 0.8911, test AP: 0.8876, train AUC: 0.9491, train AP: 0.9395, loss:0.9831\n",
      "Epoch: 866, test AUC: 0.8911, test AP: 0.8876, train AUC: 0.9491, train AP: 0.9394, loss:0.9790\n",
      "Epoch: 867, test AUC: 0.8910, test AP: 0.8875, train AUC: 0.9491, train AP: 0.9393, loss:0.9761\n",
      "Epoch: 868, test AUC: 0.8910, test AP: 0.8874, train AUC: 0.9490, train AP: 0.9393, loss:0.9827\n",
      "Epoch: 869, test AUC: 0.8909, test AP: 0.8873, train AUC: 0.9490, train AP: 0.9392, loss:0.9822\n",
      "Epoch: 870, test AUC: 0.8909, test AP: 0.8873, train AUC: 0.9490, train AP: 0.9392, loss:0.9811\n",
      "Epoch: 871, test AUC: 0.8909, test AP: 0.8872, train AUC: 0.9490, train AP: 0.9392, loss:0.9809\n",
      "Epoch: 872, test AUC: 0.8909, test AP: 0.8872, train AUC: 0.9490, train AP: 0.9392, loss:0.9825\n",
      "Epoch: 873, test AUC: 0.8910, test AP: 0.8873, train AUC: 0.9490, train AP: 0.9392, loss:0.9716\n",
      "Epoch: 874, test AUC: 0.8911, test AP: 0.8874, train AUC: 0.9491, train AP: 0.9393, loss:0.9784\n",
      "Epoch: 875, test AUC: 0.8912, test AP: 0.8875, train AUC: 0.9492, train AP: 0.9394, loss:0.9834\n",
      "Epoch: 876, test AUC: 0.8912, test AP: 0.8876, train AUC: 0.9492, train AP: 0.9394, loss:0.9792\n",
      "Epoch: 877, test AUC: 0.8913, test AP: 0.8877, train AUC: 0.9493, train AP: 0.9396, loss:0.9821\n",
      "Epoch: 878, test AUC: 0.8914, test AP: 0.8878, train AUC: 0.9494, train AP: 0.9397, loss:0.9838\n",
      "Epoch: 879, test AUC: 0.8914, test AP: 0.8878, train AUC: 0.9495, train AP: 0.9397, loss:0.9783\n",
      "Epoch: 880, test AUC: 0.8915, test AP: 0.8879, train AUC: 0.9495, train AP: 0.9397, loss:0.9794\n",
      "Epoch: 881, test AUC: 0.8914, test AP: 0.8877, train AUC: 0.9494, train AP: 0.9396, loss:0.9739\n",
      "Epoch: 882, test AUC: 0.8913, test AP: 0.8877, train AUC: 0.9493, train AP: 0.9396, loss:0.9802\n",
      "Epoch: 883, test AUC: 0.8913, test AP: 0.8876, train AUC: 0.9493, train AP: 0.9395, loss:0.9772\n",
      "Epoch: 884, test AUC: 0.8912, test AP: 0.8875, train AUC: 0.9492, train AP: 0.9394, loss:0.9802\n",
      "Epoch: 885, test AUC: 0.8912, test AP: 0.8875, train AUC: 0.9492, train AP: 0.9394, loss:0.9785\n",
      "Epoch: 886, test AUC: 0.8913, test AP: 0.8876, train AUC: 0.9493, train AP: 0.9395, loss:0.9795\n",
      "Epoch: 887, test AUC: 0.8914, test AP: 0.8878, train AUC: 0.9495, train AP: 0.9397, loss:0.9787\n",
      "Epoch: 888, test AUC: 0.8916, test AP: 0.8880, train AUC: 0.9496, train AP: 0.9399, loss:0.9766\n",
      "Epoch: 889, test AUC: 0.8917, test AP: 0.8882, train AUC: 0.9498, train AP: 0.9402, loss:0.9765\n",
      "Epoch: 890, test AUC: 0.8919, test AP: 0.8885, train AUC: 0.9500, train AP: 0.9404, loss:0.9818\n",
      "Epoch: 891, test AUC: 0.8921, test AP: 0.8887, train AUC: 0.9502, train AP: 0.9406, loss:0.9779\n",
      "Epoch: 892, test AUC: 0.8922, test AP: 0.8889, train AUC: 0.9503, train AP: 0.9408, loss:0.9757\n",
      "Epoch: 893, test AUC: 0.8922, test AP: 0.8889, train AUC: 0.9503, train AP: 0.9409, loss:0.9730\n",
      "Epoch: 894, test AUC: 0.8922, test AP: 0.8889, train AUC: 0.9503, train AP: 0.9408, loss:0.9782\n",
      "Epoch: 895, test AUC: 0.8922, test AP: 0.8889, train AUC: 0.9503, train AP: 0.9408, loss:0.9764\n",
      "Epoch: 896, test AUC: 0.8921, test AP: 0.8887, train AUC: 0.9502, train AP: 0.9407, loss:0.9784\n",
      "Epoch: 897, test AUC: 0.8920, test AP: 0.8885, train AUC: 0.9501, train AP: 0.9405, loss:0.9838\n",
      "Epoch: 898, test AUC: 0.8919, test AP: 0.8883, train AUC: 0.9499, train AP: 0.9403, loss:0.9723\n",
      "Epoch: 899, test AUC: 0.8917, test AP: 0.8881, train AUC: 0.9497, train AP: 0.9400, loss:0.9838\n",
      "Epoch: 900, test AUC: 0.8915, test AP: 0.8879, train AUC: 0.9496, train AP: 0.9398, loss:0.9722\n",
      "Epoch: 901, test AUC: 0.8915, test AP: 0.8877, train AUC: 0.9495, train AP: 0.9397, loss:0.9705\n",
      "Epoch: 902, test AUC: 0.8914, test AP: 0.8877, train AUC: 0.9495, train AP: 0.9396, loss:0.9718\n",
      "Epoch: 903, test AUC: 0.8915, test AP: 0.8877, train AUC: 0.9495, train AP: 0.9397, loss:0.9816\n",
      "Epoch: 904, test AUC: 0.8916, test AP: 0.8878, train AUC: 0.9496, train AP: 0.9398, loss:0.9742\n",
      "Epoch: 905, test AUC: 0.8917, test AP: 0.8880, train AUC: 0.9497, train AP: 0.9400, loss:0.9785\n",
      "Epoch: 906, test AUC: 0.8919, test AP: 0.8883, train AUC: 0.9499, train AP: 0.9402, loss:0.9745\n",
      "Epoch: 907, test AUC: 0.8922, test AP: 0.8886, train AUC: 0.9502, train AP: 0.9405, loss:0.9799\n",
      "Epoch: 908, test AUC: 0.8924, test AP: 0.8889, train AUC: 0.9504, train AP: 0.9408, loss:0.9785\n",
      "Epoch: 909, test AUC: 0.8925, test AP: 0.8891, train AUC: 0.9505, train AP: 0.9410, loss:0.9670\n",
      "Epoch: 910, test AUC: 0.8926, test AP: 0.8892, train AUC: 0.9506, train AP: 0.9412, loss:0.9757\n",
      "Epoch: 911, test AUC: 0.8926, test AP: 0.8893, train AUC: 0.9507, train AP: 0.9412, loss:0.9811\n",
      "Epoch: 912, test AUC: 0.8927, test AP: 0.8893, train AUC: 0.9508, train AP: 0.9413, loss:0.9763\n",
      "Epoch: 913, test AUC: 0.8926, test AP: 0.8893, train AUC: 0.9508, train AP: 0.9413, loss:0.9792\n",
      "Epoch: 914, test AUC: 0.8926, test AP: 0.8892, train AUC: 0.9507, train AP: 0.9412, loss:0.9765\n",
      "Epoch: 915, test AUC: 0.8925, test AP: 0.8891, train AUC: 0.9507, train AP: 0.9411, loss:0.9776\n",
      "Epoch: 916, test AUC: 0.8925, test AP: 0.8889, train AUC: 0.9506, train AP: 0.9410, loss:0.9815\n",
      "Epoch: 917, test AUC: 0.8924, test AP: 0.8888, train AUC: 0.9505, train AP: 0.9409, loss:0.9773\n",
      "Epoch: 918, test AUC: 0.8922, test AP: 0.8886, train AUC: 0.9503, train AP: 0.9407, loss:0.9807\n",
      "Epoch: 919, test AUC: 0.8921, test AP: 0.8885, train AUC: 0.9502, train AP: 0.9405, loss:0.9743\n",
      "Epoch: 920, test AUC: 0.8920, test AP: 0.8883, train AUC: 0.9502, train AP: 0.9404, loss:0.9795\n",
      "Epoch: 921, test AUC: 0.8921, test AP: 0.8884, train AUC: 0.9502, train AP: 0.9405, loss:0.9750\n",
      "Epoch: 922, test AUC: 0.8922, test AP: 0.8885, train AUC: 0.9503, train AP: 0.9406, loss:0.9756\n",
      "Epoch: 923, test AUC: 0.8923, test AP: 0.8887, train AUC: 0.9504, train AP: 0.9408, loss:0.9789\n",
      "Epoch: 924, test AUC: 0.8924, test AP: 0.8889, train AUC: 0.9506, train AP: 0.9410, loss:0.9814\n",
      "Epoch: 925, test AUC: 0.8926, test AP: 0.8891, train AUC: 0.9507, train AP: 0.9412, loss:0.9708\n",
      "Epoch: 926, test AUC: 0.8927, test AP: 0.8893, train AUC: 0.9508, train AP: 0.9413, loss:0.9765\n",
      "Epoch: 927, test AUC: 0.8928, test AP: 0.8895, train AUC: 0.9509, train AP: 0.9415, loss:0.9798\n",
      "Epoch: 928, test AUC: 0.8928, test AP: 0.8895, train AUC: 0.9509, train AP: 0.9415, loss:0.9739\n",
      "Epoch: 929, test AUC: 0.8929, test AP: 0.8895, train AUC: 0.9510, train AP: 0.9415, loss:0.9736\n",
      "Epoch: 930, test AUC: 0.8929, test AP: 0.8895, train AUC: 0.9510, train AP: 0.9415, loss:0.9791\n",
      "Epoch: 931, test AUC: 0.8929, test AP: 0.8894, train AUC: 0.9510, train AP: 0.9415, loss:0.9821\n",
      "Epoch: 932, test AUC: 0.8929, test AP: 0.8894, train AUC: 0.9510, train AP: 0.9415, loss:0.9771\n",
      "Epoch: 933, test AUC: 0.8928, test AP: 0.8894, train AUC: 0.9509, train AP: 0.9414, loss:0.9779\n",
      "Epoch: 934, test AUC: 0.8928, test AP: 0.8893, train AUC: 0.9509, train AP: 0.9413, loss:0.9766\n",
      "Epoch: 935, test AUC: 0.8927, test AP: 0.8892, train AUC: 0.9509, train AP: 0.9413, loss:0.9753\n",
      "Epoch: 936, test AUC: 0.8927, test AP: 0.8892, train AUC: 0.9509, train AP: 0.9413, loss:0.9689\n",
      "Epoch: 937, test AUC: 0.8927, test AP: 0.8892, train AUC: 0.9509, train AP: 0.9414, loss:0.9719\n",
      "Epoch: 938, test AUC: 0.8928, test AP: 0.8893, train AUC: 0.9509, train AP: 0.9414, loss:0.9778\n",
      "Epoch: 939, test AUC: 0.8928, test AP: 0.8893, train AUC: 0.9510, train AP: 0.9415, loss:0.9741\n",
      "Epoch: 940, test AUC: 0.8929, test AP: 0.8894, train AUC: 0.9510, train AP: 0.9415, loss:0.9763\n",
      "Epoch: 941, test AUC: 0.8929, test AP: 0.8895, train AUC: 0.9511, train AP: 0.9416, loss:0.9800\n",
      "Epoch: 942, test AUC: 0.8929, test AP: 0.8895, train AUC: 0.9511, train AP: 0.9416, loss:0.9680\n",
      "Epoch: 943, test AUC: 0.8929, test AP: 0.8895, train AUC: 0.9511, train AP: 0.9416, loss:0.9789\n",
      "Epoch: 944, test AUC: 0.8929, test AP: 0.8894, train AUC: 0.9511, train AP: 0.9416, loss:0.9726\n",
      "Epoch: 945, test AUC: 0.8929, test AP: 0.8895, train AUC: 0.9511, train AP: 0.9416, loss:0.9729\n",
      "Epoch: 946, test AUC: 0.8929, test AP: 0.8895, train AUC: 0.9512, train AP: 0.9417, loss:0.9786\n",
      "Epoch: 947, test AUC: 0.8930, test AP: 0.8896, train AUC: 0.9512, train AP: 0.9418, loss:0.9771\n",
      "Epoch: 948, test AUC: 0.8931, test AP: 0.8897, train AUC: 0.9513, train AP: 0.9419, loss:0.9775\n",
      "Epoch: 949, test AUC: 0.8931, test AP: 0.8898, train AUC: 0.9514, train AP: 0.9420, loss:0.9736\n",
      "Epoch: 950, test AUC: 0.8932, test AP: 0.8898, train AUC: 0.9515, train AP: 0.9421, loss:0.9746\n",
      "Epoch: 951, test AUC: 0.8931, test AP: 0.8898, train AUC: 0.9514, train AP: 0.9420, loss:0.9742\n",
      "Epoch: 952, test AUC: 0.8931, test AP: 0.8897, train AUC: 0.9514, train AP: 0.9420, loss:0.9754\n",
      "Epoch: 953, test AUC: 0.8931, test AP: 0.8897, train AUC: 0.9515, train AP: 0.9420, loss:0.9751\n",
      "Epoch: 954, test AUC: 0.8931, test AP: 0.8897, train AUC: 0.9515, train AP: 0.9421, loss:0.9760\n",
      "Epoch: 955, test AUC: 0.8931, test AP: 0.8898, train AUC: 0.9516, train AP: 0.9421, loss:0.9773\n",
      "Epoch: 956, test AUC: 0.8932, test AP: 0.8898, train AUC: 0.9516, train AP: 0.9422, loss:0.9733\n",
      "Epoch: 957, test AUC: 0.8932, test AP: 0.8899, train AUC: 0.9516, train AP: 0.9422, loss:0.9764\n",
      "Epoch: 958, test AUC: 0.8933, test AP: 0.8899, train AUC: 0.9517, train AP: 0.9423, loss:0.9679\n",
      "Epoch: 959, test AUC: 0.8933, test AP: 0.8900, train AUC: 0.9517, train AP: 0.9423, loss:0.9736\n",
      "Epoch: 960, test AUC: 0.8933, test AP: 0.8900, train AUC: 0.9516, train AP: 0.9423, loss:0.9734\n",
      "Epoch: 961, test AUC: 0.8933, test AP: 0.8899, train AUC: 0.9516, train AP: 0.9422, loss:0.9702\n",
      "Epoch: 962, test AUC: 0.8933, test AP: 0.8899, train AUC: 0.9516, train AP: 0.9422, loss:0.9733\n",
      "Epoch: 963, test AUC: 0.8933, test AP: 0.8899, train AUC: 0.9515, train AP: 0.9421, loss:0.9770\n",
      "Epoch: 964, test AUC: 0.8933, test AP: 0.8899, train AUC: 0.9516, train AP: 0.9422, loss:0.9785\n",
      "Epoch: 965, test AUC: 0.8934, test AP: 0.8900, train AUC: 0.9517, train AP: 0.9423, loss:0.9723\n",
      "Epoch: 966, test AUC: 0.8935, test AP: 0.8901, train AUC: 0.9517, train AP: 0.9423, loss:0.9710\n",
      "Epoch: 967, test AUC: 0.8935, test AP: 0.8901, train AUC: 0.9518, train AP: 0.9424, loss:0.9687\n",
      "Epoch: 968, test AUC: 0.8935, test AP: 0.8902, train AUC: 0.9518, train AP: 0.9425, loss:0.9746\n",
      "Epoch: 969, test AUC: 0.8936, test AP: 0.8903, train AUC: 0.9519, train AP: 0.9425, loss:0.9730\n",
      "Epoch: 970, test AUC: 0.8936, test AP: 0.8903, train AUC: 0.9519, train AP: 0.9426, loss:0.9717\n",
      "Epoch: 971, test AUC: 0.8937, test AP: 0.8904, train AUC: 0.9520, train AP: 0.9427, loss:0.9769\n",
      "Epoch: 972, test AUC: 0.8937, test AP: 0.8904, train AUC: 0.9520, train AP: 0.9427, loss:0.9702\n",
      "Epoch: 973, test AUC: 0.8937, test AP: 0.8904, train AUC: 0.9521, train AP: 0.9427, loss:0.9701\n",
      "Epoch: 974, test AUC: 0.8936, test AP: 0.8903, train AUC: 0.9520, train AP: 0.9426, loss:0.9768\n",
      "Epoch: 975, test AUC: 0.8935, test AP: 0.8902, train AUC: 0.9519, train AP: 0.9425, loss:0.9809\n",
      "Epoch: 976, test AUC: 0.8935, test AP: 0.8902, train AUC: 0.9519, train AP: 0.9426, loss:0.9822\n",
      "Epoch: 977, test AUC: 0.8935, test AP: 0.8902, train AUC: 0.9520, train AP: 0.9426, loss:0.9699\n",
      "Epoch: 978, test AUC: 0.8935, test AP: 0.8902, train AUC: 0.9520, train AP: 0.9426, loss:0.9751\n",
      "Epoch: 979, test AUC: 0.8935, test AP: 0.8902, train AUC: 0.9519, train AP: 0.9425, loss:0.9660\n",
      "Epoch: 980, test AUC: 0.8935, test AP: 0.8902, train AUC: 0.9519, train AP: 0.9425, loss:0.9720\n",
      "Epoch: 981, test AUC: 0.8935, test AP: 0.8902, train AUC: 0.9519, train AP: 0.9425, loss:0.9751\n",
      "Epoch: 982, test AUC: 0.8935, test AP: 0.8901, train AUC: 0.9519, train AP: 0.9425, loss:0.9720\n",
      "Epoch: 983, test AUC: 0.8935, test AP: 0.8901, train AUC: 0.9519, train AP: 0.9425, loss:0.9751\n",
      "Epoch: 984, test AUC: 0.8935, test AP: 0.8901, train AUC: 0.9519, train AP: 0.9425, loss:0.9723\n",
      "Epoch: 985, test AUC: 0.8934, test AP: 0.8900, train AUC: 0.9519, train AP: 0.9424, loss:0.9710\n",
      "Epoch: 986, test AUC: 0.8935, test AP: 0.8901, train AUC: 0.9519, train AP: 0.9425, loss:0.9751\n",
      "Epoch: 987, test AUC: 0.8936, test AP: 0.8902, train AUC: 0.9520, train AP: 0.9426, loss:0.9792\n",
      "Epoch: 988, test AUC: 0.8936, test AP: 0.8903, train AUC: 0.9521, train AP: 0.9427, loss:0.9769\n",
      "Epoch: 989, test AUC: 0.8937, test AP: 0.8904, train AUC: 0.9522, train AP: 0.9428, loss:0.9707\n",
      "Epoch: 990, test AUC: 0.8937, test AP: 0.8904, train AUC: 0.9522, train AP: 0.9428, loss:0.9680\n",
      "Epoch: 991, test AUC: 0.8937, test AP: 0.8905, train AUC: 0.9522, train AP: 0.9429, loss:0.9739\n",
      "Epoch: 992, test AUC: 0.8939, test AP: 0.8906, train AUC: 0.9523, train AP: 0.9430, loss:0.9758\n",
      "Epoch: 993, test AUC: 0.8939, test AP: 0.8907, train AUC: 0.9524, train AP: 0.9431, loss:0.9694\n",
      "Epoch: 994, test AUC: 0.8940, test AP: 0.8908, train AUC: 0.9525, train AP: 0.9432, loss:0.9722\n",
      "Epoch: 995, test AUC: 0.8941, test AP: 0.8909, train AUC: 0.9526, train AP: 0.9433, loss:0.9791\n",
      "Epoch: 996, test AUC: 0.8941, test AP: 0.8909, train AUC: 0.9526, train AP: 0.9434, loss:0.9770\n",
      "Epoch: 997, test AUC: 0.8942, test AP: 0.8910, train AUC: 0.9527, train AP: 0.9434, loss:0.9761\n",
      "Epoch: 998, test AUC: 0.8941, test AP: 0.8909, train AUC: 0.9526, train AP: 0.9433, loss:0.9755\n",
      "Epoch: 999, test AUC: 0.8940, test AP: 0.8908, train AUC: 0.9525, train AP: 0.9432, loss:0.9762\n",
      "Epoch: 1000, test AUC: 0.8940, test AP: 0.8907, train AUC: 0.9524, train AP: 0.9431, loss:0.9765\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "losses = []\n",
    "test_auc = []\n",
    "test_ap = []\n",
    "train_aucs = []\n",
    "train_aps = []\n",
    "\n",
    "optimizer = torch.optim.Adam(gae_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = gae_train(train_set, gae_model, optimizer)\n",
    "    losses.append(loss)\n",
    "    auc, ap = gae_test(test_set, gae_model)\n",
    "    test_auc.append(auc)\n",
    "    test_ap.append(ap)\n",
    "\n",
    "    train_auc, train_ap = gae_test(train_set, gae_model)\n",
    "\n",
    "    train_aucs.append(train_auc)\n",
    "    train_aps.append(train_ap)\n",
    "\n",
    "    print('Epoch: {:03d}, test AUC: {:.4f}, test AP: {:.4f}, train AUC: {:.4f}, train AP: {:.4f}, loss:{:.4f}'.format(epoch, auc, ap, train_auc, train_ap, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\embedded_features\\cd4'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "torch.save(gae_model, r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\embedded_features\\cd4\\cd4_autoencoder_set1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\embedded_features\\cd4\\cd4_autoencoder_set1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[12224, 15], edge_index=[2, 71316])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = T.Compose([T.ToUndirected()])\n",
    "transformed_data = t2(data)\n",
    "transformed_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_embed = gae_model.encode(transformed_data.x, transformed_data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_embed = z_embed.cpu()\n",
    "numpy_z = z_embed.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12224, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>col</th>\n",
       "      <th>z</th>\n",
       "      <th>patch_id</th>\n",
       "      <th>cellID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1052.462366</td>\n",
       "      <td>1384.064516</td>\n",
       "      <td>11.741935</td>\n",
       "      <td>000_1_0</td>\n",
       "      <td>000_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262.181818</td>\n",
       "      <td>1810.431818</td>\n",
       "      <td>22.363636</td>\n",
       "      <td>000_1_1</td>\n",
       "      <td>000_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1364.704545</td>\n",
       "      <td>1427.886364</td>\n",
       "      <td>8.727273</td>\n",
       "      <td>000_1_2</td>\n",
       "      <td>000_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1194.296296</td>\n",
       "      <td>1835.246914</td>\n",
       "      <td>11.740741</td>\n",
       "      <td>000_1_3</td>\n",
       "      <td>000_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1342.506173</td>\n",
       "      <td>1526.691358</td>\n",
       "      <td>3.851852</td>\n",
       "      <td>000_1_4</td>\n",
       "      <td>000_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12219</th>\n",
       "      <td>1442.291667</td>\n",
       "      <td>1604.791667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>029_6_48</td>\n",
       "      <td>029_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12220</th>\n",
       "      <td>1299.375000</td>\n",
       "      <td>2017.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>029_6_49</td>\n",
       "      <td>029_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12221</th>\n",
       "      <td>1403.652174</td>\n",
       "      <td>1611.478261</td>\n",
       "      <td>1.043478</td>\n",
       "      <td>029_6_50</td>\n",
       "      <td>029_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12222</th>\n",
       "      <td>1313.894737</td>\n",
       "      <td>1677.684211</td>\n",
       "      <td>46.263158</td>\n",
       "      <td>029_6_51</td>\n",
       "      <td>029_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12223</th>\n",
       "      <td>1463.947368</td>\n",
       "      <td>1761.789474</td>\n",
       "      <td>57.473684</td>\n",
       "      <td>029_6_52</td>\n",
       "      <td>029_6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12224 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               row          col          z  patch_id cellID\n",
       "0      1052.462366  1384.064516  11.741935   000_1_0  000_1\n",
       "1      1262.181818  1810.431818  22.363636   000_1_1  000_1\n",
       "2      1364.704545  1427.886364   8.727273   000_1_2  000_1\n",
       "3      1194.296296  1835.246914  11.740741   000_1_3  000_1\n",
       "4      1342.506173  1526.691358   3.851852   000_1_4  000_1\n",
       "...            ...          ...        ...       ...    ...\n",
       "12219  1442.291667  1604.791667   0.125000  029_6_48  029_6\n",
       "12220  1299.375000  2017.750000   0.000000  029_6_49  029_6\n",
       "12221  1403.652174  1611.478261   1.043478  029_6_50  029_6\n",
       "12222  1313.894737  1677.684211  46.263158  029_6_51  029_6\n",
       "12223  1463.947368  1761.789474  57.473684  029_6_52  029_6\n",
       "\n",
       "[12224 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_df = pd.DataFrame(numpy_z, index=proxi_df.index)\n",
    "z_df.insert(loc=z_df.shape[1], column='patch', value=proxi_df.index)\n",
    "z_df.insert(loc=z_df.shape[1], column='cellID', value=patch_centers['cellID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_df.to_csv(r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\embedded_features\\cd4\\set1_proxi_embedding.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-set training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = ['set3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:00<00:00, 5023.20it/s]\n",
      "100%|██████████| 211/211 [00:00<?, ?it/s]\n",
      "47110it [00:28, 1626.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, test AUC: 0.6785, test AP: 0.6894, train AUC: 0.7560, train AP: 0.7572, loss:1.4436\n",
      "Epoch: 002, test AUC: 0.6839, test AP: 0.6938, train AUC: 0.7616, train AP: 0.7617, loss:1.4398\n",
      "Epoch: 003, test AUC: 0.6893, test AP: 0.6981, train AUC: 0.7673, train AP: 0.7663, loss:1.3963\n",
      "Epoch: 004, test AUC: 0.6948, test AP: 0.7026, train AUC: 0.7729, train AP: 0.7709, loss:1.4031\n",
      "Epoch: 005, test AUC: 0.7002, test AP: 0.7071, train AUC: 0.7786, train AP: 0.7755, loss:1.3653\n",
      "Epoch: 006, test AUC: 0.7056, test AP: 0.7115, train AUC: 0.7841, train AP: 0.7802, loss:1.3627\n",
      "Epoch: 007, test AUC: 0.7109, test AP: 0.7160, train AUC: 0.7896, train AP: 0.7848, loss:1.3273\n",
      "Epoch: 008, test AUC: 0.7161, test AP: 0.7204, train AUC: 0.7950, train AP: 0.7893, loss:1.3218\n",
      "Epoch: 009, test AUC: 0.7212, test AP: 0.7247, train AUC: 0.8002, train AP: 0.7938, loss:1.3057\n",
      "Epoch: 010, test AUC: 0.7262, test AP: 0.7290, train AUC: 0.8052, train AP: 0.7981, loss:1.2794\n",
      "Epoch: 011, test AUC: 0.7310, test AP: 0.7331, train AUC: 0.8102, train AP: 0.8024, loss:1.2782\n",
      "Epoch: 012, test AUC: 0.7357, test AP: 0.7372, train AUC: 0.8149, train AP: 0.8066, loss:1.2528\n",
      "Epoch: 013, test AUC: 0.7402, test AP: 0.7411, train AUC: 0.8195, train AP: 0.8106, loss:1.2466\n",
      "Epoch: 014, test AUC: 0.7446, test AP: 0.7450, train AUC: 0.8239, train AP: 0.8145, loss:1.2351\n",
      "Epoch: 015, test AUC: 0.7488, test AP: 0.7487, train AUC: 0.8281, train AP: 0.8183, loss:1.2393\n",
      "Epoch: 016, test AUC: 0.7528, test AP: 0.7523, train AUC: 0.8321, train AP: 0.8219, loss:1.2150\n",
      "Epoch: 017, test AUC: 0.7566, test AP: 0.7557, train AUC: 0.8359, train AP: 0.8253, loss:1.2048\n",
      "Epoch: 018, test AUC: 0.7603, test AP: 0.7589, train AUC: 0.8395, train AP: 0.8286, loss:1.2016\n",
      "Epoch: 019, test AUC: 0.7638, test AP: 0.7620, train AUC: 0.8429, train AP: 0.8318, loss:1.2005\n",
      "Epoch: 020, test AUC: 0.7671, test AP: 0.7650, train AUC: 0.8461, train AP: 0.8348, loss:1.1868\n",
      "Epoch: 021, test AUC: 0.7703, test AP: 0.7678, train AUC: 0.8492, train AP: 0.8376, loss:1.1902\n",
      "Epoch: 022, test AUC: 0.7733, test AP: 0.7705, train AUC: 0.8521, train AP: 0.8404, loss:1.1859\n",
      "Epoch: 023, test AUC: 0.7761, test AP: 0.7730, train AUC: 0.8549, train AP: 0.8429, loss:1.1716\n",
      "Epoch: 024, test AUC: 0.7788, test AP: 0.7754, train AUC: 0.8575, train AP: 0.8454, loss:1.1772\n",
      "Epoch: 025, test AUC: 0.7814, test AP: 0.7777, train AUC: 0.8600, train AP: 0.8477, loss:1.1653\n",
      "Epoch: 026, test AUC: 0.7838, test AP: 0.7798, train AUC: 0.8623, train AP: 0.8499, loss:1.1610\n",
      "Epoch: 027, test AUC: 0.7860, test AP: 0.7818, train AUC: 0.8644, train AP: 0.8519, loss:1.1584\n",
      "Epoch: 028, test AUC: 0.7881, test AP: 0.7837, train AUC: 0.8665, train AP: 0.8539, loss:1.1570\n",
      "Epoch: 029, test AUC: 0.7901, test AP: 0.7855, train AUC: 0.8684, train AP: 0.8557, loss:1.1487\n",
      "Epoch: 030, test AUC: 0.7920, test AP: 0.7871, train AUC: 0.8702, train AP: 0.8574, loss:1.1515\n",
      "Epoch: 031, test AUC: 0.7938, test AP: 0.7887, train AUC: 0.8719, train AP: 0.8590, loss:1.1449\n",
      "Epoch: 032, test AUC: 0.7954, test AP: 0.7902, train AUC: 0.8735, train AP: 0.8606, loss:1.1457\n",
      "Epoch: 033, test AUC: 0.7970, test AP: 0.7917, train AUC: 0.8750, train AP: 0.8620, loss:1.1412\n",
      "Epoch: 034, test AUC: 0.7985, test AP: 0.7930, train AUC: 0.8765, train AP: 0.8634, loss:1.1375\n",
      "Epoch: 035, test AUC: 0.7999, test AP: 0.7943, train AUC: 0.8778, train AP: 0.8647, loss:1.1340\n",
      "Epoch: 036, test AUC: 0.8013, test AP: 0.7955, train AUC: 0.8791, train AP: 0.8659, loss:1.1311\n",
      "Epoch: 037, test AUC: 0.8025, test AP: 0.7966, train AUC: 0.8803, train AP: 0.8670, loss:1.1276\n",
      "Epoch: 038, test AUC: 0.8037, test AP: 0.7976, train AUC: 0.8814, train AP: 0.8681, loss:1.1240\n",
      "Epoch: 039, test AUC: 0.8049, test AP: 0.7986, train AUC: 0.8825, train AP: 0.8691, loss:1.1252\n",
      "Epoch: 040, test AUC: 0.8059, test AP: 0.7996, train AUC: 0.8835, train AP: 0.8701, loss:1.1239\n",
      "Epoch: 041, test AUC: 0.8070, test AP: 0.8005, train AUC: 0.8844, train AP: 0.8710, loss:1.1216\n",
      "Epoch: 042, test AUC: 0.8080, test AP: 0.8014, train AUC: 0.8853, train AP: 0.8719, loss:1.1249\n",
      "Epoch: 043, test AUC: 0.8089, test AP: 0.8022, train AUC: 0.8861, train AP: 0.8727, loss:1.1191\n",
      "Epoch: 044, test AUC: 0.8098, test AP: 0.8029, train AUC: 0.8869, train AP: 0.8734, loss:1.1215\n",
      "Epoch: 045, test AUC: 0.8106, test AP: 0.8037, train AUC: 0.8877, train AP: 0.8742, loss:1.1106\n",
      "Epoch: 046, test AUC: 0.8114, test AP: 0.8044, train AUC: 0.8884, train AP: 0.8748, loss:1.1155\n",
      "Epoch: 047, test AUC: 0.8121, test AP: 0.8050, train AUC: 0.8890, train AP: 0.8755, loss:1.1104\n",
      "Epoch: 048, test AUC: 0.8128, test AP: 0.8056, train AUC: 0.8896, train AP: 0.8761, loss:1.1091\n",
      "Epoch: 049, test AUC: 0.8135, test AP: 0.8062, train AUC: 0.8902, train AP: 0.8766, loss:1.1101\n",
      "Epoch: 050, test AUC: 0.8141, test AP: 0.8067, train AUC: 0.8907, train AP: 0.8771, loss:1.1055\n",
      "Epoch: 051, test AUC: 0.8146, test AP: 0.8072, train AUC: 0.8912, train AP: 0.8776, loss:1.1074\n",
      "Epoch: 052, test AUC: 0.8152, test AP: 0.8077, train AUC: 0.8917, train AP: 0.8781, loss:1.1036\n",
      "Epoch: 053, test AUC: 0.8157, test AP: 0.8081, train AUC: 0.8921, train AP: 0.8786, loss:1.1017\n",
      "Epoch: 054, test AUC: 0.8162, test AP: 0.8085, train AUC: 0.8926, train AP: 0.8790, loss:1.1010\n",
      "Epoch: 055, test AUC: 0.8166, test AP: 0.8089, train AUC: 0.8930, train AP: 0.8794, loss:1.0965\n",
      "Epoch: 056, test AUC: 0.8170, test AP: 0.8092, train AUC: 0.8933, train AP: 0.8798, loss:1.0950\n",
      "Epoch: 057, test AUC: 0.8174, test AP: 0.8096, train AUC: 0.8937, train AP: 0.8801, loss:1.0959\n",
      "Epoch: 058, test AUC: 0.8178, test AP: 0.8099, train AUC: 0.8940, train AP: 0.8805, loss:1.1004\n",
      "Epoch: 059, test AUC: 0.8181, test AP: 0.8102, train AUC: 0.8943, train AP: 0.8808, loss:1.0939\n",
      "Epoch: 060, test AUC: 0.8185, test AP: 0.8105, train AUC: 0.8946, train AP: 0.8811, loss:1.0896\n",
      "Epoch: 061, test AUC: 0.8188, test AP: 0.8108, train AUC: 0.8949, train AP: 0.8814, loss:1.0927\n",
      "Epoch: 062, test AUC: 0.8191, test AP: 0.8110, train AUC: 0.8952, train AP: 0.8817, loss:1.0883\n",
      "Epoch: 063, test AUC: 0.8194, test AP: 0.8113, train AUC: 0.8954, train AP: 0.8820, loss:1.0933\n",
      "Epoch: 064, test AUC: 0.8197, test AP: 0.8115, train AUC: 0.8957, train AP: 0.8822, loss:1.0871\n",
      "Epoch: 065, test AUC: 0.8199, test AP: 0.8117, train AUC: 0.8959, train AP: 0.8825, loss:1.0881\n",
      "Epoch: 066, test AUC: 0.8202, test AP: 0.8120, train AUC: 0.8962, train AP: 0.8827, loss:1.0881\n",
      "Epoch: 067, test AUC: 0.8205, test AP: 0.8122, train AUC: 0.8964, train AP: 0.8830, loss:1.0875\n",
      "Epoch: 068, test AUC: 0.8207, test AP: 0.8124, train AUC: 0.8966, train AP: 0.8832, loss:1.0860\n",
      "Epoch: 069, test AUC: 0.8209, test AP: 0.8126, train AUC: 0.8968, train AP: 0.8834, loss:1.0893\n",
      "Epoch: 070, test AUC: 0.8211, test AP: 0.8128, train AUC: 0.8970, train AP: 0.8837, loss:1.0807\n",
      "Epoch: 071, test AUC: 0.8213, test AP: 0.8129, train AUC: 0.8972, train AP: 0.8839, loss:1.0879\n",
      "Epoch: 072, test AUC: 0.8215, test AP: 0.8131, train AUC: 0.8974, train AP: 0.8841, loss:1.0789\n",
      "Epoch: 073, test AUC: 0.8217, test AP: 0.8133, train AUC: 0.8976, train AP: 0.8843, loss:1.0797\n",
      "Epoch: 074, test AUC: 0.8220, test AP: 0.8135, train AUC: 0.8978, train AP: 0.8845, loss:1.0815\n",
      "Epoch: 075, test AUC: 0.8222, test AP: 0.8136, train AUC: 0.8980, train AP: 0.8847, loss:1.0834\n",
      "Epoch: 076, test AUC: 0.8224, test AP: 0.8138, train AUC: 0.8982, train AP: 0.8849, loss:1.0785\n",
      "Epoch: 077, test AUC: 0.8226, test AP: 0.8140, train AUC: 0.8984, train AP: 0.8851, loss:1.0782\n",
      "Epoch: 078, test AUC: 0.8227, test AP: 0.8142, train AUC: 0.8986, train AP: 0.8853, loss:1.0780\n",
      "Epoch: 079, test AUC: 0.8229, test AP: 0.8143, train AUC: 0.8987, train AP: 0.8855, loss:1.0799\n",
      "Epoch: 080, test AUC: 0.8231, test AP: 0.8145, train AUC: 0.8989, train AP: 0.8857, loss:1.0779\n",
      "Epoch: 081, test AUC: 0.8233, test AP: 0.8147, train AUC: 0.8991, train AP: 0.8859, loss:1.0743\n",
      "Epoch: 082, test AUC: 0.8235, test AP: 0.8148, train AUC: 0.8993, train AP: 0.8861, loss:1.0788\n",
      "Epoch: 083, test AUC: 0.8237, test AP: 0.8150, train AUC: 0.8994, train AP: 0.8863, loss:1.0797\n",
      "Epoch: 084, test AUC: 0.8239, test AP: 0.8151, train AUC: 0.8996, train AP: 0.8864, loss:1.0747\n",
      "Epoch: 085, test AUC: 0.8240, test AP: 0.8153, train AUC: 0.8998, train AP: 0.8866, loss:1.0762\n",
      "Epoch: 086, test AUC: 0.8242, test AP: 0.8155, train AUC: 0.9000, train AP: 0.8868, loss:1.0706\n",
      "Epoch: 087, test AUC: 0.8244, test AP: 0.8156, train AUC: 0.9001, train AP: 0.8870, loss:1.0758\n",
      "Epoch: 088, test AUC: 0.8246, test AP: 0.8158, train AUC: 0.9003, train AP: 0.8872, loss:1.0736\n",
      "Epoch: 089, test AUC: 0.8248, test AP: 0.8159, train AUC: 0.9005, train AP: 0.8874, loss:1.0746\n",
      "Epoch: 090, test AUC: 0.8249, test AP: 0.8161, train AUC: 0.9007, train AP: 0.8875, loss:1.0789\n",
      "Epoch: 091, test AUC: 0.8251, test AP: 0.8163, train AUC: 0.9009, train AP: 0.8877, loss:1.0763\n",
      "Epoch: 092, test AUC: 0.8253, test AP: 0.8164, train AUC: 0.9010, train AP: 0.8879, loss:1.0685\n",
      "Epoch: 093, test AUC: 0.8255, test AP: 0.8166, train AUC: 0.9012, train AP: 0.8881, loss:1.0646\n",
      "Epoch: 094, test AUC: 0.8256, test AP: 0.8168, train AUC: 0.9014, train AP: 0.8883, loss:1.0705\n",
      "Epoch: 095, test AUC: 0.8258, test AP: 0.8169, train AUC: 0.9016, train AP: 0.8885, loss:1.0729\n",
      "Epoch: 096, test AUC: 0.8260, test AP: 0.8171, train AUC: 0.9018, train AP: 0.8887, loss:1.0654\n",
      "Epoch: 097, test AUC: 0.8261, test AP: 0.8172, train AUC: 0.9020, train AP: 0.8889, loss:1.0704\n",
      "Epoch: 098, test AUC: 0.8263, test AP: 0.8174, train AUC: 0.9021, train AP: 0.8891, loss:1.0644\n",
      "Epoch: 099, test AUC: 0.8264, test AP: 0.8175, train AUC: 0.9023, train AP: 0.8893, loss:1.0654\n",
      "Epoch: 100, test AUC: 0.8266, test AP: 0.8177, train AUC: 0.9025, train AP: 0.8895, loss:1.0650\n",
      "Epoch: 101, test AUC: 0.8267, test AP: 0.8178, train AUC: 0.9027, train AP: 0.8897, loss:1.0641\n",
      "Epoch: 102, test AUC: 0.8269, test AP: 0.8179, train AUC: 0.9029, train AP: 0.8899, loss:1.0639\n",
      "Epoch: 103, test AUC: 0.8270, test AP: 0.8181, train AUC: 0.9030, train AP: 0.8901, loss:1.0660\n",
      "Epoch: 104, test AUC: 0.8272, test AP: 0.8182, train AUC: 0.9032, train AP: 0.8903, loss:1.0686\n",
      "Epoch: 105, test AUC: 0.8273, test AP: 0.8184, train AUC: 0.9034, train AP: 0.8905, loss:1.0627\n",
      "Epoch: 106, test AUC: 0.8274, test AP: 0.8185, train AUC: 0.9035, train AP: 0.8907, loss:1.0626\n",
      "Epoch: 107, test AUC: 0.8276, test AP: 0.8187, train AUC: 0.9037, train AP: 0.8909, loss:1.0609\n",
      "Epoch: 108, test AUC: 0.8277, test AP: 0.8188, train AUC: 0.9039, train AP: 0.8910, loss:1.0565\n",
      "Epoch: 109, test AUC: 0.8279, test AP: 0.8190, train AUC: 0.9041, train AP: 0.8912, loss:1.0620\n",
      "Epoch: 110, test AUC: 0.8280, test AP: 0.8192, train AUC: 0.9042, train AP: 0.8914, loss:1.0623\n",
      "Epoch: 111, test AUC: 0.8282, test AP: 0.8193, train AUC: 0.9044, train AP: 0.8916, loss:1.0615\n",
      "Epoch: 112, test AUC: 0.8283, test AP: 0.8195, train AUC: 0.9046, train AP: 0.8918, loss:1.0628\n",
      "Epoch: 113, test AUC: 0.8285, test AP: 0.8196, train AUC: 0.9047, train AP: 0.8920, loss:1.0588\n",
      "Epoch: 114, test AUC: 0.8286, test AP: 0.8198, train AUC: 0.9049, train AP: 0.8922, loss:1.0612\n",
      "Epoch: 115, test AUC: 0.8288, test AP: 0.8200, train AUC: 0.9051, train AP: 0.8924, loss:1.0540\n",
      "Epoch: 116, test AUC: 0.8290, test AP: 0.8202, train AUC: 0.9052, train AP: 0.8925, loss:1.0588\n",
      "Epoch: 117, test AUC: 0.8292, test AP: 0.8204, train AUC: 0.9054, train AP: 0.8927, loss:1.0551\n",
      "Epoch: 118, test AUC: 0.8293, test AP: 0.8206, train AUC: 0.9055, train AP: 0.8929, loss:1.0603\n",
      "Epoch: 119, test AUC: 0.8295, test AP: 0.8208, train AUC: 0.9057, train AP: 0.8931, loss:1.0566\n",
      "Epoch: 120, test AUC: 0.8297, test AP: 0.8209, train AUC: 0.9059, train AP: 0.8933, loss:1.0598\n",
      "Epoch: 121, test AUC: 0.8299, test AP: 0.8211, train AUC: 0.9061, train AP: 0.8935, loss:1.0660\n",
      "Epoch: 122, test AUC: 0.8300, test AP: 0.8213, train AUC: 0.9062, train AP: 0.8937, loss:1.0554\n",
      "Epoch: 123, test AUC: 0.8302, test AP: 0.8215, train AUC: 0.9064, train AP: 0.8938, loss:1.0615\n",
      "Epoch: 124, test AUC: 0.8304, test AP: 0.8217, train AUC: 0.9066, train AP: 0.8940, loss:1.0555\n",
      "Epoch: 125, test AUC: 0.8306, test AP: 0.8219, train AUC: 0.9068, train AP: 0.8942, loss:1.0545\n",
      "Epoch: 126, test AUC: 0.8307, test AP: 0.8221, train AUC: 0.9070, train AP: 0.8944, loss:1.0575\n",
      "Epoch: 127, test AUC: 0.8309, test AP: 0.8223, train AUC: 0.9071, train AP: 0.8946, loss:1.0539\n",
      "Epoch: 128, test AUC: 0.8311, test AP: 0.8225, train AUC: 0.9073, train AP: 0.8949, loss:1.0502\n",
      "Epoch: 129, test AUC: 0.8312, test AP: 0.8227, train AUC: 0.9075, train AP: 0.8951, loss:1.0607\n",
      "Epoch: 130, test AUC: 0.8314, test AP: 0.8229, train AUC: 0.9077, train AP: 0.8953, loss:1.0551\n",
      "Epoch: 131, test AUC: 0.8316, test AP: 0.8231, train AUC: 0.9079, train AP: 0.8955, loss:1.0541\n",
      "Epoch: 132, test AUC: 0.8318, test AP: 0.8233, train AUC: 0.9081, train AP: 0.8957, loss:1.0508\n",
      "Epoch: 133, test AUC: 0.8320, test AP: 0.8235, train AUC: 0.9083, train AP: 0.8960, loss:1.0475\n",
      "Epoch: 134, test AUC: 0.8322, test AP: 0.8238, train AUC: 0.9085, train AP: 0.8962, loss:1.0637\n",
      "Epoch: 135, test AUC: 0.8323, test AP: 0.8240, train AUC: 0.9087, train AP: 0.8964, loss:1.0491\n",
      "Epoch: 136, test AUC: 0.8325, test AP: 0.8242, train AUC: 0.9089, train AP: 0.8967, loss:1.0527\n",
      "Epoch: 137, test AUC: 0.8327, test AP: 0.8244, train AUC: 0.9091, train AP: 0.8969, loss:1.0475\n",
      "Epoch: 138, test AUC: 0.8329, test AP: 0.8247, train AUC: 0.9092, train AP: 0.8971, loss:1.0525\n",
      "Epoch: 139, test AUC: 0.8331, test AP: 0.8249, train AUC: 0.9094, train AP: 0.8973, loss:1.0525\n",
      "Epoch: 140, test AUC: 0.8332, test AP: 0.8251, train AUC: 0.9096, train AP: 0.8975, loss:1.0460\n",
      "Epoch: 141, test AUC: 0.8334, test AP: 0.8253, train AUC: 0.9098, train AP: 0.8978, loss:1.0505\n",
      "Epoch: 142, test AUC: 0.8336, test AP: 0.8255, train AUC: 0.9100, train AP: 0.8980, loss:1.0531\n",
      "Epoch: 143, test AUC: 0.8338, test AP: 0.8258, train AUC: 0.9102, train AP: 0.8982, loss:1.0487\n",
      "Epoch: 144, test AUC: 0.8340, test AP: 0.8260, train AUC: 0.9104, train AP: 0.8984, loss:1.0468\n",
      "Epoch: 145, test AUC: 0.8342, test AP: 0.8262, train AUC: 0.9106, train AP: 0.8987, loss:1.0473\n",
      "Epoch: 146, test AUC: 0.8343, test AP: 0.8264, train AUC: 0.9108, train AP: 0.8989, loss:1.0442\n",
      "Epoch: 147, test AUC: 0.8345, test AP: 0.8266, train AUC: 0.9110, train AP: 0.8991, loss:1.0490\n",
      "Epoch: 148, test AUC: 0.8347, test AP: 0.8269, train AUC: 0.9112, train AP: 0.8994, loss:1.0404\n",
      "Epoch: 149, test AUC: 0.8349, test AP: 0.8271, train AUC: 0.9114, train AP: 0.8996, loss:1.0469\n",
      "Epoch: 150, test AUC: 0.8351, test AP: 0.8273, train AUC: 0.9116, train AP: 0.8998, loss:1.0496\n",
      "Epoch: 151, test AUC: 0.8353, test AP: 0.8276, train AUC: 0.9118, train AP: 0.9001, loss:1.0391\n",
      "Epoch: 152, test AUC: 0.8355, test AP: 0.8278, train AUC: 0.9120, train AP: 0.9003, loss:1.0309\n",
      "Epoch: 153, test AUC: 0.8357, test AP: 0.8281, train AUC: 0.9123, train AP: 0.9006, loss:1.0465\n",
      "Epoch: 154, test AUC: 0.8360, test AP: 0.8283, train AUC: 0.9125, train AP: 0.9008, loss:1.0469\n",
      "Epoch: 155, test AUC: 0.8362, test AP: 0.8286, train AUC: 0.9127, train AP: 0.9010, loss:1.0488\n",
      "Epoch: 156, test AUC: 0.8364, test AP: 0.8288, train AUC: 0.9129, train AP: 0.9012, loss:1.0411\n",
      "Epoch: 157, test AUC: 0.8366, test AP: 0.8291, train AUC: 0.9131, train AP: 0.9015, loss:1.0405\n",
      "Epoch: 158, test AUC: 0.8368, test AP: 0.8294, train AUC: 0.9133, train AP: 0.9017, loss:1.0391\n",
      "Epoch: 159, test AUC: 0.8371, test AP: 0.8296, train AUC: 0.9135, train AP: 0.9019, loss:1.0406\n",
      "Epoch: 160, test AUC: 0.8373, test AP: 0.8298, train AUC: 0.9137, train AP: 0.9021, loss:1.0481\n",
      "Epoch: 161, test AUC: 0.8375, test AP: 0.8301, train AUC: 0.9139, train AP: 0.9024, loss:1.0463\n",
      "Epoch: 162, test AUC: 0.8377, test AP: 0.8304, train AUC: 0.9141, train AP: 0.9026, loss:1.0469\n",
      "Epoch: 163, test AUC: 0.8380, test AP: 0.8307, train AUC: 0.9143, train AP: 0.9028, loss:1.0413\n",
      "Epoch: 164, test AUC: 0.8382, test AP: 0.8309, train AUC: 0.9145, train AP: 0.9031, loss:1.0468\n",
      "Epoch: 165, test AUC: 0.8384, test AP: 0.8312, train AUC: 0.9147, train AP: 0.9033, loss:1.0352\n",
      "Epoch: 166, test AUC: 0.8386, test AP: 0.8315, train AUC: 0.9149, train AP: 0.9035, loss:1.0362\n",
      "Epoch: 167, test AUC: 0.8389, test AP: 0.8318, train AUC: 0.9151, train AP: 0.9037, loss:1.0398\n",
      "Epoch: 168, test AUC: 0.8391, test AP: 0.8321, train AUC: 0.9153, train AP: 0.9040, loss:1.0372\n",
      "Epoch: 169, test AUC: 0.8393, test AP: 0.8323, train AUC: 0.9155, train AP: 0.9042, loss:1.0406\n",
      "Epoch: 170, test AUC: 0.8396, test AP: 0.8326, train AUC: 0.9157, train AP: 0.9044, loss:1.0330\n",
      "Epoch: 171, test AUC: 0.8398, test AP: 0.8329, train AUC: 0.9159, train AP: 0.9047, loss:1.0375\n",
      "Epoch: 172, test AUC: 0.8400, test AP: 0.8332, train AUC: 0.9161, train AP: 0.9049, loss:1.0358\n",
      "Epoch: 173, test AUC: 0.8403, test AP: 0.8335, train AUC: 0.9163, train AP: 0.9051, loss:1.0371\n",
      "Epoch: 174, test AUC: 0.8405, test AP: 0.8337, train AUC: 0.9165, train AP: 0.9054, loss:1.0387\n",
      "Epoch: 175, test AUC: 0.8407, test AP: 0.8340, train AUC: 0.9167, train AP: 0.9056, loss:1.0353\n",
      "Epoch: 176, test AUC: 0.8410, test AP: 0.8343, train AUC: 0.9169, train AP: 0.9059, loss:1.0371\n",
      "Epoch: 177, test AUC: 0.8412, test AP: 0.8346, train AUC: 0.9171, train AP: 0.9061, loss:1.0359\n",
      "Epoch: 178, test AUC: 0.8414, test AP: 0.8348, train AUC: 0.9173, train AP: 0.9063, loss:1.0324\n",
      "Epoch: 179, test AUC: 0.8416, test AP: 0.8351, train AUC: 0.9175, train AP: 0.9066, loss:1.0318\n",
      "Epoch: 180, test AUC: 0.8418, test AP: 0.8353, train AUC: 0.9177, train AP: 0.9068, loss:1.0231\n",
      "Epoch: 181, test AUC: 0.8420, test AP: 0.8356, train AUC: 0.9179, train AP: 0.9070, loss:1.0276\n",
      "Epoch: 182, test AUC: 0.8422, test AP: 0.8359, train AUC: 0.9181, train AP: 0.9072, loss:1.0328\n",
      "Epoch: 183, test AUC: 0.8425, test AP: 0.8361, train AUC: 0.9183, train AP: 0.9075, loss:1.0304\n",
      "Epoch: 184, test AUC: 0.8427, test AP: 0.8364, train AUC: 0.9184, train AP: 0.9077, loss:1.0333\n",
      "Epoch: 185, test AUC: 0.8429, test AP: 0.8367, train AUC: 0.9186, train AP: 0.9079, loss:1.0281\n",
      "Epoch: 186, test AUC: 0.8431, test AP: 0.8370, train AUC: 0.9188, train AP: 0.9081, loss:1.0253\n",
      "Epoch: 187, test AUC: 0.8434, test AP: 0.8372, train AUC: 0.9190, train AP: 0.9083, loss:1.0291\n",
      "Epoch: 188, test AUC: 0.8436, test AP: 0.8375, train AUC: 0.9192, train AP: 0.9086, loss:1.0310\n",
      "Epoch: 189, test AUC: 0.8438, test AP: 0.8378, train AUC: 0.9194, train AP: 0.9088, loss:1.0362\n",
      "Epoch: 190, test AUC: 0.8441, test AP: 0.8381, train AUC: 0.9195, train AP: 0.9090, loss:1.0326\n",
      "Epoch: 191, test AUC: 0.8443, test AP: 0.8383, train AUC: 0.9197, train AP: 0.9092, loss:1.0281\n",
      "Epoch: 192, test AUC: 0.8445, test AP: 0.8386, train AUC: 0.9199, train AP: 0.9094, loss:1.0319\n",
      "Epoch: 193, test AUC: 0.8448, test AP: 0.8389, train AUC: 0.9201, train AP: 0.9096, loss:1.0305\n",
      "Epoch: 194, test AUC: 0.8450, test AP: 0.8391, train AUC: 0.9203, train AP: 0.9099, loss:1.0245\n",
      "Epoch: 195, test AUC: 0.8452, test AP: 0.8394, train AUC: 0.9205, train AP: 0.9101, loss:1.0325\n",
      "Epoch: 196, test AUC: 0.8455, test AP: 0.8397, train AUC: 0.9207, train AP: 0.9103, loss:1.0331\n",
      "Epoch: 197, test AUC: 0.8457, test AP: 0.8399, train AUC: 0.9209, train AP: 0.9105, loss:1.0228\n",
      "Epoch: 198, test AUC: 0.8459, test AP: 0.8402, train AUC: 0.9210, train AP: 0.9107, loss:1.0281\n",
      "Epoch: 199, test AUC: 0.8461, test AP: 0.8405, train AUC: 0.9212, train AP: 0.9109, loss:1.0203\n",
      "Epoch: 200, test AUC: 0.8463, test AP: 0.8407, train AUC: 0.9214, train AP: 0.9111, loss:1.0313\n",
      "Epoch: 201, test AUC: 0.8465, test AP: 0.8410, train AUC: 0.9216, train AP: 0.9113, loss:1.0209\n",
      "Epoch: 202, test AUC: 0.8467, test AP: 0.8412, train AUC: 0.9217, train AP: 0.9115, loss:1.0215\n",
      "Epoch: 203, test AUC: 0.8469, test AP: 0.8415, train AUC: 0.9219, train AP: 0.9117, loss:1.0263\n",
      "Epoch: 204, test AUC: 0.8471, test AP: 0.8418, train AUC: 0.9221, train AP: 0.9119, loss:1.0273\n",
      "Epoch: 205, test AUC: 0.8473, test AP: 0.8420, train AUC: 0.9222, train AP: 0.9121, loss:1.0247\n",
      "Epoch: 206, test AUC: 0.8475, test AP: 0.8423, train AUC: 0.9224, train AP: 0.9123, loss:1.0200\n",
      "Epoch: 207, test AUC: 0.8477, test AP: 0.8425, train AUC: 0.9225, train AP: 0.9125, loss:1.0233\n",
      "Epoch: 208, test AUC: 0.8479, test AP: 0.8428, train AUC: 0.9227, train AP: 0.9127, loss:1.0227\n",
      "Epoch: 209, test AUC: 0.8481, test AP: 0.8430, train AUC: 0.9228, train AP: 0.9128, loss:1.0182\n",
      "Epoch: 210, test AUC: 0.8483, test AP: 0.8432, train AUC: 0.9230, train AP: 0.9130, loss:1.0220\n",
      "Epoch: 211, test AUC: 0.8485, test AP: 0.8434, train AUC: 0.9231, train AP: 0.9132, loss:1.0223\n",
      "Epoch: 212, test AUC: 0.8487, test AP: 0.8437, train AUC: 0.9233, train AP: 0.9134, loss:1.0231\n",
      "Epoch: 213, test AUC: 0.8489, test AP: 0.8439, train AUC: 0.9234, train AP: 0.9135, loss:1.0245\n",
      "Epoch: 214, test AUC: 0.8491, test AP: 0.8441, train AUC: 0.9236, train AP: 0.9137, loss:1.0222\n",
      "Epoch: 215, test AUC: 0.8493, test AP: 0.8443, train AUC: 0.9237, train AP: 0.9138, loss:1.0152\n",
      "Epoch: 216, test AUC: 0.8495, test AP: 0.8445, train AUC: 0.9239, train AP: 0.9140, loss:1.0150\n",
      "Epoch: 217, test AUC: 0.8496, test AP: 0.8447, train AUC: 0.9240, train AP: 0.9141, loss:1.0223\n",
      "Epoch: 218, test AUC: 0.8498, test AP: 0.8449, train AUC: 0.9241, train AP: 0.9143, loss:1.0200\n",
      "Epoch: 219, test AUC: 0.8500, test AP: 0.8451, train AUC: 0.9242, train AP: 0.9144, loss:1.0249\n",
      "Epoch: 220, test AUC: 0.8501, test AP: 0.8453, train AUC: 0.9244, train AP: 0.9145, loss:1.0195\n",
      "Epoch: 221, test AUC: 0.8503, test AP: 0.8454, train AUC: 0.9245, train AP: 0.9147, loss:1.0220\n",
      "Epoch: 222, test AUC: 0.8505, test AP: 0.8456, train AUC: 0.9246, train AP: 0.9148, loss:1.0332\n",
      "Epoch: 223, test AUC: 0.8506, test AP: 0.8458, train AUC: 0.9248, train AP: 0.9150, loss:1.0138\n",
      "Epoch: 224, test AUC: 0.8508, test AP: 0.8460, train AUC: 0.9249, train AP: 0.9151, loss:1.0188\n",
      "Epoch: 225, test AUC: 0.8510, test AP: 0.8462, train AUC: 0.9251, train AP: 0.9153, loss:1.0191\n",
      "Epoch: 226, test AUC: 0.8512, test AP: 0.8464, train AUC: 0.9252, train AP: 0.9155, loss:1.0183\n",
      "Epoch: 227, test AUC: 0.8514, test AP: 0.8467, train AUC: 0.9254, train AP: 0.9156, loss:1.0260\n",
      "Epoch: 228, test AUC: 0.8516, test AP: 0.8469, train AUC: 0.9255, train AP: 0.9158, loss:1.0154\n",
      "Epoch: 229, test AUC: 0.8518, test AP: 0.8472, train AUC: 0.9256, train AP: 0.9160, loss:1.0171\n",
      "Epoch: 230, test AUC: 0.8520, test AP: 0.8474, train AUC: 0.9258, train AP: 0.9161, loss:1.0257\n",
      "Epoch: 231, test AUC: 0.8522, test AP: 0.8476, train AUC: 0.9259, train AP: 0.9163, loss:1.0196\n",
      "Epoch: 232, test AUC: 0.8523, test AP: 0.8479, train AUC: 0.9260, train AP: 0.9164, loss:1.0054\n",
      "Epoch: 233, test AUC: 0.8525, test AP: 0.8481, train AUC: 0.9262, train AP: 0.9166, loss:1.0120\n",
      "Epoch: 234, test AUC: 0.8527, test AP: 0.8483, train AUC: 0.9263, train AP: 0.9167, loss:1.0147\n",
      "Epoch: 235, test AUC: 0.8528, test AP: 0.8484, train AUC: 0.9264, train AP: 0.9168, loss:1.0153\n",
      "Epoch: 236, test AUC: 0.8530, test AP: 0.8486, train AUC: 0.9265, train AP: 0.9170, loss:1.0158\n",
      "Epoch: 237, test AUC: 0.8531, test AP: 0.8488, train AUC: 0.9266, train AP: 0.9171, loss:1.0167\n",
      "Epoch: 238, test AUC: 0.8533, test AP: 0.8490, train AUC: 0.9267, train AP: 0.9172, loss:1.0114\n",
      "Epoch: 239, test AUC: 0.8534, test AP: 0.8492, train AUC: 0.9268, train AP: 0.9174, loss:1.0075\n",
      "Epoch: 240, test AUC: 0.8536, test AP: 0.8493, train AUC: 0.9269, train AP: 0.9175, loss:1.0152\n",
      "Epoch: 241, test AUC: 0.8537, test AP: 0.8495, train AUC: 0.9270, train AP: 0.9176, loss:1.0167\n",
      "Epoch: 242, test AUC: 0.8539, test AP: 0.8496, train AUC: 0.9271, train AP: 0.9177, loss:1.0185\n",
      "Epoch: 243, test AUC: 0.8540, test AP: 0.8498, train AUC: 0.9273, train AP: 0.9178, loss:1.0178\n",
      "Epoch: 244, test AUC: 0.8541, test AP: 0.8499, train AUC: 0.9274, train AP: 0.9180, loss:1.0154\n",
      "Epoch: 245, test AUC: 0.8543, test AP: 0.8501, train AUC: 0.9275, train AP: 0.9181, loss:1.0104\n",
      "Epoch: 246, test AUC: 0.8545, test AP: 0.8503, train AUC: 0.9276, train AP: 0.9182, loss:1.0129\n",
      "Epoch: 247, test AUC: 0.8546, test AP: 0.8504, train AUC: 0.9277, train AP: 0.9184, loss:1.0107\n",
      "Epoch: 248, test AUC: 0.8548, test AP: 0.8506, train AUC: 0.9279, train AP: 0.9185, loss:1.0147\n",
      "Epoch: 249, test AUC: 0.8549, test AP: 0.8507, train AUC: 0.9280, train AP: 0.9186, loss:1.0059\n",
      "Epoch: 250, test AUC: 0.8550, test AP: 0.8509, train AUC: 0.9281, train AP: 0.9187, loss:1.0183\n",
      "Epoch: 251, test AUC: 0.8552, test AP: 0.8510, train AUC: 0.9282, train AP: 0.9189, loss:1.0136\n",
      "Epoch: 252, test AUC: 0.8553, test AP: 0.8512, train AUC: 0.9283, train AP: 0.9190, loss:1.0171\n",
      "Epoch: 253, test AUC: 0.8554, test AP: 0.8513, train AUC: 0.9284, train AP: 0.9191, loss:1.0089\n",
      "Epoch: 254, test AUC: 0.8556, test AP: 0.8515, train AUC: 0.9285, train AP: 0.9192, loss:1.0168\n",
      "Epoch: 255, test AUC: 0.8557, test AP: 0.8516, train AUC: 0.9286, train AP: 0.9193, loss:1.0201\n",
      "Epoch: 256, test AUC: 0.8559, test AP: 0.8518, train AUC: 0.9288, train AP: 0.9195, loss:1.0121\n",
      "Epoch: 257, test AUC: 0.8560, test AP: 0.8519, train AUC: 0.9289, train AP: 0.9196, loss:1.0062\n",
      "Epoch: 258, test AUC: 0.8561, test AP: 0.8521, train AUC: 0.9290, train AP: 0.9197, loss:1.0002\n",
      "Epoch: 259, test AUC: 0.8563, test AP: 0.8523, train AUC: 0.9291, train AP: 0.9198, loss:1.0146\n",
      "Epoch: 260, test AUC: 0.8564, test AP: 0.8524, train AUC: 0.9292, train AP: 0.9199, loss:1.0050\n",
      "Epoch: 261, test AUC: 0.8566, test AP: 0.8526, train AUC: 0.9293, train AP: 0.9201, loss:1.0159\n",
      "Epoch: 262, test AUC: 0.8567, test AP: 0.8527, train AUC: 0.9294, train AP: 0.9202, loss:1.0071\n",
      "Epoch: 263, test AUC: 0.8568, test AP: 0.8529, train AUC: 0.9295, train AP: 0.9203, loss:1.0169\n",
      "Epoch: 264, test AUC: 0.8569, test AP: 0.8530, train AUC: 0.9296, train AP: 0.9204, loss:1.0111\n",
      "Epoch: 265, test AUC: 0.8570, test AP: 0.8531, train AUC: 0.9297, train AP: 0.9205, loss:1.0110\n",
      "Epoch: 266, test AUC: 0.8571, test AP: 0.8532, train AUC: 0.9298, train AP: 0.9206, loss:1.0141\n",
      "Epoch: 267, test AUC: 0.8572, test AP: 0.8533, train AUC: 0.9299, train AP: 0.9207, loss:1.0048\n",
      "Epoch: 268, test AUC: 0.8574, test AP: 0.8534, train AUC: 0.9300, train AP: 0.9208, loss:1.0163\n",
      "Epoch: 269, test AUC: 0.8575, test AP: 0.8536, train AUC: 0.9301, train AP: 0.9209, loss:1.0112\n",
      "Epoch: 270, test AUC: 0.8576, test AP: 0.8537, train AUC: 0.9302, train AP: 0.9210, loss:1.0058\n",
      "Epoch: 271, test AUC: 0.8578, test AP: 0.8539, train AUC: 0.9303, train AP: 0.9211, loss:1.0100\n",
      "Epoch: 272, test AUC: 0.8579, test AP: 0.8540, train AUC: 0.9303, train AP: 0.9212, loss:1.0107\n",
      "Epoch: 273, test AUC: 0.8580, test AP: 0.8542, train AUC: 0.9304, train AP: 0.9213, loss:1.0104\n",
      "Epoch: 274, test AUC: 0.8582, test AP: 0.8544, train AUC: 0.9305, train AP: 0.9214, loss:1.0030\n",
      "Epoch: 275, test AUC: 0.8583, test AP: 0.8545, train AUC: 0.9306, train AP: 0.9215, loss:1.0134\n",
      "Epoch: 276, test AUC: 0.8584, test AP: 0.8546, train AUC: 0.9307, train AP: 0.9216, loss:1.0129\n",
      "Epoch: 277, test AUC: 0.8585, test AP: 0.8548, train AUC: 0.9308, train AP: 0.9217, loss:1.0026\n",
      "Epoch: 278, test AUC: 0.8586, test AP: 0.8549, train AUC: 0.9309, train AP: 0.9219, loss:1.0145\n",
      "Epoch: 279, test AUC: 0.8587, test AP: 0.8551, train AUC: 0.9310, train AP: 0.9220, loss:1.0080\n",
      "Epoch: 280, test AUC: 0.8589, test AP: 0.8552, train AUC: 0.9311, train AP: 0.9221, loss:1.0123\n",
      "Epoch: 281, test AUC: 0.8590, test AP: 0.8553, train AUC: 0.9312, train AP: 0.9222, loss:1.0029\n",
      "Epoch: 282, test AUC: 0.8591, test AP: 0.8554, train AUC: 0.9313, train AP: 0.9223, loss:1.0060\n",
      "Epoch: 283, test AUC: 0.8592, test AP: 0.8555, train AUC: 0.9314, train AP: 0.9224, loss:1.0077\n",
      "Epoch: 284, test AUC: 0.8593, test AP: 0.8556, train AUC: 0.9315, train AP: 0.9225, loss:1.0104\n",
      "Epoch: 285, test AUC: 0.8594, test AP: 0.8557, train AUC: 0.9316, train AP: 0.9226, loss:1.0082\n",
      "Epoch: 286, test AUC: 0.8594, test AP: 0.8558, train AUC: 0.9317, train AP: 0.9227, loss:1.0092\n",
      "Epoch: 287, test AUC: 0.8595, test AP: 0.8559, train AUC: 0.9318, train AP: 0.9228, loss:1.0056\n",
      "Epoch: 288, test AUC: 0.8597, test AP: 0.8560, train AUC: 0.9319, train AP: 0.9229, loss:1.0097\n",
      "Epoch: 289, test AUC: 0.8598, test AP: 0.8562, train AUC: 0.9320, train AP: 0.9230, loss:1.0130\n",
      "Epoch: 290, test AUC: 0.8599, test AP: 0.8563, train AUC: 0.9321, train AP: 0.9231, loss:1.0064\n",
      "Epoch: 291, test AUC: 0.8600, test AP: 0.8565, train AUC: 0.9322, train AP: 0.9233, loss:1.0071\n",
      "Epoch: 292, test AUC: 0.8602, test AP: 0.8566, train AUC: 0.9323, train AP: 0.9234, loss:1.0025\n",
      "Epoch: 293, test AUC: 0.8603, test AP: 0.8568, train AUC: 0.9325, train AP: 0.9235, loss:1.0038\n",
      "Epoch: 294, test AUC: 0.8604, test AP: 0.8569, train AUC: 0.9326, train AP: 0.9237, loss:1.0011\n",
      "Epoch: 295, test AUC: 0.8605, test AP: 0.8570, train AUC: 0.9327, train AP: 0.9238, loss:1.0148\n",
      "Epoch: 296, test AUC: 0.8606, test AP: 0.8571, train AUC: 0.9328, train AP: 0.9239, loss:1.0034\n",
      "Epoch: 297, test AUC: 0.8606, test AP: 0.8571, train AUC: 0.9329, train AP: 0.9240, loss:1.0095\n",
      "Epoch: 298, test AUC: 0.8607, test AP: 0.8573, train AUC: 0.9330, train AP: 0.9241, loss:0.9994\n",
      "Epoch: 299, test AUC: 0.8608, test AP: 0.8573, train AUC: 0.9331, train AP: 0.9243, loss:1.0019\n",
      "Epoch: 300, test AUC: 0.8609, test AP: 0.8574, train AUC: 0.9332, train AP: 0.9244, loss:1.0047\n",
      "Epoch: 301, test AUC: 0.8610, test AP: 0.8576, train AUC: 0.9333, train AP: 0.9245, loss:1.0033\n",
      "Epoch: 302, test AUC: 0.8611, test AP: 0.8577, train AUC: 0.9334, train AP: 0.9245, loss:1.0019\n",
      "Epoch: 303, test AUC: 0.8611, test AP: 0.8578, train AUC: 0.9335, train AP: 0.9246, loss:1.0046\n",
      "Epoch: 304, test AUC: 0.8612, test AP: 0.8579, train AUC: 0.9335, train AP: 0.9247, loss:1.0029\n",
      "Epoch: 305, test AUC: 0.8613, test AP: 0.8580, train AUC: 0.9336, train AP: 0.9248, loss:1.0017\n",
      "Epoch: 306, test AUC: 0.8614, test AP: 0.8581, train AUC: 0.9337, train AP: 0.9249, loss:1.0039\n",
      "Epoch: 307, test AUC: 0.8615, test AP: 0.8582, train AUC: 0.9338, train AP: 0.9250, loss:1.0051\n",
      "Epoch: 308, test AUC: 0.8616, test AP: 0.8584, train AUC: 0.9339, train AP: 0.9251, loss:1.0063\n",
      "Epoch: 309, test AUC: 0.8617, test AP: 0.8585, train AUC: 0.9340, train AP: 0.9252, loss:1.0023\n",
      "Epoch: 310, test AUC: 0.8618, test AP: 0.8586, train AUC: 0.9341, train AP: 0.9253, loss:0.9980\n",
      "Epoch: 311, test AUC: 0.8620, test AP: 0.8588, train AUC: 0.9342, train AP: 0.9254, loss:0.9970\n",
      "Epoch: 312, test AUC: 0.8621, test AP: 0.8589, train AUC: 0.9343, train AP: 0.9256, loss:0.9920\n",
      "Epoch: 313, test AUC: 0.8622, test AP: 0.8591, train AUC: 0.9344, train AP: 0.9257, loss:1.0027\n",
      "Epoch: 314, test AUC: 0.8623, test AP: 0.8592, train AUC: 0.9345, train AP: 0.9258, loss:1.0034\n",
      "Epoch: 315, test AUC: 0.8624, test AP: 0.8593, train AUC: 0.9346, train AP: 0.9259, loss:1.0000\n",
      "Epoch: 316, test AUC: 0.8625, test AP: 0.8594, train AUC: 0.9347, train AP: 0.9260, loss:0.9990\n",
      "Epoch: 317, test AUC: 0.8626, test AP: 0.8595, train AUC: 0.9348, train AP: 0.9261, loss:1.0102\n",
      "Epoch: 318, test AUC: 0.8627, test AP: 0.8596, train AUC: 0.9349, train AP: 0.9262, loss:1.0019\n",
      "Epoch: 319, test AUC: 0.8628, test AP: 0.8597, train AUC: 0.9350, train AP: 0.9263, loss:0.9959\n",
      "Epoch: 320, test AUC: 0.8628, test AP: 0.8597, train AUC: 0.9351, train AP: 0.9264, loss:1.0022\n",
      "Epoch: 321, test AUC: 0.8629, test AP: 0.8598, train AUC: 0.9351, train AP: 0.9264, loss:1.0039\n",
      "Epoch: 322, test AUC: 0.8630, test AP: 0.8599, train AUC: 0.9352, train AP: 0.9265, loss:1.0011\n",
      "Epoch: 323, test AUC: 0.8630, test AP: 0.8600, train AUC: 0.9353, train AP: 0.9266, loss:0.9970\n",
      "Epoch: 324, test AUC: 0.8631, test AP: 0.8601, train AUC: 0.9354, train AP: 0.9267, loss:1.0038\n",
      "Epoch: 325, test AUC: 0.8632, test AP: 0.8602, train AUC: 0.9355, train AP: 0.9268, loss:0.9980\n",
      "Epoch: 326, test AUC: 0.8633, test AP: 0.8604, train AUC: 0.9356, train AP: 0.9269, loss:1.0000\n",
      "Epoch: 327, test AUC: 0.8634, test AP: 0.8605, train AUC: 0.9357, train AP: 0.9270, loss:1.0043\n",
      "Epoch: 328, test AUC: 0.8635, test AP: 0.8605, train AUC: 0.9358, train AP: 0.9271, loss:0.9996\n",
      "Epoch: 329, test AUC: 0.8636, test AP: 0.8607, train AUC: 0.9359, train AP: 0.9272, loss:0.9987\n",
      "Epoch: 330, test AUC: 0.8636, test AP: 0.8607, train AUC: 0.9359, train AP: 0.9273, loss:1.0016\n",
      "Epoch: 331, test AUC: 0.8637, test AP: 0.8608, train AUC: 0.9360, train AP: 0.9273, loss:1.0020\n",
      "Epoch: 332, test AUC: 0.8638, test AP: 0.8609, train AUC: 0.9361, train AP: 0.9274, loss:0.9994\n",
      "Epoch: 333, test AUC: 0.8639, test AP: 0.8610, train AUC: 0.9362, train AP: 0.9275, loss:1.0020\n",
      "Epoch: 334, test AUC: 0.8640, test AP: 0.8611, train AUC: 0.9362, train AP: 0.9276, loss:1.0026\n",
      "Epoch: 335, test AUC: 0.8640, test AP: 0.8612, train AUC: 0.9363, train AP: 0.9276, loss:1.0093\n",
      "Epoch: 336, test AUC: 0.8641, test AP: 0.8614, train AUC: 0.9364, train AP: 0.9277, loss:0.9942\n",
      "Epoch: 337, test AUC: 0.8642, test AP: 0.8615, train AUC: 0.9364, train AP: 0.9278, loss:1.0016\n",
      "Epoch: 338, test AUC: 0.8643, test AP: 0.8616, train AUC: 0.9365, train AP: 0.9279, loss:1.0015\n",
      "Epoch: 339, test AUC: 0.8644, test AP: 0.8618, train AUC: 0.9366, train AP: 0.9280, loss:1.0021\n",
      "Epoch: 340, test AUC: 0.8645, test AP: 0.8619, train AUC: 0.9366, train AP: 0.9281, loss:0.9989\n",
      "Epoch: 341, test AUC: 0.8645, test AP: 0.8619, train AUC: 0.9367, train AP: 0.9282, loss:0.9971\n",
      "Epoch: 342, test AUC: 0.8646, test AP: 0.8620, train AUC: 0.9368, train AP: 0.9282, loss:1.0012\n",
      "Epoch: 343, test AUC: 0.8646, test AP: 0.8621, train AUC: 0.9368, train AP: 0.9283, loss:0.9979\n",
      "Epoch: 344, test AUC: 0.8647, test AP: 0.8622, train AUC: 0.9369, train AP: 0.9284, loss:1.0006\n",
      "Epoch: 345, test AUC: 0.8648, test AP: 0.8623, train AUC: 0.9370, train AP: 0.9285, loss:1.0005\n",
      "Epoch: 346, test AUC: 0.8648, test AP: 0.8623, train AUC: 0.9371, train AP: 0.9285, loss:0.9998\n",
      "Epoch: 347, test AUC: 0.8649, test AP: 0.8624, train AUC: 0.9372, train AP: 0.9286, loss:0.9946\n",
      "Epoch: 348, test AUC: 0.8650, test AP: 0.8625, train AUC: 0.9373, train AP: 0.9287, loss:0.9993\n",
      "Epoch: 349, test AUC: 0.8650, test AP: 0.8625, train AUC: 0.9374, train AP: 0.9288, loss:0.9962\n",
      "Epoch: 350, test AUC: 0.8650, test AP: 0.8626, train AUC: 0.9374, train AP: 0.9289, loss:1.0016\n",
      "Epoch: 351, test AUC: 0.8651, test AP: 0.8626, train AUC: 0.9375, train AP: 0.9290, loss:1.0020\n",
      "Epoch: 352, test AUC: 0.8651, test AP: 0.8627, train AUC: 0.9376, train AP: 0.9290, loss:1.0021\n",
      "Epoch: 353, test AUC: 0.8652, test AP: 0.8627, train AUC: 0.9376, train AP: 0.9291, loss:0.9996\n",
      "Epoch: 354, test AUC: 0.8652, test AP: 0.8628, train AUC: 0.9377, train AP: 0.9292, loss:0.9958\n",
      "Epoch: 355, test AUC: 0.8653, test AP: 0.8628, train AUC: 0.9378, train AP: 0.9292, loss:0.9954\n",
      "Epoch: 356, test AUC: 0.8653, test AP: 0.8629, train AUC: 0.9378, train AP: 0.9293, loss:0.9977\n",
      "Epoch: 357, test AUC: 0.8654, test AP: 0.8630, train AUC: 0.9379, train AP: 0.9294, loss:0.9994\n",
      "Epoch: 358, test AUC: 0.8655, test AP: 0.8631, train AUC: 0.9380, train AP: 0.9295, loss:0.9937\n",
      "Epoch: 359, test AUC: 0.8656, test AP: 0.8632, train AUC: 0.9380, train AP: 0.9296, loss:1.0026\n",
      "Epoch: 360, test AUC: 0.8657, test AP: 0.8633, train AUC: 0.9381, train AP: 0.9297, loss:0.9961\n",
      "Epoch: 361, test AUC: 0.8658, test AP: 0.8634, train AUC: 0.9382, train AP: 0.9297, loss:0.9992\n",
      "Epoch: 362, test AUC: 0.8659, test AP: 0.8635, train AUC: 0.9383, train AP: 0.9298, loss:0.9954\n",
      "Epoch: 363, test AUC: 0.8660, test AP: 0.8637, train AUC: 0.9384, train AP: 0.9299, loss:0.9930\n",
      "Epoch: 364, test AUC: 0.8662, test AP: 0.8638, train AUC: 0.9385, train AP: 0.9300, loss:0.9991\n",
      "Epoch: 365, test AUC: 0.8662, test AP: 0.8638, train AUC: 0.9385, train AP: 0.9301, loss:0.9951\n",
      "Epoch: 366, test AUC: 0.8663, test AP: 0.8639, train AUC: 0.9386, train AP: 0.9301, loss:0.9967\n",
      "Epoch: 367, test AUC: 0.8663, test AP: 0.8639, train AUC: 0.9387, train AP: 0.9302, loss:1.0043\n",
      "Epoch: 368, test AUC: 0.8663, test AP: 0.8639, train AUC: 0.9387, train AP: 0.9303, loss:1.0006\n",
      "Epoch: 369, test AUC: 0.8664, test AP: 0.8639, train AUC: 0.9388, train AP: 0.9303, loss:0.9922\n",
      "Epoch: 370, test AUC: 0.8664, test AP: 0.8640, train AUC: 0.9388, train AP: 0.9304, loss:0.9924\n",
      "Epoch: 371, test AUC: 0.8664, test AP: 0.8640, train AUC: 0.9389, train AP: 0.9304, loss:0.9957\n",
      "Epoch: 372, test AUC: 0.8664, test AP: 0.8640, train AUC: 0.9389, train AP: 0.9305, loss:0.9988\n",
      "Epoch: 373, test AUC: 0.8664, test AP: 0.8641, train AUC: 0.9390, train AP: 0.9305, loss:0.9964\n",
      "Epoch: 374, test AUC: 0.8665, test AP: 0.8641, train AUC: 0.9390, train AP: 0.9306, loss:0.9947\n",
      "Epoch: 375, test AUC: 0.8665, test AP: 0.8642, train AUC: 0.9391, train AP: 0.9307, loss:0.9911\n",
      "Epoch: 376, test AUC: 0.8666, test AP: 0.8643, train AUC: 0.9392, train AP: 0.9308, loss:0.9960\n",
      "Epoch: 377, test AUC: 0.8666, test AP: 0.8644, train AUC: 0.9393, train AP: 0.9309, loss:0.9961\n",
      "Epoch: 378, test AUC: 0.8667, test AP: 0.8644, train AUC: 0.9394, train AP: 0.9310, loss:0.9952\n",
      "Epoch: 379, test AUC: 0.8667, test AP: 0.8645, train AUC: 0.9395, train AP: 0.9311, loss:0.9937\n",
      "Epoch: 380, test AUC: 0.8668, test AP: 0.8646, train AUC: 0.9396, train AP: 0.9312, loss:0.9943\n",
      "Epoch: 381, test AUC: 0.8669, test AP: 0.8647, train AUC: 0.9396, train AP: 0.9312, loss:0.9932\n",
      "Epoch: 382, test AUC: 0.8670, test AP: 0.8648, train AUC: 0.9397, train AP: 0.9313, loss:0.9920\n",
      "Epoch: 383, test AUC: 0.8670, test AP: 0.8649, train AUC: 0.9398, train AP: 0.9314, loss:0.9973\n",
      "Epoch: 384, test AUC: 0.8671, test AP: 0.8650, train AUC: 0.9399, train AP: 0.9315, loss:0.9936\n",
      "Epoch: 385, test AUC: 0.8672, test AP: 0.8651, train AUC: 0.9399, train AP: 0.9316, loss:0.9974\n",
      "Epoch: 386, test AUC: 0.8672, test AP: 0.8652, train AUC: 0.9400, train AP: 0.9316, loss:0.9898\n",
      "Epoch: 387, test AUC: 0.8673, test AP: 0.8653, train AUC: 0.9400, train AP: 0.9317, loss:0.9955\n",
      "Epoch: 388, test AUC: 0.8674, test AP: 0.8654, train AUC: 0.9401, train AP: 0.9318, loss:0.9826\n",
      "Epoch: 389, test AUC: 0.8675, test AP: 0.8655, train AUC: 0.9401, train AP: 0.9318, loss:0.9928\n",
      "Epoch: 390, test AUC: 0.8675, test AP: 0.8656, train AUC: 0.9402, train AP: 0.9319, loss:0.9866\n",
      "Epoch: 391, test AUC: 0.8676, test AP: 0.8657, train AUC: 0.9403, train AP: 0.9320, loss:0.9953\n",
      "Epoch: 392, test AUC: 0.8677, test AP: 0.8658, train AUC: 0.9403, train AP: 0.9320, loss:0.9910\n",
      "Epoch: 393, test AUC: 0.8678, test AP: 0.8659, train AUC: 0.9404, train AP: 0.9321, loss:0.9850\n",
      "Epoch: 394, test AUC: 0.8678, test AP: 0.8659, train AUC: 0.9405, train AP: 0.9322, loss:0.9888\n",
      "Epoch: 395, test AUC: 0.8679, test AP: 0.8660, train AUC: 0.9405, train AP: 0.9323, loss:0.9890\n",
      "Epoch: 396, test AUC: 0.8680, test AP: 0.8661, train AUC: 0.9406, train AP: 0.9323, loss:0.9831\n",
      "Epoch: 397, test AUC: 0.8680, test AP: 0.8661, train AUC: 0.9407, train AP: 0.9324, loss:0.9906\n",
      "Epoch: 398, test AUC: 0.8681, test AP: 0.8662, train AUC: 0.9407, train AP: 0.9324, loss:0.9848\n",
      "Epoch: 399, test AUC: 0.8681, test AP: 0.8662, train AUC: 0.9407, train AP: 0.9325, loss:0.9858\n",
      "Epoch: 400, test AUC: 0.8681, test AP: 0.8662, train AUC: 0.9408, train AP: 0.9325, loss:0.9874\n",
      "Epoch: 401, test AUC: 0.8681, test AP: 0.8663, train AUC: 0.9408, train AP: 0.9326, loss:0.9892\n",
      "Epoch: 402, test AUC: 0.8682, test AP: 0.8663, train AUC: 0.9408, train AP: 0.9326, loss:0.9861\n",
      "Epoch: 403, test AUC: 0.8682, test AP: 0.8664, train AUC: 0.9409, train AP: 0.9327, loss:0.9911\n",
      "Epoch: 404, test AUC: 0.8683, test AP: 0.8665, train AUC: 0.9410, train AP: 0.9328, loss:0.9866\n",
      "Epoch: 405, test AUC: 0.8684, test AP: 0.8667, train AUC: 0.9411, train AP: 0.9329, loss:0.9913\n",
      "Epoch: 406, test AUC: 0.8685, test AP: 0.8668, train AUC: 0.9411, train AP: 0.9329, loss:0.9921\n",
      "Epoch: 407, test AUC: 0.8686, test AP: 0.8669, train AUC: 0.9412, train AP: 0.9330, loss:0.9910\n",
      "Epoch: 408, test AUC: 0.8687, test AP: 0.8670, train AUC: 0.9413, train AP: 0.9331, loss:0.9845\n",
      "Epoch: 409, test AUC: 0.8687, test AP: 0.8670, train AUC: 0.9413, train AP: 0.9331, loss:0.9901\n",
      "Epoch: 410, test AUC: 0.8688, test AP: 0.8671, train AUC: 0.9414, train AP: 0.9332, loss:0.9863\n",
      "Epoch: 411, test AUC: 0.8689, test AP: 0.8672, train AUC: 0.9415, train AP: 0.9333, loss:0.9938\n",
      "Epoch: 412, test AUC: 0.8690, test AP: 0.8673, train AUC: 0.9415, train AP: 0.9334, loss:0.9976\n",
      "Epoch: 413, test AUC: 0.8690, test AP: 0.8674, train AUC: 0.9416, train AP: 0.9334, loss:0.9891\n",
      "Epoch: 414, test AUC: 0.8691, test AP: 0.8675, train AUC: 0.9417, train AP: 0.9335, loss:0.9884\n",
      "Epoch: 415, test AUC: 0.8692, test AP: 0.8676, train AUC: 0.9418, train AP: 0.9336, loss:0.9860\n",
      "Epoch: 416, test AUC: 0.8693, test AP: 0.8677, train AUC: 0.9419, train AP: 0.9337, loss:0.9899\n",
      "Epoch: 417, test AUC: 0.8693, test AP: 0.8678, train AUC: 0.9419, train AP: 0.9338, loss:0.9847\n",
      "Epoch: 418, test AUC: 0.8694, test AP: 0.8679, train AUC: 0.9420, train AP: 0.9339, loss:0.9804\n",
      "Epoch: 419, test AUC: 0.8695, test AP: 0.8680, train AUC: 0.9421, train AP: 0.9340, loss:0.9957\n",
      "Epoch: 420, test AUC: 0.8695, test AP: 0.8680, train AUC: 0.9422, train AP: 0.9341, loss:0.9877\n",
      "Epoch: 421, test AUC: 0.8696, test AP: 0.8681, train AUC: 0.9423, train AP: 0.9342, loss:0.9923\n",
      "Epoch: 422, test AUC: 0.8696, test AP: 0.8681, train AUC: 0.9423, train AP: 0.9342, loss:0.9800\n",
      "Epoch: 423, test AUC: 0.8696, test AP: 0.8682, train AUC: 0.9423, train AP: 0.9343, loss:0.9914\n",
      "Epoch: 424, test AUC: 0.8697, test AP: 0.8682, train AUC: 0.9424, train AP: 0.9343, loss:0.9869\n",
      "Epoch: 425, test AUC: 0.8697, test AP: 0.8682, train AUC: 0.9424, train AP: 0.9343, loss:0.9881\n",
      "Epoch: 426, test AUC: 0.8696, test AP: 0.8681, train AUC: 0.9424, train AP: 0.9343, loss:0.9892\n",
      "Epoch: 427, test AUC: 0.8696, test AP: 0.8681, train AUC: 0.9424, train AP: 0.9343, loss:0.9894\n",
      "Epoch: 428, test AUC: 0.8696, test AP: 0.8681, train AUC: 0.9424, train AP: 0.9343, loss:0.9750\n",
      "Epoch: 429, test AUC: 0.8696, test AP: 0.8681, train AUC: 0.9424, train AP: 0.9343, loss:0.9885\n",
      "Epoch: 430, test AUC: 0.8697, test AP: 0.8682, train AUC: 0.9425, train AP: 0.9344, loss:0.9900\n",
      "Epoch: 431, test AUC: 0.8697, test AP: 0.8683, train AUC: 0.9425, train AP: 0.9344, loss:0.9839\n",
      "Epoch: 432, test AUC: 0.8698, test AP: 0.8684, train AUC: 0.9426, train AP: 0.9345, loss:0.9902\n",
      "Epoch: 433, test AUC: 0.8699, test AP: 0.8685, train AUC: 0.9427, train AP: 0.9346, loss:0.9873\n",
      "Epoch: 434, test AUC: 0.8699, test AP: 0.8686, train AUC: 0.9427, train AP: 0.9347, loss:0.9889\n",
      "Epoch: 435, test AUC: 0.8700, test AP: 0.8687, train AUC: 0.9428, train AP: 0.9348, loss:0.9827\n",
      "Epoch: 436, test AUC: 0.8701, test AP: 0.8688, train AUC: 0.9429, train AP: 0.9348, loss:0.9877\n",
      "Epoch: 437, test AUC: 0.8702, test AP: 0.8689, train AUC: 0.9430, train AP: 0.9350, loss:0.9840\n",
      "Epoch: 438, test AUC: 0.8703, test AP: 0.8690, train AUC: 0.9431, train AP: 0.9351, loss:0.9907\n",
      "Epoch: 439, test AUC: 0.8704, test AP: 0.8691, train AUC: 0.9432, train AP: 0.9352, loss:0.9831\n",
      "Epoch: 440, test AUC: 0.8705, test AP: 0.8693, train AUC: 0.9433, train AP: 0.9353, loss:0.9869\n",
      "Epoch: 441, test AUC: 0.8706, test AP: 0.8694, train AUC: 0.9434, train AP: 0.9354, loss:0.9891\n",
      "Epoch: 442, test AUC: 0.8707, test AP: 0.8695, train AUC: 0.9435, train AP: 0.9355, loss:0.9835\n",
      "Epoch: 443, test AUC: 0.8708, test AP: 0.8696, train AUC: 0.9436, train AP: 0.9356, loss:0.9847\n",
      "Epoch: 444, test AUC: 0.8708, test AP: 0.8696, train AUC: 0.9436, train AP: 0.9357, loss:0.9845\n",
      "Epoch: 445, test AUC: 0.8709, test AP: 0.8697, train AUC: 0.9437, train AP: 0.9357, loss:0.9829\n",
      "Epoch: 446, test AUC: 0.8709, test AP: 0.8697, train AUC: 0.9437, train AP: 0.9358, loss:0.9905\n",
      "Epoch: 447, test AUC: 0.8709, test AP: 0.8697, train AUC: 0.9437, train AP: 0.9358, loss:0.9806\n",
      "Epoch: 448, test AUC: 0.8709, test AP: 0.8697, train AUC: 0.9438, train AP: 0.9358, loss:0.9825\n",
      "Epoch: 449, test AUC: 0.8710, test AP: 0.8697, train AUC: 0.9438, train AP: 0.9359, loss:0.9830\n",
      "Epoch: 450, test AUC: 0.8710, test AP: 0.8698, train AUC: 0.9438, train AP: 0.9359, loss:0.9793\n",
      "Epoch: 451, test AUC: 0.8710, test AP: 0.8698, train AUC: 0.9438, train AP: 0.9359, loss:0.9884\n",
      "Epoch: 452, test AUC: 0.8710, test AP: 0.8698, train AUC: 0.9439, train AP: 0.9360, loss:0.9851\n",
      "Epoch: 453, test AUC: 0.8711, test AP: 0.8699, train AUC: 0.9439, train AP: 0.9360, loss:0.9887\n",
      "Epoch: 454, test AUC: 0.8711, test AP: 0.8699, train AUC: 0.9440, train AP: 0.9361, loss:0.9826\n",
      "Epoch: 455, test AUC: 0.8712, test AP: 0.8700, train AUC: 0.9440, train AP: 0.9361, loss:0.9858\n",
      "Epoch: 456, test AUC: 0.8713, test AP: 0.8701, train AUC: 0.9441, train AP: 0.9362, loss:0.9879\n",
      "Epoch: 457, test AUC: 0.8713, test AP: 0.8702, train AUC: 0.9441, train AP: 0.9363, loss:0.9874\n",
      "Epoch: 458, test AUC: 0.8713, test AP: 0.8703, train AUC: 0.9442, train AP: 0.9363, loss:0.9869\n",
      "Epoch: 459, test AUC: 0.8714, test AP: 0.8703, train AUC: 0.9442, train AP: 0.9364, loss:0.9897\n",
      "Epoch: 460, test AUC: 0.8714, test AP: 0.8703, train AUC: 0.9442, train AP: 0.9364, loss:0.9831\n",
      "Epoch: 461, test AUC: 0.8715, test AP: 0.8704, train AUC: 0.9443, train AP: 0.9364, loss:0.9879\n",
      "Epoch: 462, test AUC: 0.8715, test AP: 0.8704, train AUC: 0.9443, train AP: 0.9365, loss:0.9774\n",
      "Epoch: 463, test AUC: 0.8715, test AP: 0.8705, train AUC: 0.9444, train AP: 0.9365, loss:0.9840\n",
      "Epoch: 464, test AUC: 0.8716, test AP: 0.8705, train AUC: 0.9444, train AP: 0.9365, loss:0.9829\n",
      "Epoch: 465, test AUC: 0.8717, test AP: 0.8706, train AUC: 0.9445, train AP: 0.9366, loss:0.9880\n",
      "Epoch: 466, test AUC: 0.8717, test AP: 0.8706, train AUC: 0.9445, train AP: 0.9366, loss:0.9859\n",
      "Epoch: 467, test AUC: 0.8718, test AP: 0.8707, train AUC: 0.9446, train AP: 0.9367, loss:0.9836\n",
      "Epoch: 468, test AUC: 0.8719, test AP: 0.8708, train AUC: 0.9447, train AP: 0.9367, loss:0.9827\n",
      "Epoch: 469, test AUC: 0.8720, test AP: 0.8709, train AUC: 0.9447, train AP: 0.9368, loss:0.9859\n",
      "Epoch: 470, test AUC: 0.8721, test AP: 0.8710, train AUC: 0.9448, train AP: 0.9369, loss:0.9857\n",
      "Epoch: 471, test AUC: 0.8722, test AP: 0.8711, train AUC: 0.9449, train AP: 0.9370, loss:0.9838\n",
      "Epoch: 472, test AUC: 0.8722, test AP: 0.8712, train AUC: 0.9450, train AP: 0.9371, loss:0.9801\n",
      "Epoch: 473, test AUC: 0.8723, test AP: 0.8712, train AUC: 0.9450, train AP: 0.9372, loss:0.9824\n",
      "Epoch: 474, test AUC: 0.8723, test AP: 0.8713, train AUC: 0.9451, train AP: 0.9373, loss:0.9856\n",
      "Epoch: 475, test AUC: 0.8724, test AP: 0.8713, train AUC: 0.9452, train AP: 0.9374, loss:0.9870\n",
      "Epoch: 476, test AUC: 0.8724, test AP: 0.8714, train AUC: 0.9452, train AP: 0.9374, loss:0.9841\n",
      "Epoch: 477, test AUC: 0.8724, test AP: 0.8714, train AUC: 0.9452, train AP: 0.9375, loss:0.9840\n",
      "Epoch: 478, test AUC: 0.8724, test AP: 0.8714, train AUC: 0.9453, train AP: 0.9375, loss:0.9836\n",
      "Epoch: 479, test AUC: 0.8724, test AP: 0.8714, train AUC: 0.9453, train AP: 0.9376, loss:0.9844\n",
      "Epoch: 480, test AUC: 0.8724, test AP: 0.8714, train AUC: 0.9453, train AP: 0.9376, loss:0.9856\n",
      "Epoch: 481, test AUC: 0.8724, test AP: 0.8715, train AUC: 0.9453, train AP: 0.9376, loss:0.9771\n",
      "Epoch: 482, test AUC: 0.8725, test AP: 0.8715, train AUC: 0.9453, train AP: 0.9376, loss:0.9827\n",
      "Epoch: 483, test AUC: 0.8725, test AP: 0.8715, train AUC: 0.9454, train AP: 0.9376, loss:0.9784\n",
      "Epoch: 484, test AUC: 0.8725, test AP: 0.8716, train AUC: 0.9454, train AP: 0.9376, loss:0.9811\n",
      "Epoch: 485, test AUC: 0.8726, test AP: 0.8716, train AUC: 0.9454, train AP: 0.9377, loss:0.9813\n",
      "Epoch: 486, test AUC: 0.8726, test AP: 0.8716, train AUC: 0.9455, train AP: 0.9377, loss:0.9885\n",
      "Epoch: 487, test AUC: 0.8726, test AP: 0.8717, train AUC: 0.9455, train AP: 0.9378, loss:0.9825\n",
      "Epoch: 488, test AUC: 0.8727, test AP: 0.8717, train AUC: 0.9456, train AP: 0.9378, loss:0.9826\n",
      "Epoch: 489, test AUC: 0.8727, test AP: 0.8717, train AUC: 0.9456, train AP: 0.9379, loss:0.9852\n",
      "Epoch: 490, test AUC: 0.8728, test AP: 0.8718, train AUC: 0.9457, train AP: 0.9379, loss:0.9782\n",
      "Epoch: 491, test AUC: 0.8729, test AP: 0.8719, train AUC: 0.9458, train AP: 0.9380, loss:0.9850\n",
      "Epoch: 492, test AUC: 0.8730, test AP: 0.8720, train AUC: 0.9459, train AP: 0.9381, loss:0.9859\n",
      "Epoch: 493, test AUC: 0.8731, test AP: 0.8722, train AUC: 0.9460, train AP: 0.9382, loss:0.9817\n",
      "Epoch: 494, test AUC: 0.8732, test AP: 0.8723, train AUC: 0.9460, train AP: 0.9383, loss:0.9877\n",
      "Epoch: 495, test AUC: 0.8733, test AP: 0.8724, train AUC: 0.9461, train AP: 0.9384, loss:0.9836\n",
      "Epoch: 496, test AUC: 0.8734, test AP: 0.8725, train AUC: 0.9462, train AP: 0.9385, loss:0.9777\n",
      "Epoch: 497, test AUC: 0.8734, test AP: 0.8725, train AUC: 0.9463, train AP: 0.9386, loss:0.9851\n",
      "Epoch: 498, test AUC: 0.8735, test AP: 0.8726, train AUC: 0.9463, train AP: 0.9387, loss:0.9837\n",
      "Epoch: 499, test AUC: 0.8735, test AP: 0.8727, train AUC: 0.9464, train AP: 0.9388, loss:0.9878\n",
      "Epoch: 500, test AUC: 0.8736, test AP: 0.8727, train AUC: 0.9465, train AP: 0.9388, loss:0.9871\n",
      "Epoch: 501, test AUC: 0.8736, test AP: 0.8728, train AUC: 0.9465, train AP: 0.9389, loss:0.9843\n",
      "Epoch: 502, test AUC: 0.8736, test AP: 0.8728, train AUC: 0.9465, train AP: 0.9389, loss:0.9768\n",
      "Epoch: 503, test AUC: 0.8737, test AP: 0.8729, train AUC: 0.9466, train AP: 0.9389, loss:0.9859\n",
      "Epoch: 504, test AUC: 0.8737, test AP: 0.8730, train AUC: 0.9466, train AP: 0.9390, loss:0.9902\n",
      "Epoch: 505, test AUC: 0.8737, test AP: 0.8730, train AUC: 0.9466, train AP: 0.9390, loss:0.9828\n",
      "Epoch: 506, test AUC: 0.8738, test AP: 0.8730, train AUC: 0.9466, train AP: 0.9390, loss:0.9830\n",
      "Epoch: 507, test AUC: 0.8738, test AP: 0.8730, train AUC: 0.9466, train AP: 0.9390, loss:0.9867\n",
      "Epoch: 508, test AUC: 0.8738, test AP: 0.8730, train AUC: 0.9466, train AP: 0.9390, loss:0.9753\n",
      "Epoch: 509, test AUC: 0.8739, test AP: 0.8730, train AUC: 0.9467, train AP: 0.9390, loss:0.9793\n",
      "Epoch: 510, test AUC: 0.8739, test AP: 0.8731, train AUC: 0.9467, train AP: 0.9391, loss:0.9792\n",
      "Epoch: 511, test AUC: 0.8740, test AP: 0.8731, train AUC: 0.9468, train AP: 0.9391, loss:0.9842\n",
      "Epoch: 512, test AUC: 0.8740, test AP: 0.8732, train AUC: 0.9468, train AP: 0.9391, loss:0.9800\n",
      "Epoch: 513, test AUC: 0.8741, test AP: 0.8732, train AUC: 0.9469, train AP: 0.9392, loss:0.9786\n",
      "Epoch: 514, test AUC: 0.8741, test AP: 0.8733, train AUC: 0.9469, train AP: 0.9392, loss:0.9792\n",
      "Epoch: 515, test AUC: 0.8741, test AP: 0.8733, train AUC: 0.9470, train AP: 0.9393, loss:0.9841\n",
      "Epoch: 516, test AUC: 0.8741, test AP: 0.8733, train AUC: 0.9470, train AP: 0.9393, loss:0.9784\n",
      "Epoch: 517, test AUC: 0.8742, test AP: 0.8733, train AUC: 0.9470, train AP: 0.9394, loss:0.9798\n",
      "Epoch: 518, test AUC: 0.8742, test AP: 0.8734, train AUC: 0.9471, train AP: 0.9395, loss:0.9807\n",
      "Epoch: 519, test AUC: 0.8742, test AP: 0.8734, train AUC: 0.9471, train AP: 0.9395, loss:0.9861\n",
      "Epoch: 520, test AUC: 0.8742, test AP: 0.8734, train AUC: 0.9472, train AP: 0.9396, loss:0.9868\n",
      "Epoch: 521, test AUC: 0.8743, test AP: 0.8735, train AUC: 0.9472, train AP: 0.9396, loss:0.9681\n",
      "Epoch: 522, test AUC: 0.8743, test AP: 0.8736, train AUC: 0.9473, train AP: 0.9397, loss:0.9859\n",
      "Epoch: 523, test AUC: 0.8743, test AP: 0.8736, train AUC: 0.9473, train AP: 0.9397, loss:0.9752\n",
      "Epoch: 524, test AUC: 0.8745, test AP: 0.8738, train AUC: 0.9474, train AP: 0.9398, loss:0.9848\n",
      "Epoch: 525, test AUC: 0.8746, test AP: 0.8739, train AUC: 0.9474, train AP: 0.9399, loss:0.9758\n",
      "Epoch: 526, test AUC: 0.8747, test AP: 0.8740, train AUC: 0.9475, train AP: 0.9399, loss:0.9790\n",
      "Epoch: 527, test AUC: 0.8748, test AP: 0.8741, train AUC: 0.9475, train AP: 0.9400, loss:0.9828\n",
      "Epoch: 528, test AUC: 0.8749, test AP: 0.8743, train AUC: 0.9476, train AP: 0.9400, loss:0.9747\n",
      "Epoch: 529, test AUC: 0.8750, test AP: 0.8744, train AUC: 0.9477, train AP: 0.9401, loss:0.9783\n",
      "Epoch: 530, test AUC: 0.8750, test AP: 0.8744, train AUC: 0.9477, train AP: 0.9402, loss:0.9805\n",
      "Epoch: 531, test AUC: 0.8751, test AP: 0.8745, train AUC: 0.9478, train AP: 0.9402, loss:0.9801\n",
      "Epoch: 532, test AUC: 0.8751, test AP: 0.8745, train AUC: 0.9478, train AP: 0.9403, loss:0.9820\n",
      "Epoch: 533, test AUC: 0.8751, test AP: 0.8745, train AUC: 0.9479, train AP: 0.9403, loss:0.9754\n",
      "Epoch: 534, test AUC: 0.8751, test AP: 0.8745, train AUC: 0.9479, train AP: 0.9404, loss:0.9843\n",
      "Epoch: 535, test AUC: 0.8751, test AP: 0.8745, train AUC: 0.9479, train AP: 0.9404, loss:0.9787\n",
      "Epoch: 536, test AUC: 0.8751, test AP: 0.8744, train AUC: 0.9479, train AP: 0.9404, loss:0.9842\n",
      "Epoch: 537, test AUC: 0.8751, test AP: 0.8744, train AUC: 0.9479, train AP: 0.9404, loss:0.9813\n",
      "Epoch: 538, test AUC: 0.8751, test AP: 0.8744, train AUC: 0.9479, train AP: 0.9404, loss:0.9714\n",
      "Epoch: 539, test AUC: 0.8752, test AP: 0.8744, train AUC: 0.9479, train AP: 0.9404, loss:0.9804\n",
      "Epoch: 540, test AUC: 0.8752, test AP: 0.8745, train AUC: 0.9480, train AP: 0.9404, loss:0.9808\n",
      "Epoch: 541, test AUC: 0.8753, test AP: 0.8746, train AUC: 0.9480, train AP: 0.9405, loss:0.9766\n",
      "Epoch: 542, test AUC: 0.8754, test AP: 0.8747, train AUC: 0.9481, train AP: 0.9406, loss:0.9762\n",
      "Epoch: 543, test AUC: 0.8754, test AP: 0.8748, train AUC: 0.9482, train AP: 0.9407, loss:0.9779\n",
      "Epoch: 544, test AUC: 0.8755, test AP: 0.8749, train AUC: 0.9482, train AP: 0.9407, loss:0.9767\n",
      "Epoch: 545, test AUC: 0.8756, test AP: 0.8749, train AUC: 0.9483, train AP: 0.9408, loss:0.9777\n",
      "Epoch: 546, test AUC: 0.8756, test AP: 0.8750, train AUC: 0.9483, train AP: 0.9408, loss:0.9795\n",
      "Epoch: 547, test AUC: 0.8757, test AP: 0.8751, train AUC: 0.9484, train AP: 0.9409, loss:0.9751\n",
      "Epoch: 548, test AUC: 0.8758, test AP: 0.8752, train AUC: 0.9484, train AP: 0.9409, loss:0.9767\n",
      "Epoch: 549, test AUC: 0.8758, test AP: 0.8753, train AUC: 0.9485, train AP: 0.9410, loss:0.9741\n",
      "Epoch: 550, test AUC: 0.8759, test AP: 0.8754, train AUC: 0.9485, train AP: 0.9410, loss:0.9782\n",
      "Epoch: 551, test AUC: 0.8759, test AP: 0.8754, train AUC: 0.9486, train AP: 0.9411, loss:0.9772\n",
      "Epoch: 552, test AUC: 0.8760, test AP: 0.8755, train AUC: 0.9486, train AP: 0.9412, loss:0.9811\n",
      "Epoch: 553, test AUC: 0.8760, test AP: 0.8755, train AUC: 0.9487, train AP: 0.9413, loss:0.9780\n",
      "Epoch: 554, test AUC: 0.8761, test AP: 0.8756, train AUC: 0.9488, train AP: 0.9414, loss:0.9744\n",
      "Epoch: 555, test AUC: 0.8761, test AP: 0.8757, train AUC: 0.9489, train AP: 0.9415, loss:0.9801\n",
      "Epoch: 556, test AUC: 0.8762, test AP: 0.8757, train AUC: 0.9490, train AP: 0.9416, loss:0.9700\n",
      "Epoch: 557, test AUC: 0.8762, test AP: 0.8758, train AUC: 0.9490, train AP: 0.9417, loss:0.9795\n",
      "Epoch: 558, test AUC: 0.8763, test AP: 0.8758, train AUC: 0.9491, train AP: 0.9418, loss:0.9811\n",
      "Epoch: 559, test AUC: 0.8763, test AP: 0.8760, train AUC: 0.9492, train AP: 0.9418, loss:0.9840\n",
      "Epoch: 560, test AUC: 0.8764, test AP: 0.8760, train AUC: 0.9493, train AP: 0.9419, loss:0.9798\n",
      "Epoch: 561, test AUC: 0.8765, test AP: 0.8761, train AUC: 0.9493, train AP: 0.9420, loss:0.9756\n",
      "Epoch: 562, test AUC: 0.8766, test AP: 0.8763, train AUC: 0.9494, train AP: 0.9420, loss:0.9817\n",
      "Epoch: 563, test AUC: 0.8768, test AP: 0.8764, train AUC: 0.9494, train AP: 0.9421, loss:0.9787\n",
      "Epoch: 564, test AUC: 0.8769, test AP: 0.8765, train AUC: 0.9495, train AP: 0.9422, loss:0.9759\n",
      "Epoch: 565, test AUC: 0.8770, test AP: 0.8767, train AUC: 0.9496, train AP: 0.9423, loss:0.9753\n",
      "Epoch: 566, test AUC: 0.8771, test AP: 0.8768, train AUC: 0.9496, train AP: 0.9423, loss:0.9783\n",
      "Epoch: 567, test AUC: 0.8772, test AP: 0.8769, train AUC: 0.9497, train AP: 0.9423, loss:0.9735\n",
      "Epoch: 568, test AUC: 0.8773, test AP: 0.8770, train AUC: 0.9497, train AP: 0.9423, loss:0.9704\n",
      "Epoch: 569, test AUC: 0.8774, test AP: 0.8770, train AUC: 0.9497, train AP: 0.9424, loss:0.9800\n",
      "Epoch: 570, test AUC: 0.8774, test AP: 0.8771, train AUC: 0.9497, train AP: 0.9424, loss:0.9730\n",
      "Epoch: 571, test AUC: 0.8775, test AP: 0.8771, train AUC: 0.9498, train AP: 0.9424, loss:0.9756\n",
      "Epoch: 572, test AUC: 0.8775, test AP: 0.8772, train AUC: 0.9498, train AP: 0.9425, loss:0.9706\n",
      "Epoch: 573, test AUC: 0.8775, test AP: 0.8772, train AUC: 0.9498, train AP: 0.9425, loss:0.9756\n",
      "Epoch: 574, test AUC: 0.8775, test AP: 0.8772, train AUC: 0.9499, train AP: 0.9425, loss:0.9712\n",
      "Epoch: 575, test AUC: 0.8775, test AP: 0.8772, train AUC: 0.9499, train AP: 0.9425, loss:0.9737\n",
      "Epoch: 576, test AUC: 0.8774, test AP: 0.8772, train AUC: 0.9499, train AP: 0.9425, loss:0.9846\n",
      "Epoch: 577, test AUC: 0.8774, test AP: 0.8771, train AUC: 0.9499, train AP: 0.9425, loss:0.9818\n",
      "Epoch: 578, test AUC: 0.8774, test AP: 0.8771, train AUC: 0.9499, train AP: 0.9425, loss:0.9686\n",
      "Epoch: 579, test AUC: 0.8774, test AP: 0.8771, train AUC: 0.9499, train AP: 0.9426, loss:0.9671\n",
      "Epoch: 580, test AUC: 0.8774, test AP: 0.8771, train AUC: 0.9499, train AP: 0.9426, loss:0.9710\n",
      "Epoch: 581, test AUC: 0.8775, test AP: 0.8772, train AUC: 0.9500, train AP: 0.9427, loss:0.9783\n",
      "Epoch: 582, test AUC: 0.8776, test AP: 0.8773, train AUC: 0.9501, train AP: 0.9428, loss:0.9808\n",
      "Epoch: 583, test AUC: 0.8777, test AP: 0.8774, train AUC: 0.9502, train AP: 0.9429, loss:0.9755\n",
      "Epoch: 584, test AUC: 0.8778, test AP: 0.8776, train AUC: 0.9503, train AP: 0.9430, loss:0.9770\n",
      "Epoch: 585, test AUC: 0.8779, test AP: 0.8777, train AUC: 0.9504, train AP: 0.9431, loss:0.9755\n",
      "Epoch: 586, test AUC: 0.8781, test AP: 0.8779, train AUC: 0.9505, train AP: 0.9432, loss:0.9738\n",
      "Epoch: 587, test AUC: 0.8782, test AP: 0.8780, train AUC: 0.9506, train AP: 0.9433, loss:0.9732\n",
      "Epoch: 588, test AUC: 0.8783, test AP: 0.8782, train AUC: 0.9506, train AP: 0.9434, loss:0.9755\n",
      "Epoch: 589, test AUC: 0.8785, test AP: 0.8783, train AUC: 0.9507, train AP: 0.9435, loss:0.9723\n",
      "Epoch: 590, test AUC: 0.8786, test AP: 0.8784, train AUC: 0.9508, train AP: 0.9436, loss:0.9752\n",
      "Epoch: 591, test AUC: 0.8787, test AP: 0.8786, train AUC: 0.9509, train AP: 0.9436, loss:0.9783\n",
      "Epoch: 592, test AUC: 0.8788, test AP: 0.8786, train AUC: 0.9509, train AP: 0.9437, loss:0.9697\n",
      "Epoch: 593, test AUC: 0.8788, test AP: 0.8787, train AUC: 0.9510, train AP: 0.9438, loss:0.9716\n",
      "Epoch: 594, test AUC: 0.8789, test AP: 0.8787, train AUC: 0.9510, train AP: 0.9438, loss:0.9747\n",
      "Epoch: 595, test AUC: 0.8789, test AP: 0.8788, train AUC: 0.9510, train AP: 0.9438, loss:0.9669\n",
      "Epoch: 596, test AUC: 0.8789, test AP: 0.8787, train AUC: 0.9510, train AP: 0.9438, loss:0.9696\n",
      "Epoch: 597, test AUC: 0.8789, test AP: 0.8787, train AUC: 0.9510, train AP: 0.9438, loss:0.9679\n",
      "Epoch: 598, test AUC: 0.8788, test AP: 0.8787, train AUC: 0.9510, train AP: 0.9438, loss:0.9811\n",
      "Epoch: 599, test AUC: 0.8788, test AP: 0.8786, train AUC: 0.9510, train AP: 0.9437, loss:0.9776\n",
      "Epoch: 600, test AUC: 0.8787, test AP: 0.8785, train AUC: 0.9509, train AP: 0.9437, loss:0.9762\n",
      "Epoch: 601, test AUC: 0.8787, test AP: 0.8784, train AUC: 0.9509, train AP: 0.9436, loss:0.9708\n",
      "Epoch: 602, test AUC: 0.8786, test AP: 0.8784, train AUC: 0.9509, train AP: 0.9436, loss:0.9726\n",
      "Epoch: 603, test AUC: 0.8786, test AP: 0.8784, train AUC: 0.9509, train AP: 0.9436, loss:0.9738\n",
      "Epoch: 604, test AUC: 0.8787, test AP: 0.8785, train AUC: 0.9509, train AP: 0.9437, loss:0.9798\n",
      "Epoch: 605, test AUC: 0.8788, test AP: 0.8787, train AUC: 0.9510, train AP: 0.9438, loss:0.9737\n",
      "Epoch: 606, test AUC: 0.8789, test AP: 0.8788, train AUC: 0.9511, train AP: 0.9439, loss:0.9802\n",
      "Epoch: 607, test AUC: 0.8791, test AP: 0.8790, train AUC: 0.9512, train AP: 0.9440, loss:0.9696\n",
      "Epoch: 608, test AUC: 0.8792, test AP: 0.8791, train AUC: 0.9513, train AP: 0.9441, loss:0.9719\n",
      "Epoch: 609, test AUC: 0.8792, test AP: 0.8792, train AUC: 0.9514, train AP: 0.9442, loss:0.9679\n",
      "Epoch: 610, test AUC: 0.8793, test AP: 0.8793, train AUC: 0.9515, train AP: 0.9443, loss:0.9799\n",
      "Epoch: 611, test AUC: 0.8794, test AP: 0.8794, train AUC: 0.9516, train AP: 0.9444, loss:0.9726\n",
      "Epoch: 612, test AUC: 0.8795, test AP: 0.8795, train AUC: 0.9516, train AP: 0.9444, loss:0.9768\n",
      "Epoch: 613, test AUC: 0.8795, test AP: 0.8795, train AUC: 0.9517, train AP: 0.9445, loss:0.9786\n",
      "Epoch: 614, test AUC: 0.8795, test AP: 0.8796, train AUC: 0.9517, train AP: 0.9446, loss:0.9751\n",
      "Epoch: 615, test AUC: 0.8795, test AP: 0.8796, train AUC: 0.9517, train AP: 0.9446, loss:0.9788\n",
      "Epoch: 616, test AUC: 0.8795, test AP: 0.8796, train AUC: 0.9517, train AP: 0.9446, loss:0.9746\n",
      "Epoch: 617, test AUC: 0.8795, test AP: 0.8796, train AUC: 0.9517, train AP: 0.9446, loss:0.9782\n",
      "Epoch: 618, test AUC: 0.8795, test AP: 0.8796, train AUC: 0.9517, train AP: 0.9446, loss:0.9799\n",
      "Epoch: 619, test AUC: 0.8796, test AP: 0.8796, train AUC: 0.9518, train AP: 0.9446, loss:0.9712\n",
      "Epoch: 620, test AUC: 0.8797, test AP: 0.8797, train AUC: 0.9518, train AP: 0.9447, loss:0.9670\n",
      "Epoch: 621, test AUC: 0.8798, test AP: 0.8798, train AUC: 0.9519, train AP: 0.9448, loss:0.9697\n",
      "Epoch: 622, test AUC: 0.8798, test AP: 0.8799, train AUC: 0.9519, train AP: 0.9448, loss:0.9751\n",
      "Epoch: 623, test AUC: 0.8799, test AP: 0.8799, train AUC: 0.9520, train AP: 0.9449, loss:0.9782\n",
      "Epoch: 624, test AUC: 0.8800, test AP: 0.8800, train AUC: 0.9521, train AP: 0.9450, loss:0.9730\n",
      "Epoch: 625, test AUC: 0.8801, test AP: 0.8801, train AUC: 0.9522, train AP: 0.9451, loss:0.9693\n",
      "Epoch: 626, test AUC: 0.8802, test AP: 0.8803, train AUC: 0.9523, train AP: 0.9452, loss:0.9784\n",
      "Epoch: 627, test AUC: 0.8803, test AP: 0.8803, train AUC: 0.9523, train AP: 0.9453, loss:0.9747\n",
      "Epoch: 628, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9524, train AP: 0.9453, loss:0.9750\n",
      "Epoch: 629, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9524, train AP: 0.9453, loss:0.9686\n",
      "Epoch: 630, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9524, train AP: 0.9453, loss:0.9792\n",
      "Epoch: 631, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9524, train AP: 0.9453, loss:0.9666\n",
      "Epoch: 632, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9524, train AP: 0.9453, loss:0.9719\n",
      "Epoch: 633, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9524, train AP: 0.9453, loss:0.9756\n",
      "Epoch: 634, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9524, train AP: 0.9454, loss:0.9748\n",
      "Epoch: 635, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9524, train AP: 0.9454, loss:0.9734\n",
      "Epoch: 636, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9524, train AP: 0.9454, loss:0.9746\n",
      "Epoch: 637, test AUC: 0.8803, test AP: 0.8804, train AUC: 0.9525, train AP: 0.9454, loss:0.9709\n",
      "Epoch: 638, test AUC: 0.8804, test AP: 0.8805, train AUC: 0.9525, train AP: 0.9455, loss:0.9738\n",
      "Epoch: 639, test AUC: 0.8805, test AP: 0.8806, train AUC: 0.9526, train AP: 0.9456, loss:0.9653\n",
      "Epoch: 640, test AUC: 0.8805, test AP: 0.8807, train AUC: 0.9526, train AP: 0.9456, loss:0.9805\n",
      "Epoch: 641, test AUC: 0.8806, test AP: 0.8808, train AUC: 0.9526, train AP: 0.9456, loss:0.9793\n",
      "Epoch: 642, test AUC: 0.8807, test AP: 0.8808, train AUC: 0.9527, train AP: 0.9457, loss:0.9683\n",
      "Epoch: 643, test AUC: 0.8808, test AP: 0.8810, train AUC: 0.9527, train AP: 0.9457, loss:0.9747\n",
      "Epoch: 644, test AUC: 0.8809, test AP: 0.8811, train AUC: 0.9528, train AP: 0.9458, loss:0.9650\n",
      "Epoch: 645, test AUC: 0.8809, test AP: 0.8811, train AUC: 0.9528, train AP: 0.9458, loss:0.9660\n",
      "Epoch: 646, test AUC: 0.8810, test AP: 0.8812, train AUC: 0.9528, train AP: 0.9459, loss:0.9728\n",
      "Epoch: 647, test AUC: 0.8811, test AP: 0.8813, train AUC: 0.9529, train AP: 0.9459, loss:0.9723\n",
      "Epoch: 648, test AUC: 0.8811, test AP: 0.8813, train AUC: 0.9529, train AP: 0.9460, loss:0.9727\n",
      "Epoch: 649, test AUC: 0.8812, test AP: 0.8814, train AUC: 0.9530, train AP: 0.9460, loss:0.9859\n",
      "Epoch: 650, test AUC: 0.8812, test AP: 0.8814, train AUC: 0.9531, train AP: 0.9461, loss:0.9753\n",
      "Epoch: 651, test AUC: 0.8813, test AP: 0.8815, train AUC: 0.9532, train AP: 0.9462, loss:0.9777\n",
      "Epoch: 652, test AUC: 0.8813, test AP: 0.8816, train AUC: 0.9532, train AP: 0.9463, loss:0.9715\n",
      "Epoch: 653, test AUC: 0.8813, test AP: 0.8816, train AUC: 0.9533, train AP: 0.9463, loss:0.9696\n",
      "Epoch: 654, test AUC: 0.8814, test AP: 0.8816, train AUC: 0.9533, train AP: 0.9464, loss:0.9695\n",
      "Epoch: 655, test AUC: 0.8814, test AP: 0.8816, train AUC: 0.9533, train AP: 0.9464, loss:0.9757\n",
      "Epoch: 656, test AUC: 0.8814, test AP: 0.8816, train AUC: 0.9533, train AP: 0.9463, loss:0.9669\n",
      "Epoch: 657, test AUC: 0.8813, test AP: 0.8816, train AUC: 0.9532, train AP: 0.9463, loss:0.9747\n",
      "Epoch: 658, test AUC: 0.8813, test AP: 0.8815, train AUC: 0.9532, train AP: 0.9462, loss:0.9725\n",
      "Epoch: 659, test AUC: 0.8812, test AP: 0.8815, train AUC: 0.9531, train AP: 0.9462, loss:0.9639\n",
      "Epoch: 660, test AUC: 0.8812, test AP: 0.8814, train AUC: 0.9531, train AP: 0.9462, loss:0.9685\n",
      "Epoch: 661, test AUC: 0.8812, test AP: 0.8814, train AUC: 0.9531, train AP: 0.9461, loss:0.9718\n",
      "Epoch: 662, test AUC: 0.8812, test AP: 0.8814, train AUC: 0.9531, train AP: 0.9462, loss:0.9667\n",
      "Epoch: 663, test AUC: 0.8813, test AP: 0.8815, train AUC: 0.9531, train AP: 0.9462, loss:0.9778\n",
      "Epoch: 664, test AUC: 0.8814, test AP: 0.8816, train AUC: 0.9532, train AP: 0.9463, loss:0.9744\n",
      "Epoch: 665, test AUC: 0.8815, test AP: 0.8817, train AUC: 0.9533, train AP: 0.9463, loss:0.9761\n",
      "Epoch: 666, test AUC: 0.8816, test AP: 0.8818, train AUC: 0.9534, train AP: 0.9465, loss:0.9735\n",
      "Epoch: 667, test AUC: 0.8817, test AP: 0.8820, train AUC: 0.9534, train AP: 0.9465, loss:0.9744\n",
      "Epoch: 668, test AUC: 0.8818, test AP: 0.8821, train AUC: 0.9535, train AP: 0.9466, loss:0.9701\n",
      "Epoch: 669, test AUC: 0.8819, test AP: 0.8823, train AUC: 0.9536, train AP: 0.9467, loss:0.9777\n",
      "Epoch: 670, test AUC: 0.8821, test AP: 0.8824, train AUC: 0.9537, train AP: 0.9469, loss:0.9787\n",
      "Epoch: 671, test AUC: 0.8822, test AP: 0.8825, train AUC: 0.9538, train AP: 0.9470, loss:0.9729\n",
      "Epoch: 672, test AUC: 0.8823, test AP: 0.8827, train AUC: 0.9539, train AP: 0.9471, loss:0.9663\n",
      "Epoch: 673, test AUC: 0.8824, test AP: 0.8828, train AUC: 0.9540, train AP: 0.9472, loss:0.9725\n",
      "Epoch: 674, test AUC: 0.8824, test AP: 0.8829, train AUC: 0.9541, train AP: 0.9473, loss:0.9724\n",
      "Epoch: 675, test AUC: 0.8825, test AP: 0.8830, train AUC: 0.9542, train AP: 0.9474, loss:0.9791\n",
      "Epoch: 676, test AUC: 0.8825, test AP: 0.8830, train AUC: 0.9542, train AP: 0.9474, loss:0.9717\n",
      "Epoch: 677, test AUC: 0.8826, test AP: 0.8831, train AUC: 0.9542, train AP: 0.9475, loss:0.9636\n",
      "Epoch: 678, test AUC: 0.8826, test AP: 0.8831, train AUC: 0.9542, train AP: 0.9475, loss:0.9672\n",
      "Epoch: 679, test AUC: 0.8826, test AP: 0.8831, train AUC: 0.9542, train AP: 0.9474, loss:0.9654\n",
      "Epoch: 680, test AUC: 0.8826, test AP: 0.8830, train AUC: 0.9542, train AP: 0.9474, loss:0.9755\n",
      "Epoch: 681, test AUC: 0.8826, test AP: 0.8830, train AUC: 0.9542, train AP: 0.9474, loss:0.9757\n",
      "Epoch: 682, test AUC: 0.8826, test AP: 0.8829, train AUC: 0.9542, train AP: 0.9474, loss:0.9675\n",
      "Epoch: 683, test AUC: 0.8825, test AP: 0.8829, train AUC: 0.9542, train AP: 0.9474, loss:0.9747\n",
      "Epoch: 684, test AUC: 0.8826, test AP: 0.8829, train AUC: 0.9542, train AP: 0.9474, loss:0.9694\n",
      "Epoch: 685, test AUC: 0.8826, test AP: 0.8830, train AUC: 0.9542, train AP: 0.9474, loss:0.9727\n",
      "Epoch: 686, test AUC: 0.8827, test AP: 0.8830, train AUC: 0.9543, train AP: 0.9475, loss:0.9702\n",
      "Epoch: 687, test AUC: 0.8828, test AP: 0.8831, train AUC: 0.9543, train AP: 0.9475, loss:0.9707\n",
      "Epoch: 688, test AUC: 0.8829, test AP: 0.8832, train AUC: 0.9543, train AP: 0.9475, loss:0.9657\n",
      "Epoch: 689, test AUC: 0.8829, test AP: 0.8833, train AUC: 0.9544, train AP: 0.9476, loss:0.9665\n",
      "Epoch: 690, test AUC: 0.8830, test AP: 0.8833, train AUC: 0.9544, train AP: 0.9476, loss:0.9747\n",
      "Epoch: 691, test AUC: 0.8830, test AP: 0.8834, train AUC: 0.9544, train AP: 0.9476, loss:0.9752\n",
      "Epoch: 692, test AUC: 0.8830, test AP: 0.8834, train AUC: 0.9544, train AP: 0.9476, loss:0.9665\n",
      "Epoch: 693, test AUC: 0.8830, test AP: 0.8834, train AUC: 0.9545, train AP: 0.9477, loss:0.9657\n",
      "Epoch: 694, test AUC: 0.8831, test AP: 0.8834, train AUC: 0.9545, train AP: 0.9477, loss:0.9705\n",
      "Epoch: 695, test AUC: 0.8831, test AP: 0.8835, train AUC: 0.9546, train AP: 0.9478, loss:0.9716\n",
      "Epoch: 696, test AUC: 0.8832, test AP: 0.8836, train AUC: 0.9546, train AP: 0.9479, loss:0.9748\n",
      "Epoch: 697, test AUC: 0.8832, test AP: 0.8836, train AUC: 0.9547, train AP: 0.9480, loss:0.9707\n",
      "Epoch: 698, test AUC: 0.8833, test AP: 0.8837, train AUC: 0.9548, train AP: 0.9481, loss:0.9805\n",
      "Epoch: 699, test AUC: 0.8833, test AP: 0.8838, train AUC: 0.9549, train AP: 0.9482, loss:0.9630\n",
      "Epoch: 700, test AUC: 0.8833, test AP: 0.8838, train AUC: 0.9549, train AP: 0.9482, loss:0.9713\n",
      "Epoch: 701, test AUC: 0.8833, test AP: 0.8838, train AUC: 0.9549, train AP: 0.9482, loss:0.9679\n",
      "Epoch: 702, test AUC: 0.8833, test AP: 0.8838, train AUC: 0.9550, train AP: 0.9483, loss:0.9654\n",
      "Epoch: 703, test AUC: 0.8834, test AP: 0.8838, train AUC: 0.9550, train AP: 0.9483, loss:0.9731\n",
      "Epoch: 704, test AUC: 0.8834, test AP: 0.8838, train AUC: 0.9550, train AP: 0.9483, loss:0.9732\n",
      "Epoch: 705, test AUC: 0.8835, test AP: 0.8839, train AUC: 0.9550, train AP: 0.9484, loss:0.9653\n",
      "Epoch: 706, test AUC: 0.8836, test AP: 0.8840, train AUC: 0.9551, train AP: 0.9484, loss:0.9645\n",
      "Epoch: 707, test AUC: 0.8836, test AP: 0.8840, train AUC: 0.9551, train AP: 0.9484, loss:0.9704\n",
      "Epoch: 708, test AUC: 0.8837, test AP: 0.8841, train AUC: 0.9552, train AP: 0.9485, loss:0.9710\n",
      "Epoch: 709, test AUC: 0.8837, test AP: 0.8841, train AUC: 0.9552, train AP: 0.9485, loss:0.9717\n",
      "Epoch: 710, test AUC: 0.8837, test AP: 0.8841, train AUC: 0.9552, train AP: 0.9485, loss:0.9649\n",
      "Epoch: 711, test AUC: 0.8838, test AP: 0.8841, train AUC: 0.9552, train AP: 0.9485, loss:0.9690\n",
      "Epoch: 712, test AUC: 0.8838, test AP: 0.8841, train AUC: 0.9552, train AP: 0.9485, loss:0.9670\n",
      "Epoch: 713, test AUC: 0.8837, test AP: 0.8841, train AUC: 0.9552, train AP: 0.9485, loss:0.9777\n",
      "Epoch: 714, test AUC: 0.8837, test AP: 0.8841, train AUC: 0.9552, train AP: 0.9485, loss:0.9697\n",
      "Epoch: 715, test AUC: 0.8837, test AP: 0.8841, train AUC: 0.9553, train AP: 0.9486, loss:0.9663\n",
      "Epoch: 716, test AUC: 0.8837, test AP: 0.8841, train AUC: 0.9553, train AP: 0.9486, loss:0.9671\n",
      "Epoch: 717, test AUC: 0.8837, test AP: 0.8841, train AUC: 0.9553, train AP: 0.9487, loss:0.9633\n",
      "Epoch: 718, test AUC: 0.8838, test AP: 0.8842, train AUC: 0.9554, train AP: 0.9487, loss:0.9707\n",
      "Epoch: 719, test AUC: 0.8838, test AP: 0.8842, train AUC: 0.9554, train AP: 0.9488, loss:0.9707\n",
      "Epoch: 720, test AUC: 0.8838, test AP: 0.8842, train AUC: 0.9554, train AP: 0.9488, loss:0.9661\n",
      "Epoch: 721, test AUC: 0.8838, test AP: 0.8842, train AUC: 0.9555, train AP: 0.9488, loss:0.9618\n",
      "Epoch: 722, test AUC: 0.8838, test AP: 0.8842, train AUC: 0.9555, train AP: 0.9488, loss:0.9621\n",
      "Epoch: 723, test AUC: 0.8838, test AP: 0.8843, train AUC: 0.9555, train AP: 0.9488, loss:0.9648\n",
      "Epoch: 724, test AUC: 0.8839, test AP: 0.8844, train AUC: 0.9556, train AP: 0.9489, loss:0.9696\n",
      "Epoch: 725, test AUC: 0.8840, test AP: 0.8844, train AUC: 0.9556, train AP: 0.9490, loss:0.9641\n",
      "Epoch: 726, test AUC: 0.8840, test AP: 0.8845, train AUC: 0.9557, train AP: 0.9490, loss:0.9679\n",
      "Epoch: 727, test AUC: 0.8841, test AP: 0.8846, train AUC: 0.9557, train AP: 0.9490, loss:0.9716\n",
      "Epoch: 728, test AUC: 0.8841, test AP: 0.8846, train AUC: 0.9557, train AP: 0.9490, loss:0.9701\n",
      "Epoch: 729, test AUC: 0.8841, test AP: 0.8846, train AUC: 0.9557, train AP: 0.9490, loss:0.9678\n",
      "Epoch: 730, test AUC: 0.8842, test AP: 0.8847, train AUC: 0.9557, train AP: 0.9490, loss:0.9716\n",
      "Epoch: 731, test AUC: 0.8843, test AP: 0.8847, train AUC: 0.9557, train AP: 0.9490, loss:0.9655\n",
      "Epoch: 732, test AUC: 0.8843, test AP: 0.8848, train AUC: 0.9557, train AP: 0.9490, loss:0.9666\n",
      "Epoch: 733, test AUC: 0.8844, test AP: 0.8848, train AUC: 0.9557, train AP: 0.9490, loss:0.9675\n",
      "Epoch: 734, test AUC: 0.8844, test AP: 0.8849, train AUC: 0.9558, train AP: 0.9491, loss:0.9698\n",
      "Epoch: 735, test AUC: 0.8844, test AP: 0.8849, train AUC: 0.9558, train AP: 0.9491, loss:0.9690\n",
      "Epoch: 736, test AUC: 0.8844, test AP: 0.8849, train AUC: 0.9558, train AP: 0.9491, loss:0.9644\n",
      "Epoch: 737, test AUC: 0.8845, test AP: 0.8849, train AUC: 0.9558, train AP: 0.9491, loss:0.9688\n",
      "Epoch: 738, test AUC: 0.8845, test AP: 0.8850, train AUC: 0.9559, train AP: 0.9492, loss:0.9647\n",
      "Epoch: 739, test AUC: 0.8846, test AP: 0.8850, train AUC: 0.9559, train AP: 0.9492, loss:0.9625\n",
      "Epoch: 740, test AUC: 0.8846, test AP: 0.8851, train AUC: 0.9559, train AP: 0.9492, loss:0.9670\n",
      "Epoch: 741, test AUC: 0.8847, test AP: 0.8851, train AUC: 0.9560, train AP: 0.9493, loss:0.9706\n",
      "Epoch: 742, test AUC: 0.8847, test AP: 0.8852, train AUC: 0.9560, train AP: 0.9493, loss:0.9685\n",
      "Epoch: 743, test AUC: 0.8847, test AP: 0.8852, train AUC: 0.9560, train AP: 0.9493, loss:0.9684\n",
      "Epoch: 744, test AUC: 0.8847, test AP: 0.8852, train AUC: 0.9560, train AP: 0.9494, loss:0.9621\n",
      "Epoch: 745, test AUC: 0.8847, test AP: 0.8852, train AUC: 0.9560, train AP: 0.9494, loss:0.9659\n",
      "Epoch: 746, test AUC: 0.8847, test AP: 0.8851, train AUC: 0.9560, train AP: 0.9494, loss:0.9724\n",
      "Epoch: 747, test AUC: 0.8847, test AP: 0.8851, train AUC: 0.9561, train AP: 0.9495, loss:0.9668\n",
      "Epoch: 748, test AUC: 0.8847, test AP: 0.8851, train AUC: 0.9561, train AP: 0.9495, loss:0.9690\n",
      "Epoch: 749, test AUC: 0.8847, test AP: 0.8851, train AUC: 0.9561, train AP: 0.9496, loss:0.9757\n",
      "Epoch: 750, test AUC: 0.8847, test AP: 0.8851, train AUC: 0.9561, train AP: 0.9496, loss:0.9635\n",
      "Epoch: 751, test AUC: 0.8846, test AP: 0.8851, train AUC: 0.9561, train AP: 0.9496, loss:0.9670\n",
      "Epoch: 752, test AUC: 0.8846, test AP: 0.8851, train AUC: 0.9561, train AP: 0.9496, loss:0.9718\n",
      "Epoch: 753, test AUC: 0.8846, test AP: 0.8851, train AUC: 0.9562, train AP: 0.9496, loss:0.9606\n",
      "Epoch: 754, test AUC: 0.8847, test AP: 0.8852, train AUC: 0.9562, train AP: 0.9496, loss:0.9674\n",
      "Epoch: 755, test AUC: 0.8847, test AP: 0.8852, train AUC: 0.9562, train AP: 0.9496, loss:0.9730\n",
      "Epoch: 756, test AUC: 0.8848, test AP: 0.8853, train AUC: 0.9562, train AP: 0.9496, loss:0.9650\n",
      "Epoch: 757, test AUC: 0.8848, test AP: 0.8853, train AUC: 0.9562, train AP: 0.9496, loss:0.9696\n",
      "Epoch: 758, test AUC: 0.8848, test AP: 0.8853, train AUC: 0.9562, train AP: 0.9496, loss:0.9635\n",
      "Epoch: 759, test AUC: 0.8848, test AP: 0.8852, train AUC: 0.9562, train AP: 0.9496, loss:0.9681\n",
      "Epoch: 760, test AUC: 0.8848, test AP: 0.8852, train AUC: 0.9562, train AP: 0.9495, loss:0.9771\n",
      "Epoch: 761, test AUC: 0.8848, test AP: 0.8852, train AUC: 0.9561, train AP: 0.9495, loss:0.9740\n",
      "Epoch: 762, test AUC: 0.8848, test AP: 0.8852, train AUC: 0.9561, train AP: 0.9495, loss:0.9634\n",
      "Epoch: 763, test AUC: 0.8848, test AP: 0.8852, train AUC: 0.9561, train AP: 0.9495, loss:0.9690\n",
      "Epoch: 764, test AUC: 0.8849, test AP: 0.8853, train AUC: 0.9562, train AP: 0.9495, loss:0.9593\n",
      "Epoch: 765, test AUC: 0.8850, test AP: 0.8853, train AUC: 0.9562, train AP: 0.9496, loss:0.9714\n",
      "Epoch: 766, test AUC: 0.8851, test AP: 0.8855, train AUC: 0.9563, train AP: 0.9497, loss:0.9756\n",
      "Epoch: 767, test AUC: 0.8852, test AP: 0.8856, train AUC: 0.9564, train AP: 0.9498, loss:0.9700\n",
      "Epoch: 768, test AUC: 0.8853, test AP: 0.8857, train AUC: 0.9565, train AP: 0.9499, loss:0.9639\n",
      "Epoch: 769, test AUC: 0.8854, test AP: 0.8858, train AUC: 0.9565, train AP: 0.9500, loss:0.9703\n",
      "Epoch: 770, test AUC: 0.8854, test AP: 0.8859, train AUC: 0.9566, train AP: 0.9501, loss:0.9731\n",
      "Epoch: 771, test AUC: 0.8855, test AP: 0.8860, train AUC: 0.9567, train AP: 0.9502, loss:0.9764\n",
      "Epoch: 772, test AUC: 0.8856, test AP: 0.8861, train AUC: 0.9567, train AP: 0.9502, loss:0.9708\n",
      "Epoch: 773, test AUC: 0.8857, test AP: 0.8862, train AUC: 0.9568, train AP: 0.9503, loss:0.9630\n",
      "Epoch: 774, test AUC: 0.8857, test AP: 0.8863, train AUC: 0.9569, train AP: 0.9504, loss:0.9705\n",
      "Epoch: 775, test AUC: 0.8858, test AP: 0.8864, train AUC: 0.9569, train AP: 0.9504, loss:0.9707\n",
      "Epoch: 776, test AUC: 0.8858, test AP: 0.8864, train AUC: 0.9569, train AP: 0.9504, loss:0.9714\n",
      "Epoch: 777, test AUC: 0.8858, test AP: 0.8863, train AUC: 0.9569, train AP: 0.9504, loss:0.9707\n",
      "Epoch: 778, test AUC: 0.8857, test AP: 0.8863, train AUC: 0.9569, train AP: 0.9503, loss:0.9588\n",
      "Epoch: 779, test AUC: 0.8857, test AP: 0.8862, train AUC: 0.9568, train AP: 0.9503, loss:0.9680\n",
      "Epoch: 780, test AUC: 0.8856, test AP: 0.8862, train AUC: 0.9568, train AP: 0.9503, loss:0.9659\n",
      "Epoch: 781, test AUC: 0.8856, test AP: 0.8862, train AUC: 0.9568, train AP: 0.9503, loss:0.9634\n",
      "Epoch: 782, test AUC: 0.8856, test AP: 0.8862, train AUC: 0.9568, train AP: 0.9503, loss:0.9605\n",
      "Epoch: 783, test AUC: 0.8856, test AP: 0.8861, train AUC: 0.9568, train AP: 0.9503, loss:0.9612\n",
      "Epoch: 784, test AUC: 0.8856, test AP: 0.8862, train AUC: 0.9569, train AP: 0.9503, loss:0.9724\n",
      "Epoch: 785, test AUC: 0.8857, test AP: 0.8862, train AUC: 0.9569, train AP: 0.9504, loss:0.9620\n",
      "Epoch: 786, test AUC: 0.8858, test AP: 0.8863, train AUC: 0.9570, train AP: 0.9505, loss:0.9678\n",
      "Epoch: 787, test AUC: 0.8859, test AP: 0.8865, train AUC: 0.9571, train AP: 0.9506, loss:0.9650\n",
      "Epoch: 788, test AUC: 0.8860, test AP: 0.8866, train AUC: 0.9572, train AP: 0.9507, loss:0.9702\n",
      "Epoch: 789, test AUC: 0.8861, test AP: 0.8867, train AUC: 0.9573, train AP: 0.9508, loss:0.9625\n",
      "Epoch: 790, test AUC: 0.8862, test AP: 0.8869, train AUC: 0.9574, train AP: 0.9509, loss:0.9708\n",
      "Epoch: 791, test AUC: 0.8863, test AP: 0.8870, train AUC: 0.9574, train AP: 0.9510, loss:0.9686\n",
      "Epoch: 792, test AUC: 0.8864, test AP: 0.8871, train AUC: 0.9575, train AP: 0.9510, loss:0.9657\n",
      "Epoch: 793, test AUC: 0.8864, test AP: 0.8871, train AUC: 0.9574, train AP: 0.9510, loss:0.9650\n",
      "Epoch: 794, test AUC: 0.8864, test AP: 0.8871, train AUC: 0.9574, train AP: 0.9510, loss:0.9729\n",
      "Epoch: 795, test AUC: 0.8864, test AP: 0.8871, train AUC: 0.9574, train AP: 0.9510, loss:0.9676\n",
      "Epoch: 796, test AUC: 0.8864, test AP: 0.8871, train AUC: 0.9574, train AP: 0.9510, loss:0.9682\n",
      "Epoch: 797, test AUC: 0.8864, test AP: 0.8871, train AUC: 0.9574, train AP: 0.9510, loss:0.9643\n",
      "Epoch: 798, test AUC: 0.8864, test AP: 0.8870, train AUC: 0.9574, train AP: 0.9509, loss:0.9629\n",
      "Epoch: 799, test AUC: 0.8863, test AP: 0.8869, train AUC: 0.9573, train AP: 0.9509, loss:0.9739\n",
      "Epoch: 800, test AUC: 0.8863, test AP: 0.8869, train AUC: 0.9573, train AP: 0.9508, loss:0.9642\n",
      "Epoch: 801, test AUC: 0.8862, test AP: 0.8868, train AUC: 0.9572, train AP: 0.9508, loss:0.9749\n",
      "Epoch: 802, test AUC: 0.8862, test AP: 0.8867, train AUC: 0.9572, train AP: 0.9508, loss:0.9685\n",
      "Epoch: 803, test AUC: 0.8861, test AP: 0.8867, train AUC: 0.9572, train AP: 0.9508, loss:0.9679\n",
      "Epoch: 804, test AUC: 0.8861, test AP: 0.8867, train AUC: 0.9572, train AP: 0.9508, loss:0.9636\n",
      "Epoch: 805, test AUC: 0.8861, test AP: 0.8867, train AUC: 0.9572, train AP: 0.9508, loss:0.9695\n",
      "Epoch: 806, test AUC: 0.8861, test AP: 0.8868, train AUC: 0.9573, train AP: 0.9508, loss:0.9673\n",
      "Epoch: 807, test AUC: 0.8861, test AP: 0.8868, train AUC: 0.9573, train AP: 0.9509, loss:0.9614\n",
      "Epoch: 808, test AUC: 0.8862, test AP: 0.8869, train AUC: 0.9574, train AP: 0.9510, loss:0.9682\n",
      "Epoch: 809, test AUC: 0.8863, test AP: 0.8870, train AUC: 0.9575, train AP: 0.9511, loss:0.9691\n",
      "Epoch: 810, test AUC: 0.8864, test AP: 0.8871, train AUC: 0.9575, train AP: 0.9511, loss:0.9699\n",
      "Epoch: 811, test AUC: 0.8866, test AP: 0.8873, train AUC: 0.9576, train AP: 0.9513, loss:0.9672\n",
      "Epoch: 812, test AUC: 0.8867, test AP: 0.8874, train AUC: 0.9577, train AP: 0.9513, loss:0.9685\n",
      "Epoch: 813, test AUC: 0.8867, test AP: 0.8876, train AUC: 0.9578, train AP: 0.9514, loss:0.9658\n",
      "Epoch: 814, test AUC: 0.8868, test AP: 0.8877, train AUC: 0.9578, train AP: 0.9515, loss:0.9742\n",
      "Epoch: 815, test AUC: 0.8869, test AP: 0.8878, train AUC: 0.9579, train AP: 0.9516, loss:0.9609\n",
      "Epoch: 816, test AUC: 0.8870, test AP: 0.8878, train AUC: 0.9580, train AP: 0.9517, loss:0.9701\n",
      "Epoch: 817, test AUC: 0.8870, test AP: 0.8879, train AUC: 0.9580, train AP: 0.9517, loss:0.9575\n",
      "Epoch: 818, test AUC: 0.8870, test AP: 0.8879, train AUC: 0.9580, train AP: 0.9517, loss:0.9614\n",
      "Epoch: 819, test AUC: 0.8871, test AP: 0.8879, train AUC: 0.9580, train AP: 0.9517, loss:0.9670\n",
      "Epoch: 820, test AUC: 0.8871, test AP: 0.8879, train AUC: 0.9580, train AP: 0.9517, loss:0.9652\n",
      "Epoch: 821, test AUC: 0.8870, test AP: 0.8878, train AUC: 0.9580, train AP: 0.9517, loss:0.9619\n",
      "Epoch: 822, test AUC: 0.8870, test AP: 0.8878, train AUC: 0.9580, train AP: 0.9517, loss:0.9682\n",
      "Epoch: 823, test AUC: 0.8869, test AP: 0.8877, train AUC: 0.9579, train AP: 0.9516, loss:0.9717\n",
      "Epoch: 824, test AUC: 0.8868, test AP: 0.8876, train AUC: 0.9578, train AP: 0.9515, loss:0.9648\n",
      "Epoch: 825, test AUC: 0.8868, test AP: 0.8875, train AUC: 0.9577, train AP: 0.9514, loss:0.9664\n",
      "Epoch: 826, test AUC: 0.8867, test AP: 0.8875, train AUC: 0.9577, train AP: 0.9513, loss:0.9731\n",
      "Epoch: 827, test AUC: 0.8867, test AP: 0.8874, train AUC: 0.9576, train AP: 0.9512, loss:0.9701\n",
      "Epoch: 828, test AUC: 0.8867, test AP: 0.8874, train AUC: 0.9576, train AP: 0.9512, loss:0.9694\n",
      "Epoch: 829, test AUC: 0.8868, test AP: 0.8875, train AUC: 0.9577, train AP: 0.9513, loss:0.9623\n",
      "Epoch: 830, test AUC: 0.8869, test AP: 0.8876, train AUC: 0.9577, train AP: 0.9513, loss:0.9672\n",
      "Epoch: 831, test AUC: 0.8870, test AP: 0.8877, train AUC: 0.9578, train AP: 0.9514, loss:0.9588\n",
      "Epoch: 832, test AUC: 0.8871, test AP: 0.8878, train AUC: 0.9579, train AP: 0.9515, loss:0.9616\n",
      "Epoch: 833, test AUC: 0.8871, test AP: 0.8879, train AUC: 0.9580, train AP: 0.9516, loss:0.9613\n",
      "Epoch: 834, test AUC: 0.8872, test AP: 0.8880, train AUC: 0.9581, train AP: 0.9518, loss:0.9642\n",
      "Epoch: 835, test AUC: 0.8873, test AP: 0.8881, train AUC: 0.9582, train AP: 0.9519, loss:0.9693\n",
      "Epoch: 836, test AUC: 0.8873, test AP: 0.8881, train AUC: 0.9582, train AP: 0.9519, loss:0.9650\n",
      "Epoch: 837, test AUC: 0.8873, test AP: 0.8881, train AUC: 0.9582, train AP: 0.9519, loss:0.9701\n",
      "Epoch: 838, test AUC: 0.8872, test AP: 0.8880, train AUC: 0.9582, train AP: 0.9519, loss:0.9638\n",
      "Epoch: 839, test AUC: 0.8872, test AP: 0.8880, train AUC: 0.9582, train AP: 0.9519, loss:0.9607\n",
      "Epoch: 840, test AUC: 0.8872, test AP: 0.8879, train AUC: 0.9581, train AP: 0.9518, loss:0.9614\n",
      "Epoch: 841, test AUC: 0.8871, test AP: 0.8878, train AUC: 0.9581, train AP: 0.9518, loss:0.9645\n",
      "Epoch: 842, test AUC: 0.8871, test AP: 0.8878, train AUC: 0.9580, train AP: 0.9517, loss:0.9687\n",
      "Epoch: 843, test AUC: 0.8871, test AP: 0.8878, train AUC: 0.9580, train AP: 0.9517, loss:0.9596\n",
      "Epoch: 844, test AUC: 0.8872, test AP: 0.8878, train AUC: 0.9580, train AP: 0.9517, loss:0.9633\n",
      "Epoch: 845, test AUC: 0.8872, test AP: 0.8879, train AUC: 0.9580, train AP: 0.9516, loss:0.9653\n",
      "Epoch: 846, test AUC: 0.8873, test AP: 0.8880, train AUC: 0.9580, train AP: 0.9517, loss:0.9630\n",
      "Epoch: 847, test AUC: 0.8874, test AP: 0.8881, train AUC: 0.9581, train AP: 0.9517, loss:0.9653\n",
      "Epoch: 848, test AUC: 0.8876, test AP: 0.8883, train AUC: 0.9581, train AP: 0.9518, loss:0.9649\n",
      "Epoch: 849, test AUC: 0.8877, test AP: 0.8885, train AUC: 0.9582, train AP: 0.9519, loss:0.9615\n",
      "Epoch: 850, test AUC: 0.8878, test AP: 0.8886, train AUC: 0.9583, train AP: 0.9520, loss:0.9666\n",
      "Epoch: 851, test AUC: 0.8879, test AP: 0.8888, train AUC: 0.9584, train AP: 0.9521, loss:0.9584\n",
      "Epoch: 852, test AUC: 0.8880, test AP: 0.8888, train AUC: 0.9584, train AP: 0.9521, loss:0.9596\n",
      "Epoch: 853, test AUC: 0.8880, test AP: 0.8888, train AUC: 0.9584, train AP: 0.9521, loss:0.9654\n",
      "Epoch: 854, test AUC: 0.8880, test AP: 0.8888, train AUC: 0.9584, train AP: 0.9521, loss:0.9605\n",
      "Epoch: 855, test AUC: 0.8880, test AP: 0.8888, train AUC: 0.9584, train AP: 0.9521, loss:0.9713\n",
      "Epoch: 856, test AUC: 0.8880, test AP: 0.8888, train AUC: 0.9584, train AP: 0.9521, loss:0.9635\n",
      "Epoch: 857, test AUC: 0.8880, test AP: 0.8888, train AUC: 0.9584, train AP: 0.9521, loss:0.9652\n",
      "Epoch: 858, test AUC: 0.8880, test AP: 0.8887, train AUC: 0.9584, train AP: 0.9521, loss:0.9675\n",
      "Epoch: 859, test AUC: 0.8880, test AP: 0.8887, train AUC: 0.9584, train AP: 0.9521, loss:0.9680\n",
      "Epoch: 860, test AUC: 0.8880, test AP: 0.8887, train AUC: 0.9584, train AP: 0.9521, loss:0.9663\n",
      "Epoch: 861, test AUC: 0.8880, test AP: 0.8886, train AUC: 0.9584, train AP: 0.9521, loss:0.9640\n",
      "Epoch: 862, test AUC: 0.8879, test AP: 0.8886, train AUC: 0.9583, train AP: 0.9520, loss:0.9694\n",
      "Epoch: 863, test AUC: 0.8879, test AP: 0.8886, train AUC: 0.9583, train AP: 0.9519, loss:0.9542\n",
      "Epoch: 864, test AUC: 0.8879, test AP: 0.8886, train AUC: 0.9583, train AP: 0.9519, loss:0.9607\n",
      "Epoch: 865, test AUC: 0.8879, test AP: 0.8886, train AUC: 0.9583, train AP: 0.9520, loss:0.9573\n",
      "Epoch: 866, test AUC: 0.8880, test AP: 0.8887, train AUC: 0.9584, train AP: 0.9520, loss:0.9627\n",
      "Epoch: 867, test AUC: 0.8881, test AP: 0.8888, train AUC: 0.9585, train AP: 0.9521, loss:0.9627\n",
      "Epoch: 868, test AUC: 0.8881, test AP: 0.8889, train AUC: 0.9585, train AP: 0.9522, loss:0.9681\n",
      "Epoch: 869, test AUC: 0.8882, test AP: 0.8890, train AUC: 0.9586, train AP: 0.9523, loss:0.9661\n",
      "Epoch: 870, test AUC: 0.8883, test AP: 0.8891, train AUC: 0.9587, train AP: 0.9524, loss:0.9638\n",
      "Epoch: 871, test AUC: 0.8883, test AP: 0.8892, train AUC: 0.9588, train AP: 0.9525, loss:0.9650\n",
      "Epoch: 872, test AUC: 0.8884, test AP: 0.8892, train AUC: 0.9588, train AP: 0.9525, loss:0.9600\n",
      "Epoch: 873, test AUC: 0.8884, test AP: 0.8892, train AUC: 0.9588, train AP: 0.9525, loss:0.9587\n",
      "Epoch: 874, test AUC: 0.8885, test AP: 0.8893, train AUC: 0.9588, train AP: 0.9526, loss:0.9612\n",
      "Epoch: 875, test AUC: 0.8885, test AP: 0.8893, train AUC: 0.9589, train AP: 0.9526, loss:0.9653\n",
      "Epoch: 876, test AUC: 0.8885, test AP: 0.8893, train AUC: 0.9589, train AP: 0.9526, loss:0.9575\n",
      "Epoch: 877, test AUC: 0.8886, test AP: 0.8894, train AUC: 0.9589, train AP: 0.9527, loss:0.9650\n",
      "Epoch: 878, test AUC: 0.8886, test AP: 0.8894, train AUC: 0.9589, train AP: 0.9527, loss:0.9620\n",
      "Epoch: 879, test AUC: 0.8886, test AP: 0.8894, train AUC: 0.9589, train AP: 0.9526, loss:0.9658\n",
      "Epoch: 880, test AUC: 0.8886, test AP: 0.8893, train AUC: 0.9588, train AP: 0.9526, loss:0.9633\n",
      "Epoch: 881, test AUC: 0.8886, test AP: 0.8893, train AUC: 0.9588, train AP: 0.9525, loss:0.9587\n",
      "Epoch: 882, test AUC: 0.8885, test AP: 0.8893, train AUC: 0.9587, train AP: 0.9525, loss:0.9693\n",
      "Epoch: 883, test AUC: 0.8886, test AP: 0.8893, train AUC: 0.9587, train AP: 0.9524, loss:0.9664\n",
      "Epoch: 884, test AUC: 0.8886, test AP: 0.8893, train AUC: 0.9587, train AP: 0.9524, loss:0.9629\n",
      "Epoch: 885, test AUC: 0.8887, test AP: 0.8894, train AUC: 0.9588, train AP: 0.9525, loss:0.9653\n",
      "Epoch: 886, test AUC: 0.8888, test AP: 0.8896, train AUC: 0.9588, train AP: 0.9526, loss:0.9637\n",
      "Epoch: 887, test AUC: 0.8890, test AP: 0.8898, train AUC: 0.9589, train AP: 0.9527, loss:0.9624\n",
      "Epoch: 888, test AUC: 0.8891, test AP: 0.8900, train AUC: 0.9591, train AP: 0.9529, loss:0.9612\n",
      "Epoch: 889, test AUC: 0.8892, test AP: 0.8901, train AUC: 0.9592, train AP: 0.9530, loss:0.9696\n",
      "Epoch: 890, test AUC: 0.8893, test AP: 0.8903, train AUC: 0.9593, train AP: 0.9531, loss:0.9709\n",
      "Epoch: 891, test AUC: 0.8893, test AP: 0.8903, train AUC: 0.9593, train AP: 0.9532, loss:0.9674\n",
      "Epoch: 892, test AUC: 0.8893, test AP: 0.8903, train AUC: 0.9593, train AP: 0.9532, loss:0.9636\n",
      "Epoch: 893, test AUC: 0.8893, test AP: 0.8903, train AUC: 0.9594, train AP: 0.9532, loss:0.9627\n",
      "Epoch: 894, test AUC: 0.8893, test AP: 0.8903, train AUC: 0.9594, train AP: 0.9532, loss:0.9671\n",
      "Epoch: 895, test AUC: 0.8893, test AP: 0.8902, train AUC: 0.9593, train AP: 0.9532, loss:0.9674\n",
      "Epoch: 896, test AUC: 0.8892, test AP: 0.8901, train AUC: 0.9593, train AP: 0.9531, loss:0.9687\n",
      "Epoch: 897, test AUC: 0.8891, test AP: 0.8900, train AUC: 0.9592, train AP: 0.9530, loss:0.9603\n",
      "Epoch: 898, test AUC: 0.8890, test AP: 0.8898, train AUC: 0.9591, train AP: 0.9529, loss:0.9661\n",
      "Epoch: 899, test AUC: 0.8889, test AP: 0.8897, train AUC: 0.9590, train AP: 0.9528, loss:0.9658\n",
      "Epoch: 900, test AUC: 0.8889, test AP: 0.8897, train AUC: 0.9590, train AP: 0.9528, loss:0.9650\n",
      "Epoch: 901, test AUC: 0.8889, test AP: 0.8898, train AUC: 0.9590, train AP: 0.9528, loss:0.9632\n",
      "Epoch: 902, test AUC: 0.8890, test AP: 0.8898, train AUC: 0.9590, train AP: 0.9528, loss:0.9642\n",
      "Epoch: 903, test AUC: 0.8890, test AP: 0.8898, train AUC: 0.9590, train AP: 0.9528, loss:0.9590\n",
      "Epoch: 904, test AUC: 0.8890, test AP: 0.8898, train AUC: 0.9590, train AP: 0.9528, loss:0.9600\n",
      "Epoch: 905, test AUC: 0.8891, test AP: 0.8899, train AUC: 0.9591, train AP: 0.9529, loss:0.9658\n",
      "Epoch: 906, test AUC: 0.8892, test AP: 0.8900, train AUC: 0.9591, train AP: 0.9530, loss:0.9607\n",
      "Epoch: 907, test AUC: 0.8892, test AP: 0.8901, train AUC: 0.9592, train AP: 0.9530, loss:0.9658\n",
      "Epoch: 908, test AUC: 0.8893, test AP: 0.8902, train AUC: 0.9593, train AP: 0.9531, loss:0.9540\n",
      "Epoch: 909, test AUC: 0.8894, test AP: 0.8904, train AUC: 0.9594, train AP: 0.9532, loss:0.9606\n",
      "Epoch: 910, test AUC: 0.8895, test AP: 0.8905, train AUC: 0.9594, train AP: 0.9533, loss:0.9613\n",
      "Epoch: 911, test AUC: 0.8896, test AP: 0.8906, train AUC: 0.9595, train AP: 0.9533, loss:0.9677\n",
      "Epoch: 912, test AUC: 0.8896, test AP: 0.8907, train AUC: 0.9595, train AP: 0.9534, loss:0.9609\n",
      "Epoch: 913, test AUC: 0.8897, test AP: 0.8907, train AUC: 0.9595, train AP: 0.9534, loss:0.9625\n",
      "Epoch: 914, test AUC: 0.8897, test AP: 0.8908, train AUC: 0.9596, train AP: 0.9534, loss:0.9684\n",
      "Epoch: 915, test AUC: 0.8898, test AP: 0.8909, train AUC: 0.9596, train AP: 0.9535, loss:0.9634\n",
      "Epoch: 916, test AUC: 0.8898, test AP: 0.8909, train AUC: 0.9596, train AP: 0.9535, loss:0.9643\n",
      "Epoch: 917, test AUC: 0.8898, test AP: 0.8908, train AUC: 0.9596, train AP: 0.9535, loss:0.9636\n",
      "Epoch: 918, test AUC: 0.8897, test AP: 0.8907, train AUC: 0.9596, train AP: 0.9535, loss:0.9659\n",
      "Epoch: 919, test AUC: 0.8896, test AP: 0.8906, train AUC: 0.9595, train AP: 0.9534, loss:0.9679\n",
      "Epoch: 920, test AUC: 0.8895, test AP: 0.8904, train AUC: 0.9595, train AP: 0.9533, loss:0.9635\n",
      "Epoch: 921, test AUC: 0.8894, test AP: 0.8903, train AUC: 0.9594, train AP: 0.9532, loss:0.9600\n",
      "Epoch: 922, test AUC: 0.8893, test AP: 0.8901, train AUC: 0.9593, train AP: 0.9532, loss:0.9712\n",
      "Epoch: 923, test AUC: 0.8892, test AP: 0.8901, train AUC: 0.9593, train AP: 0.9532, loss:0.9614\n",
      "Epoch: 924, test AUC: 0.8892, test AP: 0.8900, train AUC: 0.9593, train AP: 0.9532, loss:0.9561\n",
      "Epoch: 925, test AUC: 0.8892, test AP: 0.8901, train AUC: 0.9593, train AP: 0.9532, loss:0.9582\n",
      "Epoch: 926, test AUC: 0.8892, test AP: 0.8901, train AUC: 0.9593, train AP: 0.9532, loss:0.9643\n",
      "Epoch: 927, test AUC: 0.8892, test AP: 0.8901, train AUC: 0.9594, train AP: 0.9533, loss:0.9630\n",
      "Epoch: 928, test AUC: 0.8893, test AP: 0.8902, train AUC: 0.9594, train AP: 0.9533, loss:0.9586\n",
      "Epoch: 929, test AUC: 0.8894, test AP: 0.8903, train AUC: 0.9595, train AP: 0.9533, loss:0.9671\n",
      "Epoch: 930, test AUC: 0.8895, test AP: 0.8905, train AUC: 0.9595, train AP: 0.9534, loss:0.9679\n",
      "Epoch: 931, test AUC: 0.8897, test AP: 0.8907, train AUC: 0.9597, train AP: 0.9536, loss:0.9642\n",
      "Epoch: 932, test AUC: 0.8898, test AP: 0.8910, train AUC: 0.9598, train AP: 0.9537, loss:0.9603\n",
      "Epoch: 933, test AUC: 0.8900, test AP: 0.8911, train AUC: 0.9599, train AP: 0.9538, loss:0.9592\n",
      "Epoch: 934, test AUC: 0.8901, test AP: 0.8912, train AUC: 0.9599, train AP: 0.9539, loss:0.9663\n",
      "Epoch: 935, test AUC: 0.8901, test AP: 0.8913, train AUC: 0.9600, train AP: 0.9539, loss:0.9584\n",
      "Epoch: 936, test AUC: 0.8901, test AP: 0.8913, train AUC: 0.9600, train AP: 0.9539, loss:0.9643\n",
      "Epoch: 937, test AUC: 0.8901, test AP: 0.8913, train AUC: 0.9599, train AP: 0.9539, loss:0.9634\n",
      "Epoch: 938, test AUC: 0.8901, test AP: 0.8913, train AUC: 0.9599, train AP: 0.9539, loss:0.9605\n",
      "Epoch: 939, test AUC: 0.8901, test AP: 0.8912, train AUC: 0.9599, train AP: 0.9538, loss:0.9663\n",
      "Epoch: 940, test AUC: 0.8901, test AP: 0.8912, train AUC: 0.9599, train AP: 0.9538, loss:0.9635\n",
      "Epoch: 941, test AUC: 0.8900, test AP: 0.8911, train AUC: 0.9599, train AP: 0.9538, loss:0.9587\n",
      "Epoch: 942, test AUC: 0.8900, test AP: 0.8911, train AUC: 0.9599, train AP: 0.9538, loss:0.9579\n",
      "Epoch: 943, test AUC: 0.8899, test AP: 0.8910, train AUC: 0.9599, train AP: 0.9538, loss:0.9644\n",
      "Epoch: 944, test AUC: 0.8898, test AP: 0.8909, train AUC: 0.9598, train AP: 0.9538, loss:0.9512\n",
      "Epoch: 945, test AUC: 0.8898, test AP: 0.8908, train AUC: 0.9598, train AP: 0.9537, loss:0.9601\n",
      "Epoch: 946, test AUC: 0.8897, test AP: 0.8906, train AUC: 0.9597, train AP: 0.9537, loss:0.9591\n",
      "Epoch: 947, test AUC: 0.8896, test AP: 0.8906, train AUC: 0.9597, train AP: 0.9536, loss:0.9674\n",
      "Epoch: 948, test AUC: 0.8897, test AP: 0.8906, train AUC: 0.9597, train AP: 0.9536, loss:0.9658\n",
      "Epoch: 949, test AUC: 0.8897, test AP: 0.8907, train AUC: 0.9597, train AP: 0.9537, loss:0.9546\n",
      "Epoch: 950, test AUC: 0.8898, test AP: 0.8908, train AUC: 0.9598, train AP: 0.9537, loss:0.9583\n",
      "Epoch: 951, test AUC: 0.8899, test AP: 0.8909, train AUC: 0.9598, train AP: 0.9537, loss:0.9633\n",
      "Epoch: 952, test AUC: 0.8900, test AP: 0.8910, train AUC: 0.9598, train AP: 0.9538, loss:0.9714\n",
      "Epoch: 953, test AUC: 0.8900, test AP: 0.8911, train AUC: 0.9599, train AP: 0.9538, loss:0.9639\n",
      "Epoch: 954, test AUC: 0.8901, test AP: 0.8912, train AUC: 0.9599, train AP: 0.9538, loss:0.9576\n",
      "Epoch: 955, test AUC: 0.8902, test AP: 0.8913, train AUC: 0.9599, train AP: 0.9539, loss:0.9645\n",
      "Epoch: 956, test AUC: 0.8902, test AP: 0.8913, train AUC: 0.9599, train AP: 0.9539, loss:0.9575\n",
      "Epoch: 957, test AUC: 0.8902, test AP: 0.8913, train AUC: 0.9599, train AP: 0.9539, loss:0.9569\n",
      "Epoch: 958, test AUC: 0.8903, test AP: 0.8914, train AUC: 0.9600, train AP: 0.9539, loss:0.9597\n",
      "Epoch: 959, test AUC: 0.8903, test AP: 0.8913, train AUC: 0.9599, train AP: 0.9539, loss:0.9637\n",
      "Epoch: 960, test AUC: 0.8903, test AP: 0.8913, train AUC: 0.9599, train AP: 0.9539, loss:0.9529\n",
      "Epoch: 961, test AUC: 0.8903, test AP: 0.8913, train AUC: 0.9599, train AP: 0.9539, loss:0.9619\n",
      "Epoch: 962, test AUC: 0.8903, test AP: 0.8913, train AUC: 0.9600, train AP: 0.9539, loss:0.9593\n",
      "Epoch: 963, test AUC: 0.8903, test AP: 0.8913, train AUC: 0.9600, train AP: 0.9540, loss:0.9714\n",
      "Epoch: 964, test AUC: 0.8903, test AP: 0.8914, train AUC: 0.9601, train AP: 0.9541, loss:0.9610\n",
      "Epoch: 965, test AUC: 0.8904, test AP: 0.8915, train AUC: 0.9602, train AP: 0.9542, loss:0.9556\n",
      "Epoch: 966, test AUC: 0.8903, test AP: 0.8915, train AUC: 0.9602, train AP: 0.9542, loss:0.9634\n",
      "Epoch: 967, test AUC: 0.8903, test AP: 0.8914, train AUC: 0.9602, train AP: 0.9542, loss:0.9552\n",
      "Epoch: 968, test AUC: 0.8903, test AP: 0.8914, train AUC: 0.9602, train AP: 0.9542, loss:0.9600\n",
      "Epoch: 969, test AUC: 0.8902, test AP: 0.8913, train AUC: 0.9601, train AP: 0.9541, loss:0.9584\n",
      "Epoch: 970, test AUC: 0.8902, test AP: 0.8913, train AUC: 0.9601, train AP: 0.9541, loss:0.9654\n",
      "Epoch: 971, test AUC: 0.8901, test AP: 0.8912, train AUC: 0.9601, train AP: 0.9541, loss:0.9643\n",
      "Epoch: 972, test AUC: 0.8900, test AP: 0.8911, train AUC: 0.9600, train AP: 0.9540, loss:0.9564\n",
      "Epoch: 973, test AUC: 0.8899, test AP: 0.8910, train AUC: 0.9599, train AP: 0.9539, loss:0.9562\n",
      "Epoch: 974, test AUC: 0.8899, test AP: 0.8910, train AUC: 0.9599, train AP: 0.9538, loss:0.9625\n",
      "Epoch: 975, test AUC: 0.8900, test AP: 0.8911, train AUC: 0.9599, train AP: 0.9539, loss:0.9593\n",
      "Epoch: 976, test AUC: 0.8901, test AP: 0.8912, train AUC: 0.9600, train AP: 0.9539, loss:0.9583\n",
      "Epoch: 977, test AUC: 0.8902, test AP: 0.8913, train AUC: 0.9601, train AP: 0.9540, loss:0.9637\n",
      "Epoch: 978, test AUC: 0.8903, test AP: 0.8915, train AUC: 0.9601, train AP: 0.9541, loss:0.9619\n",
      "Epoch: 979, test AUC: 0.8904, test AP: 0.8916, train AUC: 0.9602, train AP: 0.9541, loss:0.9661\n",
      "Epoch: 980, test AUC: 0.8905, test AP: 0.8917, train AUC: 0.9602, train AP: 0.9542, loss:0.9537\n",
      "Epoch: 981, test AUC: 0.8906, test AP: 0.8917, train AUC: 0.9602, train AP: 0.9542, loss:0.9568\n",
      "Epoch: 982, test AUC: 0.8906, test AP: 0.8918, train AUC: 0.9602, train AP: 0.9542, loss:0.9598\n",
      "Epoch: 983, test AUC: 0.8907, test AP: 0.8918, train AUC: 0.9603, train AP: 0.9543, loss:0.9601\n",
      "Epoch: 984, test AUC: 0.8908, test AP: 0.8919, train AUC: 0.9603, train AP: 0.9543, loss:0.9634\n",
      "Epoch: 985, test AUC: 0.8908, test AP: 0.8919, train AUC: 0.9604, train AP: 0.9544, loss:0.9690\n",
      "Epoch: 986, test AUC: 0.8908, test AP: 0.8920, train AUC: 0.9604, train AP: 0.9545, loss:0.9581\n",
      "Epoch: 987, test AUC: 0.8909, test AP: 0.8920, train AUC: 0.9605, train AP: 0.9545, loss:0.9631\n",
      "Epoch: 988, test AUC: 0.8909, test AP: 0.8921, train AUC: 0.9606, train AP: 0.9546, loss:0.9598\n",
      "Epoch: 989, test AUC: 0.8909, test AP: 0.8922, train AUC: 0.9606, train AP: 0.9547, loss:0.9570\n",
      "Epoch: 990, test AUC: 0.8909, test AP: 0.8922, train AUC: 0.9606, train AP: 0.9547, loss:0.9571\n",
      "Epoch: 991, test AUC: 0.8908, test AP: 0.8921, train AUC: 0.9606, train AP: 0.9547, loss:0.9657\n",
      "Epoch: 992, test AUC: 0.8908, test AP: 0.8920, train AUC: 0.9606, train AP: 0.9546, loss:0.9581\n",
      "Epoch: 993, test AUC: 0.8907, test AP: 0.8919, train AUC: 0.9605, train AP: 0.9545, loss:0.9654\n",
      "Epoch: 994, test AUC: 0.8906, test AP: 0.8918, train AUC: 0.9604, train AP: 0.9544, loss:0.9587\n",
      "Epoch: 995, test AUC: 0.8906, test AP: 0.8918, train AUC: 0.9604, train AP: 0.9544, loss:0.9614\n",
      "Epoch: 996, test AUC: 0.8906, test AP: 0.8918, train AUC: 0.9603, train AP: 0.9543, loss:0.9541\n",
      "Epoch: 997, test AUC: 0.8907, test AP: 0.8918, train AUC: 0.9603, train AP: 0.9543, loss:0.9594\n",
      "Epoch: 998, test AUC: 0.8907, test AP: 0.8918, train AUC: 0.9603, train AP: 0.9543, loss:0.9598\n",
      "Epoch: 999, test AUC: 0.8907, test AP: 0.8918, train AUC: 0.9603, train AP: 0.9543, loss:0.9576\n",
      "Epoch: 1000, test AUC: 0.8907, test AP: 0.8918, train AUC: 0.9603, train AP: 0.9543, loss:0.9589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:17<00:00, 137.07s/it]\n"
     ]
    }
   ],
   "source": [
    "proxi_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\proxi_dfs\\cd8'\n",
    "edges_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\connected_patches\\cd8'\n",
    "centers_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\patch_centers\\cd8'\n",
    "out_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\embedded_features\\cd8'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "HIDDEN_SIZE = 15\n",
    "OUT_CHANNELS = 10\n",
    "EPOCHS = 1000\n",
    "\n",
    "for s in tqdm(sets):\n",
    "    # Load data\n",
    "    proxi_df = pd.read_csv(os.path.join(proxi_dir,s+'.csv'), index_col=0)\n",
    "    edges = pd.read_pickle(os.path.join(edges_dir,s+'.pkl'))\n",
    "    patch_centers = pd.read_csv(os.path.join(centers_dir,s+'_centers.csv'), index_col=0)\n",
    "    \n",
    "    # Read edges\n",
    "    edge_temp = []\n",
    "    for item in tqdm(edges):\n",
    "        edge_temp = edge_temp + item\n",
    "\n",
    "    total_connections = 0\n",
    "    for item in tqdm(edges):\n",
    "        total_connections = total_connections + len(item)\n",
    "\n",
    "    edges = np.zeros((2,total_connections))\n",
    "    for i,item in tqdm(enumerate(edge_temp)):\n",
    "        idx1 = proxi_df.index.tolist().index(item[0])\n",
    "        idx2 = proxi_df.index.tolist().index(item[1])\n",
    "        edges[0,i] = idx1\n",
    "        edges[1,i] = idx2\n",
    "    \n",
    "    # Create data object\n",
    "    X = proxi_df.values\n",
    "    X_tensor = torch.tensor(proxi_df.values, dtype=torch.float)\n",
    "    edge_tensor = torch.tensor(edges, dtype=torch.long)\n",
    "    data = Data(x=X_tensor,edge_index=edge_tensor)\n",
    "\n",
    "    # transformation\n",
    "    t = T.Compose([T.ToUndirected(),T.RandomLinkSplit(is_undirected=True,split_labels=True)])\n",
    "    train_set,val_set,test_set = t(data)\n",
    "    train_set.to(device)\n",
    "    val_set.to(device)\n",
    "    test_set.to(device)\n",
    "\n",
    "    # Train\n",
    "    NUM_FEATURES = X.shape[1]\n",
    "    gae_model = GAE(GCNEncoder(NUM_FEATURES, HIDDEN_SIZE, OUT_CHANNELS, 0.5))\n",
    "    gae_model = gae_model.to(device)\n",
    "\n",
    "    losses = []\n",
    "    test_auc = []\n",
    "    test_ap = []\n",
    "    train_aucs = []\n",
    "    train_aps = []\n",
    "\n",
    "    optimizer = torch.optim.Adam(gae_model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        loss = gae_train(train_set, gae_model, optimizer)\n",
    "        losses.append(loss)\n",
    "        auc, ap = gae_test(test_set, gae_model)\n",
    "        test_auc.append(auc)\n",
    "        test_ap.append(ap)\n",
    "\n",
    "        train_auc, train_ap = gae_test(train_set, gae_model)\n",
    "\n",
    "        train_aucs.append(train_auc)\n",
    "        train_aps.append(train_ap)\n",
    "\n",
    "        print('Epoch: {:03d}, test AUC: {:.4f}, test AP: {:.4f}, train AUC: {:.4f}, train AP: {:.4f}, loss:{:.4f}'.format(epoch, auc, ap, train_auc, train_ap, loss))\n",
    "\n",
    "    torch.save(gae_model, os.path.join(out_dir,'cd8_autoencoder_'+s+'.pth'))\n",
    "    \n",
    "    # Embedding\n",
    "    t2 = T.Compose([T.ToUndirected()])\n",
    "    transformed_data = t2(data)\n",
    "    transformed_data.to(device)\n",
    "    with torch.no_grad():\n",
    "        z_embed = gae_model.encode(transformed_data.x, transformed_data.edge_index)\n",
    "\n",
    "    z_embed = z_embed.cpu()\n",
    "    numpy_z = z_embed.numpy()\n",
    "    z_df = pd.DataFrame(numpy_z, index=proxi_df.index)\n",
    "    z_df.insert(loc=z_df.shape[1], column='patch', value=proxi_df.index)\n",
    "    z_df.insert(loc=z_df.shape[1], column='cellID', value=patch_centers['cellID'])\n",
    "    z_df.to_csv(os.path.join(out_dir,'cd8_'+s+'_proxi_embedding.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explanability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import remove_self_loops\n",
    "from torch_geometric.explain import Explainer, GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = ['set3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\embedded_features\\cd8'\n",
    "models = {}\n",
    "for s in sets:\n",
    "    models[s] = torch.load(os.path.join(models_dir, 'cd8_autoencoder_' + s + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNEncoder(\n",
       "  (conv1): GCNConv(15, 15)\n",
       "  (conv2): GCNConv(15, 10)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['set3'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd4_central_proxi = pd.read_pickle(r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\cd4_central_proxi.pkl')\n",
    "cd8_central_proxi = pd.read_pickle(r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\cd8_central_proxi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_centers = pd.read_csv(r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\patch_centers\\cd8\\set3_centers.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1055/1055 [36:08<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "proxi_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\proxi_dfs\\cd8'\n",
    "edge_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\connected_patches\\cd8'\n",
    "centers_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\networks\\patch_centers\\cd8'\n",
    "out_dir = r'..\\..\\coculture_diagonal\\primed_pbmc\\00_analysis\\gae_encoder_explainer\\cd8'\n",
    "t = T.Compose([T.ToUndirected()])\n",
    "for s in sets:\n",
    "    model = models[s]\n",
    "    model.to(device)\n",
    "    central_proxi = cd8_central_proxi[s]\n",
    "    proxi_df = pd.read_csv(os.path.join(proxi_dir, s + '.csv'), index_col=0)\n",
    "    patch_centers = pd.read_csv(os.path.join(centers_dir, s + '_centers.csv'), index_col=0)\n",
    "    edges = pd.read_pickle(os.path.join(edge_dir, s + '.pkl'))\n",
    "\n",
    "    proxi_df['patch_id'] = patch_centers['patch_id']\n",
    "    proxi_df['cellID'] = patch_centers['cellID']\n",
    "    cells = proxi_df['cellID'].unique().tolist()\n",
    "\n",
    "    explainer = Explainer(model=model.encoder,\n",
    "                        algorithm=GNNExplainer(epochs=200),\n",
    "                        explanation_type='model',\n",
    "                        node_mask_type='attributes',\n",
    "                        edge_mask_type='object',\n",
    "                        model_config=dict(mode='regression',\n",
    "                                            task_level='graph',\n",
    "                                            return_type='raw')\n",
    "                        )\n",
    "\n",
    "    for i in tqdm(central_proxi.index.tolist()):\n",
    "        patch_id = central_proxi['patch_id'][i]\n",
    "        cell = central_proxi['cellID'][i]\n",
    "\n",
    "        cell_index = cells.index(cell)\n",
    "        cell_x = proxi_df[proxi_df['cellID'] == cell]\n",
    "        patch_index = cell_x['patch_id'].tolist().index(patch_id)\n",
    "        cell_x = cell_x.drop(columns=['patch_id','cellID'])\n",
    "        single_cell_x = torch.Tensor(cell_x.values.astype('float'))\n",
    "\n",
    "        edge = np.array(edges[cell_index]).T\n",
    "        min_idx = np.min(edge)\n",
    "        edge = edge - min_idx\n",
    "        cell_edge = torch.LongTensor(edge)\n",
    "\n",
    "        cell_data = Data(x=single_cell_x, edge_index=cell_edge)\n",
    "        single_cell_transformed_data = t(cell_data)\n",
    "        single_cell_transformed_data.to(device)\n",
    "\n",
    "        edge_index, edge_attr = remove_self_loops(single_cell_transformed_data.edge_index, single_cell_transformed_data.edge_attr)\n",
    "\n",
    "        explanation = explainer(single_cell_transformed_data.x, edge_index, index = patch_index)\n",
    "\n",
    "        edge_mask = explanation.edge_mask.detach().cpu().numpy()\n",
    "        node_mask = explanation.node_mask.detach().cpu().numpy()\n",
    "\n",
    "        explained_edges = edge_index.cpu().numpy()\n",
    "\n",
    "        with open(os.path.join(out_dir,s + '_' + patch_id + '.pkl'),'wb') as f:\n",
    "            pickle.dump({'edge_mask':edge_mask,'node_mask':node_mask,'explained_edges':explained_edges},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
